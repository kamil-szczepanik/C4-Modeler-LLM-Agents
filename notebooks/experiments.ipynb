{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWKAdYuddYXX",
        "outputId": "8dcd6c5d-7c26-45d8-a16d-da6728c0803e"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet -U langchain langchain_openai langgraph langchainhub langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbhYLWKPgHp0",
        "outputId": "3da746c2-2754-4933-d0c1-c4b9834858b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-unstructured langchain-community langchain-deepseek langchain-google-genai langchain_xai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\kamil\\documents\\kamil\\hicss\\.venv\\lib\\site-packages (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2-TADFkNgNUT"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Now set them manually if needed, or access directly with os.getenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = os.getenv(\"DEEPSEEK_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
        "os.environ[\"XAI_API_KEY\"] = os.getenv(\"XAI_API_KEY\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvcK5pgQkDKY"
      },
      "source": [
        "# SAD Agents v4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xlQ8N-JWkGr5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "from typing_extensions import Annotated, Literal, TypedDict\n",
        "\n",
        "# langchain imports (schema, messages, prompts, tools)\n",
        "from langchain.schema import HumanMessage, BaseMessage\n",
        "from langchain_core.messages import (\n",
        "    RemoveMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage,  # Already imported above, could remove one\n",
        "    trim_messages,\n",
        "    SystemMessage,\n",
        "    ToolMessage\n",
        ")\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.tools.base import InjectedToolCallId\n",
        "\n",
        "# langgraph imports\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import InjectedState, chat_agent_executor\n",
        "from langgraph.types import Command\n",
        "\n",
        "# model providers\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "# prompts\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "import functools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8zDyZe-kGr5"
      },
      "source": [
        "## State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Deh51oOToRDX"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence, Dict, List\n",
        "from collections import deque\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "# A standard structure for the outputs of each level/diagram\n",
        "class LevelOutput(TypedDict, total=False):\n",
        "    \"\"\"Holds the artifacts for a single C4 diagram.\"\"\"\n",
        "    analysis: str  # The textual analysis from the agent\n",
        "    yaml_definition: str  # The structured YAML output\n",
        "    diagram: str  # The generated PlantUML diagram code\n",
        "\n",
        "# The main TypedDict to hold our entire C4 model as it's built\n",
        "class C4Model(TypedDict):\n",
        "    \"\"\"A nested dictionary to store all generated C4 model artifacts.\"\"\"\n",
        "    context: LevelOutput\n",
        "    containers: LevelOutput\n",
        "    # Components are stored in a dict, keyed by the container's name\n",
        "    components: Dict[str, LevelOutput]\n",
        "\n",
        "# The final state for our LangGraph application\n",
        "class State(TypedDict):\n",
        "    messages: Sequence[BaseMessage]\n",
        "    system_brief: str\n",
        "    c4_model: C4Model\n",
        "    component_queue: deque[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Raxemp3QvESW"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# --- 1. Setup the Language Model ---\n",
        "# It's recommended to set the API key as an environment variable\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "temperature = 0\n",
        "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "# llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=temperature)\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-preview-06-05\",temperature=temperature)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\",temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tAoScfuraKK"
      },
      "source": [
        "## Analysis agent team subgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthJtPPaQGa-"
      },
      "source": [
        "#### Agent personas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7TxcKHQwQIi9"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Agent:\n",
        "    \"\"\"A simple dataclass to hold agent information.\"\"\"\n",
        "    name: str\n",
        "    persona: str\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Level 1 (System Context) Team ---\n",
        "# Goal: High-level, business-focused view of users and external systems.\n",
        "# ==============================================================================\n",
        "\n",
        "context_level_team = [\n",
        "    Agent(\n",
        "        name=\"Product_Owner\",\n",
        "        persona=\"\"\"You are a visionary Product Owner. Your primary goal is to represent the user's voice and ensure the system delivers clear business value.\n",
        "\n",
        "Your focus is on:\n",
        "- **Defining the 'Why':** The core problem this system solves and for whom.\n",
        "- **User Personas:** Clearly identifying the different types of users and their motivations.\n",
        "- **Business Goals:** Ensuring the system's scope aligns with strategic objectives.\n",
        "\n",
        "Your contribution is to **champion the user's perspective.** You must define the 'People' who will use the system and relentlessly question how the proposed design serves their needs.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Business_Analyst\",\n",
        "        persona=\"\"\"You are a pragmatic Business Analyst. Your primary goal is to map business requirements to system capabilities and define the system's boundaries.\n",
        "\n",
        "Your focus is on:\n",
        "- **System Scope:** The system's core purpose and functional boundaries.\n",
        "- **External Interactions:** Identifying all key external systems the system must interact with.\n",
        "- **High-Level Data Flow:** Mapping the essential data exchange with external systems.\n",
        "\n",
        "Your contribution is to **define the 'External Systems' and the system's boundary.** You must clearly state the external dependencies required and how they fit into the overall business process.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Lead_Software_Architect\",\n",
        "        persona=\"\"\"You are a high-level Lead Software Architect. Your goal is to provide an early technical feasibility assessment, treating the entire system as a single black box.\n",
        "\n",
        "Your focus is on:\n",
        "- **Technical Sanity Check:** Ensuring the high-level business goals are technically plausible.\n",
        "- **System Responsibilities:** Summarizing the system's core function from a technical standpoint.\n",
        "- **Identifying Major Constraints:** Pointing out significant technical or resource constraints early.\n",
        "\n",
        "Your contribution is to **ground the business vision in technical reality.** You translate the discussion into a concise, high-level technical summary of the system's purpose and interactions.\"\"\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Level 2 (Containers) Team ---\n",
        "# Goal: Decompose the system into major technical building blocks.\n",
        "# ==============================================================================\n",
        "\n",
        "container_level_team = [\n",
        "    Agent(\n",
        "        name=\"Software_Architect\",\n",
        "        persona=\"\"\"You are a hands-on Software Architect. Having understood the high-level context, your goal is now to design the system's macro-architecture.\n",
        "\n",
        "Your focus is on:\n",
        "- **Container Decomposition:** Breaking the system into logical, deployable units (e.g., Web App, API, Database, Message Queue).\n",
        "- **Technology Choices:** Proposing the primary technology stack for each container.\n",
        "- **High-Level Relationships:** Defining how containers communicate and the protocols they use.\n",
        "\n",
        "Your contribution is to **propose a clear set of 'Containers' and the 'Relationships' between them,** justifying your architectural patterns and technology choices.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Lead_Developer\",\n",
        "        persona=\"\"\"You are a practical Lead Developer. Your goal is to ensure the proposed architecture is buildable and maintainable for the development team.\n",
        "\n",
        "Your focus is on:\n",
        "- **Implementation Feasibility:** Assessing the complexity of the proposed containers.\n",
        "- **Technology Trade-offs:** Debating the pros and cons of technology choices (e.g., specific frameworks, databases).\n",
        "- **Developer Experience:** Considering how easy it will be for a team to build and work with this design.\n",
        "\n",
        "Your contribution is to **validate or challenge the architectural design from a hands-on implementation perspective,** suggesting concrete alternatives where appropriate.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"DevOps_Specialist\",\n",
        "        persona=\"\"\"You are a production-focused DevOps Specialist. Your goal is to ensure the system is designed for operational excellence.\n",
        "\n",
        "Your focus is on:\n",
        "- **Deployability:** How the proposed containers will be packaged and deployed.\n",
        "- **Observability:** How the system will be monitored for health and performance.\n",
        "- **Scalability & Reliability:** Identifying potential operational bottlenecks and failure modes.\n",
        "\n",
        "Your contribution is to **critique the proposed architecture from an operational standpoint,** pointing out potential issues with deployment, scaling, or monitoring.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Security_Specialist\",\n",
        "        persona=\"\"\"You are a vigilant Security Specialist. Your goal is to establish a secure foundation for the system at the container level.\n",
        "\n",
        "Your focus is on:\n",
        "- **Trust Boundaries:** Defining the security posture of communications between containers.\n",
        "- **Data Protection in Transit:** Specifying security requirements for APIs and data flows (e.g., TLS, mTLS).\n",
        "- **Authentication Gateways:** Assessing where and how initial authentication should be handled (e.g., API Gateway).\n",
        "\n",
        "Your contribution is to **analyze the proposed container relationships for security flaws** and recommend high-level security controls.\"\"\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Level 3 (Components) Team ---\n",
        "# Goal: Detail the internal design of a single container.\n",
        "# ==============================================================================\n",
        "\n",
        "component_level_team = [\n",
        "    Agent(\n",
        "        name=\"Lead_Developer\",\n",
        "        persona=\"\"\"You are the Lead Developer for this specific container. Your goal is to drive the detailed internal design of your service.\n",
        "\n",
        "Your focus is on:\n",
        "- **Component Decomposition:** Breaking your container into key logical components (e.g., API Controller, Service, Repository, Domain Model).\n",
        "- **Internal APIs & Interfaces:** Defining the contracts and responsibilities for each component.\n",
        "- **Design Patterns:** Selecting and applying appropriate design patterns for a clean and maintainable internal structure.\n",
        "\n",
        "Your contribution is to **propose a clear set of 'Components' and their 'Relationships' inside this container,** leading the detailed design discussion.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Senior_Developer\",\n",
        "        persona=\"\"\"You are a Senior Developer on the team. Your goal is to contribute to a high-quality component design by focusing on implementation details and best practices.\n",
        "\n",
        "Your focus is on:\n",
        "- **Code-Level Design:** Considering class and function responsibilities.\n",
        "- **Adherence to Patterns:** Ensuring the proposed design patterns are used correctly.\n",
        "- **Testability:** Thinking about how the proposed components can be effectively unit-tested.\n",
        "\n",
        "Your contribution is to **refine the detailed component design,** ask clarifying questions, and suggest improvements based on your implementation experience.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Database_Administrator\",\n",
        "        persona=\"\"\"You are a Database Administrator. Your goal is to ensure the data model for this container is efficient, secure, and reliable.\n",
        "\n",
        "Your focus is on:\n",
        "- **Schema Design:** The specific tables, columns, and data types for components that interact with the database.\n",
        "- **Query Performance:** How components will query data and whether indexes are needed.\n",
        "- **Data Integrity:** Ensuring relationships and constraints are correctly defined.\n",
        "\n",
        "Your contribution is to **define and critique the database-related aspects of the component design.** You are only active if the container has a database.\"\"\"\n",
        "    ),\n",
        "    Agent(\n",
        "        name=\"Security_Specialist\",\n",
        "        persona=\"\"\"You are a vigilant Security Specialist, now focusing on the internals of this container. Your goal is to find and mitigate vulnerabilities at the code level.\n",
        "\n",
        "Your focus is on:\n",
        "- **Input Validation & Sanitization:** Ensuring data entering each component is safe.\n",
        "- **Fine-Grained Authorization:** How permissions are checked within the component's logic.\n",
        "- **Secure Coding Practices:** Identifying potential risks like injection flaws or improper error handling.\n",
        "\n",
        "Your contribution is to **analyze the internal component design for security vulnerabilities** and recommend specific, code-level security controls.\"\"\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lf6zLGHQMpu"
      },
      "source": [
        "#### subgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB8Noq2rIpZQ"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import os\n",
        "import re # <<< ADDED: Missing import for sanitization\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Annotated, TypedDict\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "# from langchain_deepseek import ChatDeepSeek # Your LLM import\n",
        "\n",
        "# --- ADDED: Agent dataclass for context ---\n",
        "@dataclass\n",
        "class Agent:\n",
        "    \"\"\"A simple dataclass to hold agent information.\"\"\"\n",
        "    name: str\n",
        "    persona: str\n",
        "\n",
        "# --- State is now more generic ---\n",
        "class CollaborativeAnalysisState(TypedDict):\n",
        "    \"\"\"Internal state for the collaborative analysis subgraph.\"\"\"\n",
        "    messages: Annotated[List[BaseMessage], add_messages]\n",
        "    system_brief: str\n",
        "    context: str\n",
        "    level: str\n",
        "    max_rounds: int\n",
        "    final_analysis: str\n",
        "    team: List[Agent] # <<< NEW: The team of agents for the current collaboration\n",
        "\n",
        "# --- agent_node and report_generator_node require NO CHANGES ---\n",
        "def agent_node(state: CollaborativeAnalysisState, agent: Agent, llm: BaseChatModel) -> dict:\n",
        "    print(f\"--- ðŸ—£ï¸  Turn: {agent.name} on C4 Level: '{state['level']}' ---\")\n",
        "    system_prompt = f\"\"\"You are a member of an expert team collaboratively creating the analysis for a C4 model diagram.\n",
        "    Your current task is to analyze the provided system brief for the **C4 {state['level']} level**.\n",
        "    {state['context']}\n",
        "    Your specific role is as follows:\n",
        "    ---\n",
        "    {agent.persona}\n",
        "    ---\n",
        "    Read the conversation history and add your next insight based on your specific role. Provide your analysis directly and concisely.\"\"\"\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "    chain = prompt_template | llm\n",
        "    response = chain.invoke({\"messages\": state[\"messages\"]})\n",
        "    sanitized_name = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", agent.name)\n",
        "    named_message = AIMessage(content=response.content, name=sanitized_name)\n",
        "    return {\"messages\": [named_message]}\n",
        "\n",
        "\n",
        "def report_generator_node(state: CollaborativeAnalysisState, llm: BaseChatModel) -> dict:\n",
        "    print(\"--- ðŸ”¬ Generating Final Analysis Report ---\")\n",
        "    system_prompt = \"\"\"You are a meticulous Scribe-Agent for a C4 model design session. Your sole mission is to create a single, comprehensive, and exhaustive transcript of the architectural decisions made.\n",
        "\n",
        "**This output is critical as it will be used as a direct input for an automated process, so it must be a complete and unfiltered record of the facts.**\n",
        "\n",
        "Your role is to **compile and integrate** every insight from the collaborative discussion into a final, consolidated report.\n",
        "\n",
        "**CRITICAL RULES:**\n",
        "1.  **DO NOT SUMMARIZE:** Your task is to collate and transcribe, not to condense or interpret. Every point, proposal, critique, and decision must be captured.\n",
        "2.  **PRESERVE EVERY FACT:** No detail is too small. If a technology, version number, security concern, or component name was mentioned as part of a final decision, it must be included in the report.\n",
        "3.  **CONSOLIDATE WITHOUT OMISSION:** You must logically structure the final output, but you must ensure that this consolidation process does not lead to any information loss. Integrate all points into one coherent document.\n",
        "\n",
        "---\n",
        "**Reference Context: Original System Brief**\n",
        "{system_brief}\n",
        "---\n",
        "\n",
        "Now, review the ENTIRE conversation history and generate the final, all-inclusive, consolidated analysis report based on these strict rules.\n",
        "\"\"\"\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "    chain = prompt_template | llm\n",
        "    final_report = chain.invoke({\n",
        "        \"system_brief\": state[\"system_brief\"],\n",
        "        \"messages\": state[\"messages\"]\n",
        "    }).content\n",
        "    return {\"final_analysis\": final_report}\n",
        "\n",
        "\n",
        "# --- Router now gets the team from the STATE ---\n",
        "def collaboration_router(state: CollaborativeAnalysisState) -> str:\n",
        "    \"\"\"Routes the conversation based on the number of rounds completed.\"\"\"\n",
        "\n",
        "    # <<< CHANGED: Access the team from the state dictionary >>>\n",
        "    active_team = state['team']\n",
        "\n",
        "    # The number of AI turns is the total messages minus the initial human message\n",
        "    num_ai_turns = len(state['messages']) - 1\n",
        "\n",
        "    # The number of full rounds is the AI turns divided by the number of agents in the current team\n",
        "    rounds_completed = num_ai_turns // len(active_team)\n",
        "\n",
        "    if rounds_completed >= state[\"max_rounds\"]:\n",
        "        print(f\"--- âœ… Collaboration Complete: Max rounds ({state['max_rounds']}) reached. ---\")\n",
        "        return \"generate_report\"\n",
        "    else:\n",
        "        # Otherwise, loop back to the first agent to start the next round\n",
        "        return active_team[0].name\n",
        "\n",
        "\n",
        "# --- Graph Factory now accepts a `team` parameter ---\n",
        "def create_collaboration_graph(llm: BaseChatModel, team: List[Agent], max_rounds: int = 2):\n",
        "    \"\"\"\n",
        "    Builds and returns a compiled collaborative analysis subgraph for a GIVEN TEAM.\n",
        "    \"\"\"\n",
        "\n",
        "    builder = StateGraph(CollaborativeAnalysisState)\n",
        "\n",
        "    # <<< CHANGED: Iterate over the passed-in `team` parameter >>>\n",
        "    for agent in team:\n",
        "        # Sanitize the agent name for the node name to be safe\n",
        "        node_name = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", agent.name)\n",
        "        builder.add_node(node_name, functools.partial(agent_node, agent=agent, llm=llm))\n",
        "\n",
        "    builder.add_node(\"generate_report\", functools.partial(report_generator_node, llm=llm))\n",
        "\n",
        "    # Set the entry point to the first agent of the given team\n",
        "    entry_point = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", team[0].name)\n",
        "    builder.set_entry_point(entry_point)\n",
        "\n",
        "    # Create the chain: Agent 1 -> Agent 2 -> etc. for the given team\n",
        "    for i in range(len(team) - 1):\n",
        "        source_node = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", team[i].name)\n",
        "        target_node = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", team[i+1].name)\n",
        "        builder.add_edge(source_node, target_node)\n",
        "\n",
        "    # The last agent in the team calls the router\n",
        "    last_agent_name = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", team[-1].name)\n",
        "    first_agent_name = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", team[0].name)\n",
        "\n",
        "    builder.add_conditional_edges(\n",
        "        last_agent_name,\n",
        "        collaboration_router,\n",
        "        {\n",
        "            \"generate_report\": \"generate_report\",\n",
        "            # The key must also be the sanitized name\n",
        "            first_agent_name: first_agent_name\n",
        "        }\n",
        "    )\n",
        "\n",
        "    builder.add_edge(\"generate_report\", END)\n",
        "\n",
        "    return builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDvMol3zF_G"
      },
      "source": [
        "## Analysis node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hrh51iJ9kPWw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import copy # Import the copy module for deepcopy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Create a Reusable Analysis Chain ---\n",
        "def create_analysis_chain(level: str, persona_prompt: str, llm):\n",
        "    \"\"\"Creates an LLM chain specifically for generating C4 analysis.\"\"\"\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", persona_prompt),\n",
        "        (\"human\",\n",
        "         \"\"\"Analyze the following System Design Brief to produce the textual analysis for the C4 **{level}** level.\n",
        "\n",
        "         **System Design Brief:**\n",
        "         ```yaml\n",
        "         {brief}\n",
        "         ```\n",
        "\n",
        "         {context}\n",
        "\n",
        "         **Your Task:**\n",
        "         Generate a clear, well-structured textual analysis ONLY. Do NOT generate YAML or any diagram code.\n",
        "         - For 'context' level: Identify the system, the key user roles (People), and all external system dependencies.\n",
        "         - For 'container' level: Identify the key deployable containers within the system, their technology choices, and their relationships.\n",
        "         - For 'component' level: Identify the main components inside the '{component_target}' container.\n",
        "\n",
        "         Focus on clearly defining elements and the reasoning for their relationships based on the brief.\"\"\")\n",
        "    ])\n",
        "\n",
        "    return prompt_template | llm | StrOutputParser()\n",
        "\n",
        "# --- 3. Define the Agent Node Function ---\n",
        "def analysis_agent_node(state: State, llm) -> Dict:\n",
        "    \"\"\"\n",
        "    Agent node that generates the textual analysis for the next required C4 level.\n",
        "    \"\"\"\n",
        "    system_brief = state[\"system_brief\"]\n",
        "    c4_model = state[\"c4_model\"]\n",
        "    # Initialize component_target to None\n",
        "    component_target = None\n",
        "\n",
        "    # State-driven logic: determine which analysis to perform\n",
        "    if not c4_model.get(\"context\"):\n",
        "        level = \"context\"\n",
        "        print(\"--- âœï¸ Generating Context Level Analysis ---\")\n",
        "        context = \"\"\n",
        "    elif not c4_model.get(\"containers\"):\n",
        "        level = \"container\"\n",
        "        print(\"--- âœï¸ Generating Container Level Analysis ---\")\n",
        "        # Provide the context analysis as additional context for the LLM\n",
        "        context = f\"**Context Level Analysis (for context):**\\n{c4_model['context']['analysis']}\"\n",
        "    else:\n",
        "        # Check the queue for the next component to analyze\n",
        "        component_queue = state.get(\"component_queue\", [])\n",
        "        if component_queue:\n",
        "            component_target = component_queue[0] # Peek at the next item\n",
        "            level = \"component\"\n",
        "            print(f\"--- âœï¸ Generating Component Level Analysis for '{component_target}' ---\")\n",
        "            context = f\"\"\"**Container Level Analysis (for context):**\n",
        "                         \\n{c4_model['containers']['analysis']}\"\"\"\n",
        "        else:\n",
        "            # Nothing left to do\n",
        "            return {}\n",
        "\n",
        "    # Create the appropriate chain and invoke it\n",
        "    persona = \"You are an expert software architect specializing in the C4 model.\"\n",
        "    analysis_chain = create_analysis_chain(level, persona, llm=llm)\n",
        "    analysis = analysis_chain.invoke({\n",
        "        \"level\": level,\n",
        "        \"brief\": system_brief,\n",
        "        \"context\": context,\n",
        "        \"component_target\": component_target or \"\"\n",
        "    })\n",
        "\n",
        "    # 1. Create a deep copy of the model to avoid modifying the state directly.\n",
        "    updated_model = copy.deepcopy(c4_model)\n",
        "\n",
        "    # 2. Initialize and update the correct nested dictionary in the copy.\n",
        "    if level == \"context\":\n",
        "        updated_model[\"context\"] = {\"analysis\": analysis}\n",
        "    elif level == \"container\":\n",
        "        updated_model[\"containers\"] = {\"analysis\": analysis}\n",
        "    elif level == \"component\" and component_target:\n",
        "        # This ensures the 'components' dictionary itself exists\n",
        "        if \"components\" not in updated_model:\n",
        "            updated_model[\"components\"] = {}\n",
        "        # This creates the nested dictionary for the specific component\n",
        "        updated_model[\"components\"][component_target] = {\"analysis\": analysis}\n",
        "\n",
        "    # 3. Return the entire updated model.\n",
        "    return {\"c4_model\": updated_model}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kAt7jcy-0-"
      },
      "source": [
        "## YAML structure node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GEQqsncRpmKP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import copy # Import the copy module for deepcopy\n",
        "\n",
        "\n",
        "# --- 2. Store YAML Templates ---\n",
        "# These templates are provided to the LLM as examples of the desired output format.\n",
        "\n",
        "CONTEXT_YAML_TEMPLATE = \"\"\"\n",
        "# C4 Model: Level 1 - System Context\n",
        "level: context\n",
        "scope: \"System Context diagram for [System Name]\"\n",
        "system:\n",
        "  name: \"[System Name]\"\n",
        "  description: \"[High-level description of the system's purpose and value.]\"\n",
        "elements:\n",
        "  - type: \"person\"\n",
        "    name: \"[User Role A]\"\n",
        "    description: \"[Description of this user and their goals.]\"\n",
        "  - type: \"externalSystem\"\n",
        "    name: \"[External System A]\"\n",
        "    description: \"[Description of the external system and its function.]\"\n",
        "relationships:\n",
        "  - source: \"[User Role A]\"\n",
        "    destination: \"[System Name]\"\n",
        "    description: \"[Description of the interaction]\"\n",
        "    technology: \"[e.g., HTTPS]\"\n",
        "\"\"\"\n",
        "\n",
        "CONTAINER_YAML_TEMPLATE = \"\"\"\n",
        "# C4 Model: Level 2 - Container\n",
        "level: container\n",
        "scope: \"Container diagram for [System Name]\"\n",
        "system:\n",
        "  name: \"[System Name]\"\n",
        "elements:\n",
        "  # People and External Systems from Level 1 that interact with the containers\n",
        "  - type: \"person\"\n",
        "    name: \"[User Role A]\"\n",
        "    description: \"[Description of this user.]\"\n",
        "  - type: \"externalSystem\"\n",
        "    name: \"[External System A]\"\n",
        "    description: \"[Description of this external system.]\"\n",
        "  # Containers within the system boundary\n",
        "  - type: \"container\"\n",
        "    name: \"[Container A, e.g., Web Application]\"\n",
        "    technology: \"[e.g., React, Angular]\"\n",
        "    description: \"[Responsibility of this container.]\"\n",
        "relationships:\n",
        "  - source: \"[User Role A]\"\n",
        "    destination: \"[Container A, e.g., Web Application]\"\n",
        "    description: \"[e.g., Uses]\"\n",
        "    technology: \"[e.g., HTTPS]\"\n",
        "\"\"\"\n",
        "\n",
        "# A plausible template for the Component level\n",
        "COMPONENT_YAML_TEMPLATE = \"\"\"\n",
        "# C4 Model: Level 3 - Component\n",
        "level: component\n",
        "scope: \"Component diagram for the [Parent Container Name] container\"\n",
        "parentContainer:\n",
        "  name: \"[Parent Container Name]\"\n",
        "elements:\n",
        "  # Components within the parent container's boundary\n",
        "  - type: \"component\"\n",
        "    name: \"[Component A, e.g., Order Controller]\"\n",
        "    technology: \"[e.g., Spring MVC Controller]\"\n",
        "    description: \"[Responsibility of this component.]\"\n",
        "relationships:\n",
        "  - source: \"[Component A, e.g., Order Controller]\"\n",
        "    destination: \"[Component B, e.g., Order Service]\"\n",
        "    description: \"[e.g., Invokes]\"\n",
        "\"\"\"\n",
        "\n",
        "# --- 3. Define the Agent Node Function ---\n",
        "\n",
        "def yaml_structure_node(state: State, llm) -> Dict:\n",
        "    \"\"\"\n",
        "    Agent node that generates a structured YAML string based on the textual analysis.\n",
        "    \"\"\"\n",
        "    c4_model = state[\"c4_model\"]\n",
        "    component_target = None\n",
        "\n",
        "    # State-driven logic: determine which YAML to generate\n",
        "    # Check Context Level\n",
        "    if c4_model.get(\"context\", {}).get(\"analysis\") and not c4_model.get(\"context\", {}).get(\"yaml_definition\"):\n",
        "        level = \"context\"\n",
        "        print(\"--- ðŸ“ Generating Context Level YAML ---\")\n",
        "        analysis = c4_model[\"context\"][\"analysis\"]\n",
        "        template = CONTEXT_YAML_TEMPLATE\n",
        "        context = \"\"\n",
        "    # Check Container Level\n",
        "    elif c4_model.get(\"containers\", {}).get(\"analysis\") and not c4_model.get(\"containers\", {}).get(\"yaml_definition\"):\n",
        "        level = \"container\"\n",
        "        print(\"--- ðŸ“ Generating Container Level YAML ---\")\n",
        "        analysis = c4_model[\"containers\"][\"analysis\"]\n",
        "        template = CONTAINER_YAML_TEMPLATE\n",
        "        context = f\"Context Level YAML (for reference):\\n{c4_model['context']['yaml_definition']}\"\n",
        "    # Check Component Level\n",
        "    else:\n",
        "        component_queue = state.get(\"component_queue\", [])\n",
        "        if component_queue:\n",
        "            component_target = component_queue[0] # Peek at the next item\n",
        "            if c4_model.get(\"components\", {}).get(component_target, {}).get(\"analysis\") and not c4_model.get(\"components\", {}).get(component_target, {}).get(\"yaml_definition\"):\n",
        "                level = \"component\"\n",
        "                print(f\"--- ðŸ“ Generating Component Level YAML for '{component_target}' ---\")\n",
        "                analysis = c4_model[\"components\"][component_target][\"analysis\"]\n",
        "                template = COMPONENT_YAML_TEMPLATE\n",
        "                context = f\"Container Level YAML (for reference):\\n{c4_model['containers']['yaml_definition']}\"\n",
        "            else:\n",
        "                return {} # This component's analysis isn't ready or its YAML is done\n",
        "        else:\n",
        "            return {} # Nothing left to do\n",
        "\n",
        "    # --- Create the LLM Chain for this task ---\n",
        "    persona = \"You are a meticulous software architect. Your task is to convert a textual analysis into a structured YAML file. You must adhere strictly to the provided template.\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", persona),\n",
        "        (\"human\", \"\"\"Based on the provided textual analysis, generate a YAML file that strictly follows the structure of the YAML template.\n",
        "\n",
        "        **Textual Analysis:**\n",
        "        ```\n",
        "        {analysis}\n",
        "        ```\n",
        "\n",
        "        {context}\n",
        "\n",
        "        **YAML Template (Your output MUST follow this format):**\n",
        "        ```yaml\n",
        "        {template}\n",
        "        ```\n",
        "\n",
        "        **Instructions:**\n",
        "        - Populate all fields in the template based on the analysis.\n",
        "        - Do NOT deviate from the template's structure.\n",
        "        - Your output must be ONLY the raw YAML string, starting with `level: ...`. Do not add any commentary, explanations, or markdown fences like ```yaml.\n",
        "        \"\"\"),\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # --- Invoke the chain and prepare the state patch ---\n",
        "    yaml_output = chain.invoke({\n",
        "        \"analysis\": analysis,\n",
        "        \"template\": template,\n",
        "        \"context\": context,\n",
        "    })\n",
        "\n",
        "\n",
        "    # 1. Create a deep copy of the model to avoid modifying the state directly.\n",
        "    updated_model = copy.deepcopy(c4_model)\n",
        "\n",
        "    # 2. Add the new key to the *existing* nested dictionary.\n",
        "    if level == \"context\":\n",
        "        updated_model[\"context\"][\"yaml_definition\"] = yaml_output\n",
        "    elif level == \"container\":\n",
        "        updated_model[\"containers\"][\"yaml_definition\"] = yaml_output\n",
        "    elif level == \"component\" and component_target:\n",
        "        # This logic was already better, but we'll make it consistent\n",
        "        if component_target not in updated_model[\"components\"]:\n",
        "            updated_model[\"components\"][component_target] = {}\n",
        "        updated_model[\"components\"][component_target][\"yaml_definition\"] = yaml_output\n",
        "\n",
        "    # 3. Return the entire updated model. LangGraph will merge this correctly.\n",
        "    return {\"c4_model\": updated_model}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivSjXloQy68m"
      },
      "source": [
        "## PlantUML Diagram Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CV_5Oy2epmMf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# --- 2. Store PlantUML Syntax Guide ---\n",
        "# This guide is crucial for ensuring the LLM generates valid C4 PlantUML.\n",
        "PLANTUML_SYNTAX_GUIDE = \"\"\"\n",
        "You must use the C4-PlantUML library syntax. Here are the key elements:\n",
        "\n",
        "1.  **Header:** Always start with `@startuml` and include the C4_Context.puml, C4_Container.puml, or C4_Component.puml file.\n",
        "    ```plantuml\n",
        "    @startuml\n",
        "    !include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Context.puml\n",
        "    LAYOUT_WITH_LEGEND()\n",
        "    ```\n",
        "\n",
        "2.  **Elements:** Define elements using these functions. Use the 'name' from YAML as the alias and label.\n",
        "    - `Person(alias, label, description)`\n",
        "    - `System(alias, label, description)`\n",
        "    - `System_Ext(alias, label, description)`\n",
        "    - `Container(alias, label, technology, description)`\n",
        "    - `ContainerDb(alias, label, technology, description)`\n",
        "    - `Component(alias, label, technology, description)`\n",
        "\n",
        "3.  **Boundaries:** Use boundaries for container and component diagrams.\n",
        "    - `System_Boundary(alias, label) { ... elements ... }`\n",
        "    - `Container_Boundary(alias, label) { ... elements ... }`\n",
        "\n",
        "4.  **Relationships:** Connect elements with `Rel`.\n",
        "    - `Rel(source_alias, destination_alias, label, technology)`\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- 3. Define the Agent Node Function ---\n",
        "\n",
        "def plantuml_diagram_node(state: State, llm) -> Dict:\n",
        "    \"\"\"\n",
        "    Agent node that generates a PlantUML diagram string from the analysis and YAML.\n",
        "    \"\"\"\n",
        "    c4_model = state[\"c4_model\"]\n",
        "    component_target = None\n",
        "\n",
        "    # State-driven logic: determine which diagram to generate\n",
        "    # Check if a level has analysis and yaml, but no diagram\n",
        "    # Check Context Level\n",
        "    if c4_model.get(\"context\", {}).get(\"yaml_definition\") and not c4_model.get(\"context\", {}).get(\"diagram\"):\n",
        "        level = \"context\"\n",
        "        print(f\"--- ðŸŽ¨ Generating {level.capitalize()} Level Diagram ---\")\n",
        "        analysis = c4_model[\"context\"][\"analysis\"]\n",
        "        yaml_def = c4_model[\"context\"][\"yaml_definition\"]\n",
        "    # Check Container Level\n",
        "    elif c4_model.get(\"containers\", {}).get(\"yaml_definition\") and not c4_model.get(\"containers\", {}).get(\"diagram\"):\n",
        "        level = \"container\"\n",
        "        print(f\"--- ðŸŽ¨ Generating {level.capitalize()} Level Diagram ---\")\n",
        "        analysis = c4_model[\"containers\"][\"analysis\"]\n",
        "        yaml_def = c4_model[\"containers\"][\"yaml_definition\"]\n",
        "    # Check Component Level\n",
        "    else:\n",
        "        component_queue = state.get(\"component_queue\", [])\n",
        "        if component_queue:\n",
        "            component_target = component_queue[0]  # Peek\n",
        "            # Check if this component has yaml but no diagram\n",
        "            if c4_model.get(\"components\", {}).get(component_target, {}).get(\"yaml_definition\") and not c4_model.get(\"components\", {}).get(component_target, {}).get(\"diagram\"):\n",
        "                level = \"component\"\n",
        "                print(f\"--- ðŸŽ¨ Generating {level.capitalize()} Level Diagram for '{component_target}' ---\")\n",
        "                analysis = c4_model[\"components\"][component_target][\"analysis\"]\n",
        "                yaml_def = c4_model[\"components\"][component_target][\"yaml_definition\"]\n",
        "            else:\n",
        "                return {} # This component isn't ready or is already done\n",
        "        else:\n",
        "            return {} # Nothing left to do\n",
        "\n",
        "    # --- Create the LLM Chain for this task ---\n",
        "    persona = \"You are an expert software architect and a specialist in generating C4 diagrams using PlantUML. Your task is to convert a YAML definition into a valid PlantUML diagram, using the accompanying analysis for context.\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", persona),\n",
        "        (\"human\", \"\"\"Generate a C4 PlantUML diagram based on the provided YAML definition and textual analysis.\n",
        "\n",
        "        **Reference Syntax Guide:**\n",
        "        ```plantuml\n",
        "        {syntax_guide}\n",
        "        ```\n",
        "\n",
        "        **Source YAML Definition:**\n",
        "        ```yaml\n",
        "        {yaml_def}\n",
        "        ```\n",
        "\n",
        "        **Source Textual Analysis (for context on relationships and descriptions):**\n",
        "        ```\n",
        "        {analysis}\n",
        "        ```\n",
        "\n",
        "        **Instructions:**\n",
        "        - Convert every element and relationship from the YAML file into the correct PlantUML syntax.\n",
        "        - Use the `analysis` text to write better descriptions for relationships if needed.\n",
        "        - Your output must be ONLY the raw PlantUML code, starting with `@startuml`. Do not add any commentary, explanations, or markdown fences like ```plantuml.\n",
        "        \"\"\"),\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # --- Invoke the chain and prepare the state patch ---\n",
        "    diagram_code = chain.invoke({\n",
        "        \"syntax_guide\": PLANTUML_SYNTAX_GUIDE,\n",
        "        \"yaml_def\": yaml_def,\n",
        "        \"analysis\": analysis,\n",
        "    })\n",
        "\n",
        "\n",
        "    # 1. Create a deep copy of the model to avoid modifying the state directly.\n",
        "    updated_model = copy.deepcopy(c4_model)\n",
        "\n",
        "    # 2. Add the new key to the *existing* nested dictionary in the copy.\n",
        "    if level == \"context\":\n",
        "        updated_model[\"context\"][\"diagram\"] = diagram_code\n",
        "    elif level == \"container\":\n",
        "        updated_model[\"containers\"][\"diagram\"] = diagram_code\n",
        "    elif level == \"component\" and component_target:\n",
        "        if component_target not in updated_model[\"components\"]:\n",
        "            updated_model[\"components\"][component_target] = {}\n",
        "        updated_model[\"components\"][component_target][\"diagram\"] = diagram_code\n",
        "\n",
        "    # 3. Return the entire updated model. LangGraph will merge this correctly.\n",
        "    return {\"c4_model\": updated_model}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3gsidfiMSW"
      },
      "source": [
        "## Utility nodes and routers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GSeK3MwBh_vH"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "\n",
        "def populate_component_queue_node(state: State) -> Dict:\n",
        "    \"\"\"\n",
        "    Parses the container YAML to find container names and adds them to the queue.\n",
        "    \"\"\"\n",
        "    print(\"--- âš™ï¸ Populating Component Queue ---\")\n",
        "    container_yaml = state[\"c4_model\"][\"containers\"][\"yaml_definition\"]\n",
        "    try:\n",
        "        data = yaml.safe_load(container_yaml)\n",
        "        # Find all elements of type 'container'\n",
        "        container_names = [\n",
        "            elem[\"name\"] for elem in data.get(\"elements\", []) if elem.get(\"type\") == \"container\"\n",
        "        ]\n",
        "        print(f\"Found containers to process: {container_names}\")\n",
        "        return {\"component_queue\": deque(container_names)}\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error parsing YAML: {e}\")\n",
        "        return {} # No change to the queue on error\n",
        "\n",
        "def complete_component_node(state: State) -> Dict:\n",
        "    \"\"\"\n",
        "    Pops the completed component from the front of the queue.\n",
        "    \"\"\"\n",
        "    print(\"--- âœ… Completing Component Task ---\")\n",
        "    queue = state[\"component_queue\"]\n",
        "    if queue:\n",
        "        completed_item = queue.popleft()\n",
        "        print(f\"Finished processing: {completed_item}\")\n",
        "    return {\"component_queue\": queue}\n",
        "\n",
        "def should_process_components(state: State) -> str:\n",
        "    \"\"\"\n",
        "    Router that checks the component queue to decide whether to continue or end.\n",
        "    \"\"\"\n",
        "    print(\"--- ðŸ¤” Checking Component Queue ---\")\n",
        "    if state[\"component_queue\"]:\n",
        "        print(\"Queue is not empty. Processing next component.\")\n",
        "        return \"process_component\"\n",
        "    else:\n",
        "        print(\"Queue is empty. Finishing workflow.\")\n",
        "        return \"end_workflow\"\n",
        "\n",
        "# 2. After a diagram is created, decide what to do next\n",
        "def post_diagram_router(state: State) -> str:\n",
        "    \"\"\"Decides the next step after any diagram is generated.\"\"\"\n",
        "    # If context is done but container isn't, start container analysis\n",
        "    if state[\"c4_model\"][\"context\"].get(\"diagram\") and not state[\"c4_model\"].get(\"containers\"):\n",
        "        return \"analysis\"\n",
        "    # If container is done, populate the component queue\n",
        "    elif state[\"c4_model\"].get(\"containers\", {}).get(\"diagram\") and state[\"component_queue\"] is None:\n",
        "         return \"populate_queue\"\n",
        "    # If a component diagram was just made, mark it as complete\n",
        "    else:\n",
        "        return \"complete_component\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR2Fvwf-N0HP"
      },
      "source": [
        "## Build graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cik6gbmkP99v"
      },
      "source": [
        "### Choose LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w0nILrn_QAUI"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_xai import ChatXAI\n",
        "\n",
        "\n",
        "MODEL_PROVIDER_MAP = {\n",
        "    \"gemini-1.5-flash-latest\": ChatGoogleGenerativeAI,\n",
        "    \"gemini-1.5-pro-latest\": ChatGoogleGenerativeAI,\n",
        "    \"gemini-2.5-flash-preview-05-20\": ChatGoogleGenerativeAI,\n",
        "    \"gemini-2.5-pro-preview-05-20\": ChatGoogleGenerativeAI,\n",
        "    \"gemini-2.5-pro-preview-06-05\": ChatGoogleGenerativeAI,\n",
        "    \"gpt-4o\": ChatOpenAI,\n",
        "    \"gpt-4o-mini\": ChatOpenAI,\n",
        "    \"deepseek-chat\": ChatDeepSeek,\n",
        "    \"grok-beta\": ChatXAI,\n",
        "    \"grok-3-latest\": ChatXAI,\n",
        "}\n",
        "\n",
        "# The type hint is generated from the keys of our dictionary\n",
        "ModelName = Literal[\n",
        "    \"gemini-1.5-flash-latest\", \"gemini-1.5-pro-latest\",\n",
        "    \"gpt-4o\", \"gpt-4o-mini\",\n",
        "    \"deepseek-chat\",\n",
        "    \"grok-beta\", \"grok-3-latest\"\n",
        "]\n",
        "\n",
        "def get_llm(model_name: ModelName, temperature: float = 0.0) -> BaseChatModel:\n",
        "    \"\"\"\n",
        "    Instantiates and returns a language model based on a direct mapping.\n",
        "    \"\"\"\n",
        "    print(f\"--- âš™ï¸  Instantiating model: {model_name} ---\")\n",
        "\n",
        "    model_class = MODEL_PROVIDER_MAP.get(model_name)\n",
        "\n",
        "    if model_class is None:\n",
        "        raise ImportError(\n",
        "            f\"Model '{model_name}' is not available. \"\n",
        "            f\"Check if its provider library (e.g., langchain_xai) is installed.\"\n",
        "        )\n",
        "\n",
        "    return model_class(model=model_name, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbakNqkc-El4"
      },
      "source": [
        "### Build C4 modeler graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jQa5j-J2r6oq"
      },
      "outputs": [],
      "source": [
        "from typing import Literal, Type, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "import copy\n",
        "import functools\n",
        "\n",
        "\n",
        "def create_c4_modeler_graph(\n",
        "    checkpointer,\n",
        "    model_name: str = \"gemini-1.5-flash-latest\",\n",
        "    analysis_method: Literal[\"simple\", \"collaborative\"] = \"collaborative\",\n",
        "    collab_rounds: int = 2,\n",
        ") -> Type[StateGraph]:\n",
        "    \"\"\"\n",
        "    Factory function to build the C4 Modeler workflow.\n",
        "    \"\"\"\n",
        "    print(f\"--- ðŸ—ï¸ Building graph with model: '{model_name}' and analysis: '{analysis_method}' ---\")\n",
        "\n",
        "    llm = get_llm(model_name=model_name)\n",
        "\n",
        "    workflow = StateGraph(State)\n",
        "\n",
        "    if analysis_method == \"simple\":\n",
        "        bound_analysis_agent_node = functools.partial(analysis_agent_node, llm=llm)\n",
        "        workflow.add_node(\"analysis\", bound_analysis_agent_node)\n",
        "    else:\n",
        "        # --- This is the fully updated collaborative node ---\n",
        "        def collaborative_analysis_node(state: State) -> dict:\n",
        "            \"\"\"\n",
        "            This node acts as a smart orchestrator. It determines the C4 level,\n",
        "            selects the correct expert team, and invokes the appropriate subgraph.\n",
        "            \"\"\"\n",
        "            print(\"--- ðŸš€ Orchestrating Collaborative Analysis ---\")\n",
        "\n",
        "            # 1. Determine the current C4 level and select the appropriate team\n",
        "            level = \"\"\n",
        "            component_target = None\n",
        "            active_team: List[Agent] = [] # The team we will use for the subgraph\n",
        "\n",
        "            if not state[\"c4_model\"].get(\"context\"):\n",
        "                level = \"context\"\n",
        "                active_team = context_level_team # <<< CHANGED\n",
        "                print(\"--- Selecting Team: Context Level ---\")\n",
        "            elif not state[\"c4_model\"].get(\"containers\"):\n",
        "                level = \"container\"\n",
        "                active_team = container_level_team # <<< CHANGED\n",
        "                print(\"--- Selecting Team: Container Level ---\")\n",
        "            else:\n",
        "                level = \"component\"\n",
        "                active_team = component_level_team # <<< CHANGED\n",
        "                if state.get(\"component_queue\"):\n",
        "                    component_target = state[\"component_queue\"][0]\n",
        "                print(f\"--- Selecting Team: Component Level for '{component_target}' ---\")\n",
        "\n",
        "            # 2. Create the specialized subgraph using the selected team\n",
        "            # <<< CHANGED: Pass the active_team to the factory >>>\n",
        "            analysis_subgraph = create_collaboration_graph(\n",
        "                llm=llm,\n",
        "                team=active_team,\n",
        "                max_rounds=collab_rounds\n",
        "            )\n",
        "\n",
        "            # 3. Prepare the input for the subgraph\n",
        "            subgraph_level_description = f\"component '{component_target}'\" if component_target else level\n",
        "            subgraph_input = {\n",
        "                \"messages\": [HumanMessage(content=f\"Let's begin the C4 analysis for the {subgraph_level_description}:\\n\\n{state['system_brief']}\")],\n",
        "                \"system_brief\": state['system_brief'],\n",
        "                \"context\": state['c4_model'].get('context', {}).get('analysis', ''),\n",
        "                \"level\": subgraph_level_description,\n",
        "                \"max_rounds\": collab_rounds,\n",
        "                \"team\": active_team, # <<< CRITICAL: Pass the team into the subgraph's state\n",
        "            }\n",
        "\n",
        "            # 4. Invoke the subgraph\n",
        "            print(f\"--- Invoking subgraph for: {subgraph_level_description} ---\")\n",
        "            subgraph_output = analysis_subgraph.invoke(subgraph_input)\n",
        "            final_analysis = subgraph_output['final_analysis']\n",
        "\n",
        "            # 5. Update the main graph's state with the result\n",
        "            print(f\"--- âœ… Subgraph complete. Updating main C4 model for: {subgraph_level_description} ---\")\n",
        "            updated_model = copy.deepcopy(state['c4_model'])\n",
        "\n",
        "            if level == \"context\":\n",
        "                updated_model[\"context\"] = {\"analysis\": final_analysis}\n",
        "            elif level == \"container\":\n",
        "                updated_model[\"containers\"] = {\"analysis\": final_analysis}\n",
        "            elif level == \"component\" and component_target:\n",
        "                if \"components\" not in updated_model:\n",
        "                    updated_model[\"components\"] = {}\n",
        "                if component_target not in updated_model[\"components\"]:\n",
        "                    updated_model[\"components\"][component_target] = {}\n",
        "                updated_model[\"components\"][component_target][\"analysis\"] = final_analysis\n",
        "                print(f\"--- Successfully updated analysis for component: '{component_target}' ---\")\n",
        "\n",
        "            return {\"c4_model\": updated_model}\n",
        "\n",
        "        workflow.add_node(\"analysis\", collaborative_analysis_node)\n",
        "\n",
        "    # --- The rest of the graph definition remains unchanged ---\n",
        "    bound_yaml_structure_node = functools.partial(yaml_structure_node, llm=llm)\n",
        "    bound_plantuml_diagram_node = functools.partial(plantuml_diagram_node, llm=llm)\n",
        "\n",
        "    workflow.add_node(\"yaml\", bound_yaml_structure_node)\n",
        "    workflow.add_node(\"diagram\", bound_plantuml_diagram_node)\n",
        "    workflow.add_node(\"populate_queue\", populate_component_queue_node)\n",
        "    workflow.add_node(\"complete_component\", complete_component_node)\n",
        "\n",
        "    workflow.set_entry_point(\"analysis\")\n",
        "    workflow.add_edge(\"analysis\", \"yaml\")\n",
        "    workflow.add_edge(\"yaml\", \"diagram\")\n",
        "\n",
        "    workflow.add_conditional_edges(\"diagram\", post_diagram_router, {\n",
        "        \"analysis\": \"analysis\",\n",
        "        \"populate_queue\": \"populate_queue\",\n",
        "        \"complete_component\": \"complete_component\"\n",
        "    })\n",
        "    workflow.add_conditional_edges(\n",
        "        \"populate_queue\", should_process_components,\n",
        "        {\"process_component\": \"analysis\", \"end_workflow\": END}\n",
        "    )\n",
        "    workflow.add_conditional_edges(\n",
        "        \"complete_component\", should_process_components,\n",
        "        {\"process_component\": \"analysis\", \"end_workflow\": END}\n",
        "    )\n",
        "\n",
        "    app = workflow.compile(checkpointer=checkpointer)\n",
        "    print(\"âœ… LangGraph C4 Modeler compiled successfully with checkpointer!\")\n",
        "    return app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zv3e2NACtZmH"
      },
      "outputs": [],
      "source": [
        "# # Optionally, visualize the graph.\n",
        "# from IPython.display import Image, display\n",
        "\n",
        "# display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ZSBZa3whwN"
      },
      "source": [
        "## Run all experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "houEc6ApwhwO"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "import re\n",
        "import uuid\n",
        "\n",
        "def run_all_experiments(\n",
        "    app_instance,\n",
        "    system_briefs_data: Dict[str, str]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Runs the LangGraph C4 model generation for each system brief\n",
        "    and collects the results.\n",
        "\n",
        "    Args:\n",
        "        app_instance: The compiled LangGraph application.\n",
        "        system_briefs_data (Dict[str, str]): A dictionary where keys are brief names\n",
        "                                              and values are the system brief content strings.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n",
        "                              contains the results of a single experiment run,\n",
        "                              including 'brief_name', 'thread_id', 'system_brief_content',\n",
        "                              and 'final_c4_model'.\n",
        "    \"\"\"\n",
        "    experiment_results = []\n",
        "    print(\"\\n--- ðŸš€ Starting C4 Model Generation Experiments ---\")\n",
        "\n",
        "    for brief_name, system_brief_content in system_briefs_data.items():\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"--- Processing: {brief_name} ---\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        initial_state = {\n",
        "            \"messages\": [],\n",
        "            \"system_brief\": system_brief_content,\n",
        "            \"c4_model\": {\"context\": {}, \"containers\": {}, \"components\": {}},\n",
        "            \"component_queue\": None,\n",
        "        }\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        brief_name_slug = re.sub(r'[^a-zA-Z0-9-]', '', brief_name.replace(\" \", \"-\").lower())\n",
        "        current_thread_id = f\"{timestamp}-{brief_name_slug}-{uuid.uuid4().hex[:8]}\"\n",
        "        config = {\"configurable\": {\"thread_id\": current_thread_id}, \"recursion_limit\": 200}\n",
        "\n",
        "        print(f\"\\n--- LangGraph Thread ID: {current_thread_id} ---\")\n",
        "\n",
        "        for event in app_instance.stream(initial_state, config):\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(f\"Node: {list(event.keys())[0]}\")\n",
        "            print(\"=\"*40)\n",
        "\n",
        "        final_state_snapshot = app_instance.get_state(config)\n",
        "        final_c4_model = final_state_snapshot.values['c4_model']\n",
        "\n",
        "        experiment_results.append({\n",
        "            'brief_name': brief_name,\n",
        "            'thread_id': current_thread_id,\n",
        "            'system_brief_content': system_brief_content,\n",
        "            'final_c4_model': final_c4_model\n",
        "        })\n",
        "\n",
        "        print(f\"\\n--- ðŸŽ‰ C4 Model Generation Complete for {brief_name}! ---\")\n",
        "        print(f\"Final state for thread '{current_thread_id}' retrieved and stored.\")\n",
        "\n",
        "    print(\"\\n\\n--- âœ… All C4 Model Generation Experiments Complete! ---\")\n",
        "    return experiment_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XYVOOgzsxL6v"
      },
      "outputs": [],
      "source": [
        "# Run all the experiments to generate C4 models\n",
        "# all_experiment_raw_results = run_all_experiments(\n",
        "#     app_instance=app,\n",
        "#     system_briefs_data=system_briefs\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Hj9Do8V-iM"
      },
      "source": [
        "### Saving to zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4GJvs1Jy2CuG"
      },
      "outputs": [],
      "source": [
        "# --- RETRIEVE FINAL STATE AND SAVE ARTIFACTS TO FILES ---\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "def save_c4_artifacts(output_dir, final_c4_model):\n",
        "    \"\"\"\n",
        "    Saves C4 model artifacts (analysis, definitions, diagrams) to the specified output directory.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): The base directory where artifacts will be saved.\n",
        "        final_c4_model (dict): A dictionary containing the C4 model data,\n",
        "                               including 'context', 'containers', and 'components' sections.\n",
        "    \"\"\"\n",
        "\n",
        "    def save_artifact(filepath, content):\n",
        "        \"\"\"\n",
        "        Safely saves content to a file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "            print(f\"  - Saved {os.path.basename(filepath)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  - Failed to save {os.path.basename(filepath)}: {e}\")\n",
        "\n",
        "    def sanitize_filename(name):\n",
        "        \"\"\"\n",
        "        Replaces non-alphanumeric characters with underscores for safe filenames.\n",
        "        \"\"\"\n",
        "        return re.sub(r'[^a-zA-Z0-9_-]', '_', name).lower()\n",
        "\n",
        "    # Save Context level artifacts\n",
        "    if final_c4_model.get(\"context\"):\n",
        "        save_artifact(f\"{output_dir}/1_context_analysis.md\", final_c4_model[\"context\"].get(\"analysis\", \"\"))\n",
        "        save_artifact(f\"{output_dir}/1_context_definition.yaml\", final_c4_model[\"context\"].get(\"yaml_definition\", \"\"))\n",
        "        save_artifact(f\"{output_dir}/1_context_diagram.puml\", final_c4_model[\"context\"].get(\"diagram\", \"\"))\n",
        "\n",
        "    # Save Container level artifacts\n",
        "    if final_c4_model.get(\"containers\"):\n",
        "        save_artifact(f\"{output_dir}/2_container_analysis.md\", final_c4_model[\"containers\"].get(\"analysis\", \"\"))\n",
        "        save_artifact(f\"{output_dir}/2_container_definition.yaml\", final_c4_model[\"containers\"].get(\"yaml_definition\", \"\"))\n",
        "        save_artifact(f\"{output_dir}/2_container_diagram.puml\", final_c4_model[\"containers\"].get(\"diagram\", \"\"))\n",
        "\n",
        "    # Save Component level artifacts\n",
        "    if final_c4_model.get(\"components\"):\n",
        "        component_dir = f\"{output_dir}/3_components\"\n",
        "        os.makedirs(component_dir, exist_ok=True)\n",
        "        for container_name, component_data in final_c4_model[\"components\"].items():\n",
        "            safe_container_name = sanitize_filename(container_name)\n",
        "            save_artifact(f\"{component_dir}/{safe_container_name}_analysis.md\", component_data.get(\"analysis\", \"\"))\n",
        "            save_artifact(f\"{component_dir}/{safe_container_name}_definition.yaml\", component_data.get(\"yaml_definition\", \"\"))\n",
        "            save_artifact(f\"{component_dir}/{safe_container_name}_diagram.puml\", component_data.get(\"diagram\", \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hwgosIl1sruP"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# folder_to_zip = 'c4_artifacts'\n",
        "# zip_filename = 'c4_model_artifacts.zip'\n",
        "\n",
        "# print(f\"Zipping the folder: '{folder_to_zip}'...\")\n",
        "\n",
        "# try:\n",
        "#     shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "#     print(f\"Successfully created zip file: '{zip_filename}'\")\n",
        "#     print(\"Starting download... Please wait.\")\n",
        "#     print(\"Note: If the download doesn't start, check if your browser is blocking pop-ups for this site.\")\n",
        "\n",
        "#     files.download(zip_filename)\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Error: The directory '{folder_to_zip}' was not found. Please ensure the previous script ran successfully.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zMp9TOZ9KTF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJZevNFBBwM1"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7H0qA9lj_9b"
      },
      "source": [
        "## Evaluation agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVDycKMLyX--"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uzq8FQp3ByRc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import uuid\n",
        "import requests\n",
        "import subprocess\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Use a powerful, unbiased model as the judge\n",
        "# judge_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "judge_llm = llm\n",
        "\n",
        "# Helper to ensure plantuml.jar exists for the automated check\n",
        "PLANTUML_JAR_URL = \"https://github.com/plantuml/plantuml/releases/download/v1.2024.5/plantuml-1.2024.5.jar\"\n",
        "PLANTUML_JAR_PATH = \"plantuml.jar\"\n",
        "\n",
        "def setup_plantuml():\n",
        "    \"\"\"Downloads plantuml.jar if it doesn't exist.\"\"\"\n",
        "    if not os.path.exists(PLANTUML_JAR_PATH):\n",
        "        print(f\"Downloading PlantUML runner from {PLANTUML_JAR_URL}...\")\n",
        "        try:\n",
        "            response = requests.get(PLANTUML_JAR_URL, stream=True)\n",
        "            response.raise_for_status()\n",
        "            with open(PLANTUML_JAR_PATH, \"wb\") as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(\"PlantUML runner downloaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading PlantUML: {e}\")\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZ4YpkMjClr"
      },
      "source": [
        "#### Compilation success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5DQ0c9Z8C-Zb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import subprocess\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Assume setup_plantuml() and PLANTUML_JAR_PATH are defined elsewhere\n",
        "\n",
        "def evaluate_compilation_success(c4_model: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Calculates the percentage of diagrams that compile and captures detailed\n",
        "    diagnostics for each failure.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ¤– Evaluating Metric: PlantUML Compilation Success...\")\n",
        "    if not setup_plantuml():\n",
        "        return {\"error\": \"PlantUML runner not available.\"}\n",
        "\n",
        "    # --- NEW: Collect diagrams as a list of dicts to track their source ---\n",
        "    diagram_sources = []\n",
        "    if c4_model.get(\"context\", {}).get(\"diagram\") is not None:\n",
        "        diagram_sources.append({\"source\": \"1_Context\", \"code\": c4_model[\"context\"][\"diagram\"]})\n",
        "    if c4_model.get(\"containers\", {}).get(\"diagram\") is not None:\n",
        "        diagram_sources.append({\"source\": \"2_Containers\", \"code\": c4_model[\"containers\"][\"diagram\"]})\n",
        "    for name, comp in c4_model.get(\"components\", {}).items():\n",
        "        if comp.get(\"diagram\") is not None:\n",
        "            diagram_sources.append({\"source\": f\"3_Component_{name}\", \"code\": comp[\"diagram\"]})\n",
        "\n",
        "    total_files = len(diagram_sources)\n",
        "    if total_files == 0:\n",
        "        return {\"score\": 0, \"successful\": 0, \"total\": 0, \"details\": []}\n",
        "\n",
        "    compilation_details = []\n",
        "    successful_compilations = 0\n",
        "    temp_filename = f\"temp_diagram_{uuid.uuid4()}.puml\"\n",
        "\n",
        "    for diagram in diagram_sources:\n",
        "        source_name = diagram[\"source\"]\n",
        "        puml_code = diagram[\"code\"]\n",
        "\n",
        "        if not puml_code or not puml_code.strip():\n",
        "            print(f\"  - âŒ FAILED: '{source_name}' content is empty.\")\n",
        "            compilation_details.append({\"source\": source_name, \"status\": \"Failed - Empty\", \"error\": \"Diagram content was empty or whitespace.\"})\n",
        "            continue\n",
        "\n",
        "        with open(temp_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(puml_code)\n",
        "\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"java\", \"-jar\", PLANTUML_JAR_PATH, \"-failfast2\", \"-tsvg\", temp_filename],\n",
        "                check=True, capture_output=True, text=True\n",
        "            )\n",
        "            successful_compilations += 1\n",
        "            compilation_details.append({\"source\": source_name, \"status\": \"Compiled\", \"error\": None})\n",
        "            print(f\"  - âœ… SUCCESS: '{source_name}' compiled.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            # Capture the actual error message from PlantUML\n",
        "            error_message = e.stderr.strip()\n",
        "            compilation_details.append({\"source\": source_name, \"status\": \"Failed - Syntax Error\", \"error\": error_message})\n",
        "            print(f\"  - âŒ FAILED: '{source_name}' syntax error.\")\n",
        "        except FileNotFoundError:\n",
        "            # Handle cases where java or the jar file isn't found\n",
        "            return {\"error\": \"Java or PlantUML JAR not found. Cannot perform compilation check.\"}\n",
        "        finally:\n",
        "            if os.path.exists(temp_filename):\n",
        "                os.remove(temp_filename)\n",
        "            svg_filename = temp_filename.replace(\".puml\", \".svg\")\n",
        "            if os.path.exists(svg_filename):\n",
        "                os.remove(svg_filename)\n",
        "\n",
        "    score = (successful_compilations / total_files) * 100 if total_files > 0 else 0\n",
        "    return {\n",
        "        \"metric\": \"Compilation Success Rate\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"successful\": successful_compilations,\n",
        "        \"total\": total_files,\n",
        "        \"details\": compilation_details  # <<< NEW: Rich diagnostic details\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Bev4RIME11qW"
      },
      "outputs": [],
      "source": [
        "# evaluate_compilation_success(all_experiment_raw_results[0]['final_c4_model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkG4l_XjFXi"
      },
      "source": [
        "#### Abstraction adherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0-Gysr5nEjx0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DEDICATED & DETAILED RULE CHECKERS\n",
        "# These now return a tuple: (bool, str) -> (is_valid, reason)\n",
        "# ==============================================================================\n",
        "\n",
        "def _check_context_rules(code: str) -> Tuple[bool, str]:\n",
        "    \"\"\"Checks rules for a C4 Context Diagram.\"\"\"\n",
        "    if re.search(r'Container\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Illegal 'Container' element found in a Context diagram.\")\n",
        "    if re.search(r'Component\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Illegal 'Component' element found in a Context diagram.\")\n",
        "    # A context diagram should generally define the main system\n",
        "    if not (re.search(r'System\\s*\\(', code, re.IGNORECASE) or re.search(r'SystemDb\\s*\\(', code, re.IGNORECASE) or re.search(r'System_Ext\\s*\\(', code, re.IGNORECASE)):\n",
        "        return (False, \"Required 'System', 'SystemDb', or 'System_Ext' element appears to be missing.\")\n",
        "    return (True, \"Adheres to abstraction level.\")\n",
        "\n",
        "def _check_container_rules(code: str) -> Tuple[bool, str]:\n",
        "    \"\"\"Checks rules for a C4 Container Diagram.\"\"\"\n",
        "    if re.search(r'Component\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Illegal 'Component' element found in a Container diagram.\")\n",
        "    if not re.search(r'Container\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Required 'Container' element appears to be missing.\")\n",
        "    if not re.search(r'System_Boundary\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Required 'System_Boundary' element appears to be missing.\")\n",
        "    return (True, \"Adheres to abstraction level.\")\n",
        "\n",
        "def _check_component_rules(code: str) -> Tuple[bool, str]:\n",
        "    \"\"\"Checks rules for a C4 Component Diagram.\"\"\"\n",
        "    if not re.search(r'Component\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Required 'Component' element appears to be missing.\")\n",
        "    if not re.search(r'Container_Boundary\\s*\\(', code, re.IGNORECASE):\n",
        "        return (False, \"Required 'Container_Boundary' element appears to be missing.\")\n",
        "    return (True, \"Adheres to abstraction level.\")\n",
        "\n",
        "\n",
        "def evaluate_abstraction_adherence(c4_model: dict) -> dict:\n",
        "    \"\"\"Checks if each diagram uses PlantUML elements appropriate for its C4 level.\"\"\"\n",
        "    print(\"ðŸ¤– Evaluating Metric: C4 Abstraction Adherence...\")\n",
        "\n",
        "    # --- A dispatcher mapping diagram types to their rule checkers ---\n",
        "    RULE_CHECKERS = {\n",
        "        'Context': _check_context_rules,\n",
        "        'Containers': _check_container_rules,\n",
        "        'Component': _check_component_rules,\n",
        "    }\n",
        "\n",
        "    # --- Collect all diagrams to be checked ---\n",
        "    diagrams_to_check = []\n",
        "    if c4_model.get(\"context\", {}).get(\"diagram\"):\n",
        "        diagrams_to_check.append({\"type\": \"Context\", \"name\": \"Context\", \"code\": c4_model[\"context\"][\"diagram\"]})\n",
        "    if c4_model.get(\"containers\", {}).get(\"diagram\"):\n",
        "        diagrams_to_check.append({\"type\": \"Containers\", \"name\": \"Containers\", \"code\": c4_model[\"containers\"][\"diagram\"]})\n",
        "    for name, comp_data in c4_model.get(\"components\", {}).items():\n",
        "        if comp_data.get(\"diagram\"):\n",
        "            diagrams_to_check.append({\"type\": \"Component\", \"name\": f\"Component: {name}\", \"code\": comp_data[\"diagram\"]})\n",
        "\n",
        "    total_diagrams = len(diagrams_to_check)\n",
        "    if total_diagrams == 0:\n",
        "        return {\"score\": 0, \"details\": {}}\n",
        "\n",
        "    passes = 0\n",
        "    detailed_results = {}\n",
        "\n",
        "    for diagram in diagrams_to_check:\n",
        "        diagram_type = diagram[\"type\"]\n",
        "        diagram_name = diagram[\"name\"]\n",
        "        diagram_code = diagram[\"code\"]\n",
        "\n",
        "        # Find the correct checker function from the dispatcher\n",
        "        checker_func = RULE_CHECKERS.get(diagram_type)\n",
        "\n",
        "        if not checker_func:\n",
        "            # Should not happen with this structure, but it's safe to have\n",
        "            detailed_results[diagram_name] = {\"status\": \"Unknown\", \"reason\": \"No rule checker found for this diagram type.\"}\n",
        "            continue\n",
        "\n",
        "        is_valid, reason = checker_func(diagram_code)\n",
        "\n",
        "        if is_valid:\n",
        "            passes += 1\n",
        "\n",
        "        detailed_results[diagram_name] = {\n",
        "            \"status\": \"Pass\" if is_valid else \"Fail\",\n",
        "            \"reason\": reason\n",
        "        }\n",
        "\n",
        "    score = (passes / total_diagrams) * 100 if total_diagrams > 0 else 0\n",
        "    return {\n",
        "        \"metric\": \"Abstraction Adherence\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"details\": detailed_results # <<< NEW: Rich diagnostic details\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDXp-_rGjH-C"
      },
      "source": [
        "#### Semantic consitency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0vEE_FDDEl-U"
      },
      "outputs": [],
      "source": [
        "def evaluate_semantic_consistency(system_brief: str, c4_model: dict, judge_llm) -> dict:\n",
        "    \"\"\"Evaluates how well the diagrams capture entities from the input brief.\"\"\"\n",
        "    print(\"âš–ï¸ Evaluating Metric 3: Semantic Consistency...\")\n",
        "    context_diag = c4_model.get(\"context\", {}).get(\"diagram\")\n",
        "    if not context_diag:\n",
        "        return {\"error\": \"Context diagram not found.\"}\n",
        "\n",
        "    # Step 1: Extract \"ground truth\" entities from the brief\n",
        "    extraction_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a requirements analyst. Your task is to extract key entities from a system description. List all people (user roles), external systems, and the main system itself.\"),\n",
        "        (\"human\", \"Please extract the entities from the following brief:\\n\\n{brief}\")\n",
        "    ])\n",
        "    extraction_chain = extraction_prompt | judge_llm | StrOutputParser()\n",
        "    extracted_items_str = extraction_chain.invoke({\"brief\": system_brief})\n",
        "\n",
        "    # Simple parsing of the extracted items\n",
        "    extracted_items = [item.strip() for item in extracted_items_str.split('\\n') if item.strip()]\n",
        "\n",
        "    # Step 2: Verify the extracted items against the diagram\n",
        "    verification_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a meticulous verifier. For each item in the checklist, check if it is clearly represented in the provided PlantUML diagram. Respond with only 'YES' or 'NO' for each item.\"),\n",
        "        (\"human\", \"\"\"**Checklist:**\n",
        "                   {checklist}\n",
        "\n",
        "                   **PlantUML Diagram:**\n",
        "                   ```puml\n",
        "                   {diagram}\n",
        "                   ```\"\"\")\n",
        "    ])\n",
        "    verification_chain = verification_prompt | judge_llm | StrOutputParser()\n",
        "    verification_results_str = verification_chain.invoke({\n",
        "        \"checklist\": \"\\n\".join(f\"- {item}\" for item in extracted_items),\n",
        "        \"diagram\": context_diag\n",
        "    })\n",
        "\n",
        "    verified_count = verification_results_str.upper().count(\"YES\")\n",
        "    total_items = len(extracted_items)\n",
        "    score = (verified_count / total_items) * 100 if total_items > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"metric\": \"Semantic Consistency\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"verified_items\": verified_count,\n",
        "        \"total_items\": total_items\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "n6VxfejXgW7q"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import re\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def evaluate_definitional_consistency(\n",
        "    yaml_definition_str: str,\n",
        "    diagram_code_str: str,\n",
        "    element_type: str # e.g., 'container' or 'component'\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Checks if all elements defined in a YAML spec are present in a PlantUML diagram.\n",
        "    This is used to ensure consistency between a definition and its visualization.\n",
        "\n",
        "    Args:\n",
        "        yaml_definition_str: The string content of the YAML definition file.\n",
        "        diagram_code_str: The string content of the PlantUML diagram file.\n",
        "        element_type: The key to look for in the YAML file (e.g., 'containers' or 'components').\n",
        "    \"\"\"\n",
        "    if not yaml_definition_str or not diagram_code_str:\n",
        "        return {\"score\": 0, \"details\": {\"error\": f\"Missing YAML definition or diagram for {element_type}.\"}}\n",
        "\n",
        "    try:\n",
        "        # 1. Parse the YAML to get the \"ground truth\" list of names\n",
        "        definition_data = yaml.safe_load(yaml_definition_str)\n",
        "        # The structure might be {'system': {'containers': [...]}} or just {'containers': [...]}\n",
        "        # We'll navigate down to find the list of elements.\n",
        "        elements = definition_data.get(element_type) or definition_data.get('system', {}).get(element_type, [])\n",
        "\n",
        "        if not isinstance(elements, list):\n",
        "             return {\"score\": 0, \"details\": {\"error\": f\"Could not find a list of '{element_type}' in the YAML.\"}}\n",
        "\n",
        "        defined_names = [item['name'] for item in elements if 'name' in item]\n",
        "\n",
        "        if not defined_names:\n",
        "            return {\"score\": 100, \"details\": {\"message\": f\"No {element_type}s with a 'name' key found in the YAML definition.\"}}\n",
        "\n",
        "        # 2. Verify each name is present in the diagram\n",
        "        found_count = 0\n",
        "        verification_details = []\n",
        "        for name in defined_names:\n",
        "            # Check if the name appears within a PlantUML element definition, e.g., Container(name, ...)\n",
        "            # This regex is a simple but effective check.\n",
        "            if re.search(fr'\\(\\s*\"{re.escape(name)}\"\\s*,', diagram_code_str) or re.search(fr'\\(\\s*{re.escape(name)}\\s*,', diagram_code_str):\n",
        "                found_count += 1\n",
        "                verification_details.append({\"element_name\": name, \"status\": \"Found\"})\n",
        "            else:\n",
        "                verification_details.append({\"element_name\": name, \"status\": \"Missing\"})\n",
        "\n",
        "        total_defined = len(defined_names)\n",
        "        score = (found_count / total_defined) * 100 if total_defined > 0 else 100\n",
        "\n",
        "        return {\n",
        "            \"metric\": f\"{element_type.capitalize()} Definitional Consistency\",\n",
        "            \"score\": round(score, 2),\n",
        "            \"found_count\": found_count,\n",
        "            \"total_defined\": total_defined,\n",
        "            \"details\": verification_details\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to process definitional consistency check: {e}\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgzYWNLjJ4n"
      },
      "source": [
        "#### Qualitative rubric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Fi-1hYPHEo4U"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def evaluate_qualitative_rubric(\n",
        "    diagram_code: str,\n",
        "    diagram_name: str,\n",
        "    system_brief: str, # <<< NEW: The original system requirements\n",
        "    judge_llm\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Scores a single diagram based on a qualitative rubric using an LLM-as-a-Judge,\n",
        "    providing the judge with the necessary system brief for context.\n",
        "    \"\"\"\n",
        "    print(f\"âš–ï¸ Evaluating Metric: Qualitative Rubric for {diagram_name}...\")\n",
        "\n",
        "    # --- 1. JSON Schema is unchanged ---\n",
        "    rubric_schema = {\n",
        "        \"title\": \"QualitativeRubricEvaluation\",\n",
        "        # ... (schema remains identical)\n",
        "    }\n",
        "\n",
        "    # --- 2. Update the prompt to include the system_brief ---\n",
        "    rubric_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are an expert software architect acting as a judge. Your task is to evaluate the provided C4 diagram against the requirements of the original system brief. Provide a score from 1 (poor) to 5 (excellent) for each criterion, along with a brief justification. You must format your response as a JSON object that adheres to the provided schema.\"),\n",
        "        # <<< CHANGED: The human message now includes the system brief >>>\n",
        "        (\"human\", \"\"\"\n",
        "        **System Brief to Reference:**\n",
        "        ---\n",
        "        {system_brief}\n",
        "        ---\n",
        "\n",
        "        **Rubric:**\n",
        "        | Criterion         | 1 (Poor)                                                    | 3 (Average)                                                    | 5 (Excellent)                                                                      |\n",
        "        |-------------------|-------------------------------------------------------------|----------------------------------------------------------------|------------------------------------------------------------------------------------|\n",
        "        | **Completeness** | The diagram is missing core elements from the system brief. | The diagram includes most major elements but misses minor details. | All key entities and relationships required by the system brief are present.       |\n",
        "        | **Correctness** | Relationships are illogical or incorrectly depicted.        | Relationships are mostly correct but have minor inaccuracies.    | All relationships are logical, well-defined, and correctly directed.               |\n",
        "        | **Plausibility** | The architecture is unrealistic for the system's goals.     | The architecture is plausible but might be suboptimal.           | The architecture is highly plausible and follows common industry patterns.         |\n",
        "        | **Clarity & Naming**| Element names are vague, inconsistent, or use jargon poorly.| Names are understandable but could be clearer.                 | Names are clear, concise, and follow standard naming conventions.                  |\n",
        "\n",
        "        **Diagram to Evaluate:**\n",
        "        ```puml\n",
        "        {diagram}\n",
        "        ```\n",
        "\n",
        "        Please provide your JSON response now.\n",
        "        \"\"\")\n",
        "    ])\n",
        "\n",
        "    # --- 3. Bind the schema to the LLM (unchanged) ---\n",
        "    structured_judge_llm = judge_llm.with_structured_output(rubric_schema)\n",
        "    rubric_chain = rubric_prompt | structured_judge_llm\n",
        "\n",
        "    # --- 4. Invoke the chain with the new variable ---\n",
        "    try:\n",
        "        # <<< CHANGED: Pass the system_brief into the invoke call >>>\n",
        "        results = rubric_chain.invoke({\n",
        "            \"diagram\": diagram_code,\n",
        "            \"system_brief\": system_brief\n",
        "        })\n",
        "        scores = [v['score'] for v in results.values()]\n",
        "        average_score = sum(scores) / len(scores) if scores else 0\n",
        "        return {\n",
        "            \"diagram_name\": diagram_name,\n",
        "            \"average_score\": round(average_score, 2),\n",
        "            \"details\": results\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to get rubric score: {e}\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyrfkw2ljN1b"
      },
      "source": [
        "#### Cross level consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Tmt-raPZKBo0"
      },
      "outputs": [],
      "source": [
        "import yaml # Requires PyYAML: pip install pyyaml\n",
        "from typing import Dict, List, Set, Any\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def parse_yaml_safe(yaml_string: str) -> Dict:\n",
        "    \"\"\"Safely parses a YAML string, returning an empty dict on error.\"\"\"\n",
        "    if not yaml_string:\n",
        "        return {}\n",
        "    try:\n",
        "        return yaml.safe_load(yaml_string) or {}\n",
        "    except yaml.YAMLError:\n",
        "        return {}\n",
        "\n",
        "def extract_element_names(parsed_yaml: Dict, element_types: List[str]) -> Set[str]:\n",
        "    \"\"\"Extracts a set of names for given element types from parsed YAML data.\"\"\"\n",
        "    names = set()\n",
        "    for element in parsed_yaml.get(\"elements\", []):\n",
        "        if element.get(\"type\") in element_types:\n",
        "            names.add(element.get(\"name\"))\n",
        "    return names\n",
        "\n",
        "# --- Main Evaluation Function ---\n",
        "\n",
        "def _check_context_to_container(context_data: Dict, container_data: Dict) -> Tuple[bool, str]:\n",
        "    \"\"\"Performs a two-way consistency check between Context and Container levels.\"\"\"\n",
        "    context_externals = extract_element_names(context_data, [\"person\", \"externalSystem\"])\n",
        "    container_externals = extract_element_names(container_data, [\"person\", \"externalSystem\"])\n",
        "\n",
        "    # Check 1: Elements illegally added to the container diagram\n",
        "    added_elements = container_externals - context_externals\n",
        "    # Check 2: Elements incorrectly missing from the container diagram\n",
        "    missing_elements = context_externals - container_externals\n",
        "\n",
        "    errors = []\n",
        "    if added_elements:\n",
        "        errors.append(f\"Illegally added elements not found in Context: {list(added_elements)}\")\n",
        "    if missing_elements:\n",
        "        errors.append(f\"Elements from Context missing from diagram: {list(missing_elements)}\")\n",
        "\n",
        "    if not errors:\n",
        "        return (True, \"External elements are consistent with the Context level.\")\n",
        "    else:\n",
        "        return (False, \" \".join(errors))\n",
        "\n",
        "def _check_container_to_components(container_data: Dict, component_data_map: Dict) -> Dict:\n",
        "    \"\"\"Checks each component diagram for consistency with the container level.\"\"\"\n",
        "    check_results = {}\n",
        "    container_known_elements = extract_element_names(container_data, [\"container\", \"person\", \"externalSystem\", \"database\"])\n",
        "\n",
        "    for comp_name, comp_data in component_data_map.items():\n",
        "        if not comp_data:\n",
        "            continue\n",
        "\n",
        "        # A component can reference other containers, people, or external systems.\n",
        "        comp_referenced_externals = extract_element_names(comp_data, [\"container\", \"person\", \"externalSystem\", \"database\", \"component\"])\n",
        "\n",
        "        # The logic here is correct: a component cannot reference something unknown at the container level.\n",
        "        mismatched_elements = comp_referenced_externals - container_known_elements\n",
        "\n",
        "        key = f\"Container->Component ({comp_name})\"\n",
        "        if not mismatched_elements:\n",
        "            check_results[key] = {\"status\": \"Pass\", \"reason\": \"All references are consistent with the Container level.\"}\n",
        "        else:\n",
        "            check_results[key] = {\n",
        "                \"status\": \"Fail\",\n",
        "                \"reason\": f\"References elements not found in Container scope: {list(mismatched_elements)}\"\n",
        "            }\n",
        "    return check_results\n",
        "\n",
        "# --- Main Evaluation Function (Now much cleaner) ---\n",
        "\n",
        "def evaluate_cross_level_consistency(c4_model: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"Measures two-way consistency of elements across C4 levels.\"\"\"\n",
        "    print(\"ðŸ¤– Evaluating Metric: Cross-Level Consistency Check...\")\n",
        "\n",
        "    # 1. Parse all relevant YAML definitions\n",
        "    context_data = parse_yaml_safe(c4_model.get(\"context\", {}).get(\"yaml_definition\"))\n",
        "    container_data = parse_yaml_safe(c4_model.get(\"containers\", {}).get(\"yaml_definition\"))\n",
        "    component_data_map = {\n",
        "        name: parse_yaml_safe(data.get(\"yaml_definition\", \"\"))\n",
        "        for name, data in c4_model.get(\"components\", {}).items()\n",
        "    }\n",
        "\n",
        "    all_details = {}\n",
        "\n",
        "    # 2. Run Context -> Container Check\n",
        "    if context_data and container_data:\n",
        "        is_consistent, reason = _check_context_to_container(context_data, container_data)\n",
        "        all_details[\"Context->Container\"] = {\"status\": \"Pass\" if is_consistent else \"Fail\", \"reason\": reason}\n",
        "\n",
        "    # 3. Run Container -> Component Checks\n",
        "    if container_data and component_data_map:\n",
        "        component_results = _check_container_to_components(container_data, component_data_map)\n",
        "        all_details.update(component_results)\n",
        "\n",
        "    # 4. Calculate Final Score\n",
        "    total_checks = len(all_details)\n",
        "    passed_checks = sum(1 for result in all_details.values() if result[\"status\"] == \"Pass\")\n",
        "    score = (passed_checks / total_checks) * 100 if total_checks > 0 else 100\n",
        "\n",
        "    return {\n",
        "        \"metric\": \"Cross-Level Consistency\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"passed\": passed_checks,\n",
        "        \"total\": total_checks,\n",
        "        \"details\": all_details\n",
        "    }\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Assuming 'final_c4_model' is the dictionary containing all generated artifacts\n",
        "\n",
        "# consistency_report = evaluate_cross_level_consistency(final_c4_model)\n",
        "# import json\n",
        "# print(json.dumps(consistency_report, indent=2))|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHdy--5CjRca"
      },
      "source": [
        "#### Emergent naming consitency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1YgAE9b5KyOz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import yaml\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Any, Set\n",
        "\n",
        "# --- Helper Function (reused) ---\n",
        "\n",
        "def parse_yaml_safe(yaml_string: str) -> Dict:\n",
        "    \"\"\"Safely parses a YAML string, returning an empty dict on error.\"\"\"\n",
        "    if not yaml_string:\n",
        "        return {}\n",
        "    try:\n",
        "        return yaml.safe_load(yaml_string) or {}\n",
        "    except yaml.YAMLError:\n",
        "        return {}\n",
        "\n",
        "# --- Main Evaluation Function (New Logic) ---\n",
        "\n",
        "def evaluate_emergent_naming_consistency(c4_model: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Measures the internal consistency of naming conventions by first detecting\n",
        "    the dominant convention and then identifying outliers.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ¤– Evaluating Metric 6 (New): Emergent Naming Consistency...\")\n",
        "\n",
        "    # 1. Define known patterns to classify names\n",
        "    PATTERNS = {\n",
        "        \"PascalCase\": r'^(?:[A-Z][a-z0-9]+)+$',\n",
        "        \"camelCase\": r'^[a-z]+(?:[A-Z][a-z0-9]+)*$',\n",
        "        \"snake_case\": r'^[a-z0-9]+(?:_[a-z0-9]+)*$',\n",
        "        \"kebab-case\": r'^[a-z0-9]+(?:-[a-z0-9]+)*$'\n",
        "    }\n",
        "\n",
        "    def classify_name(name: str) -> str:\n",
        "        \"\"\"Classifies a name into a known convention or 'other'.\"\"\"\n",
        "        if not name:\n",
        "            return \"missing\"\n",
        "        for convention, pattern in PATTERNS.items():\n",
        "            if re.match(pattern, name):\n",
        "                return convention\n",
        "        return \"other\"\n",
        "\n",
        "    # 2. Collect all relevant elements to check (same logic as before)\n",
        "    elements_to_check = []\n",
        "    context_data = parse_yaml_safe(c4_model.get(\"context\", {}).get(\"yaml_definition\"))\n",
        "    if context_data.get(\"system\"):\n",
        "        elements_to_check.append({\"type\": \"system\", \"name\": context_data[\"system\"].get(\"name\", \"\")})\n",
        "\n",
        "    container_data = parse_yaml_safe(c4_model.get(\"containers\", {}).get(\"yaml_definition\"))\n",
        "    for element in container_data.get(\"elements\", []):\n",
        "        if element.get(\"type\") == \"container\":\n",
        "            elements_to_check.append({\"type\": \"container\", \"name\": element.get(\"name\", \"\")})\n",
        "\n",
        "    component_yamls = {n: d.get(\"yaml_definition\") for n, d in c4_model.get(\"components\", {}).items()}\n",
        "    for comp_name, yaml_str in component_yamls.items():\n",
        "        comp_data = parse_yaml_safe(yaml_str)\n",
        "        for element in comp_data.get(\"elements\", []):\n",
        "            if element.get(\"type\") == \"component\":\n",
        "                elements_to_check.append({\"type\": f\"component (in {comp_name})\", \"name\": element.get(\"name\", \"\")})\n",
        "\n",
        "    # 3. Handle case with no elements\n",
        "    total_elements = len(elements_to_check)\n",
        "    if total_elements == 0:\n",
        "        return {\"metric\": \"Emergent Naming Consistency\", \"score\": 100.0, \"details\": \"No elements found to evaluate.\"}\n",
        "\n",
        "    # 4. Classify all names and find the dominant convention\n",
        "    classifications = [classify_name(elem.get(\"name\")) for elem in elements_to_check]\n",
        "    convention_counts = Counter(classifications)\n",
        "\n",
        "    dominant_convention = \"none\"\n",
        "    if convention_counts:\n",
        "        # Find the most common convention, ignoring 'other' or 'missing' if possible\n",
        "        top_conventions = [item for item in convention_counts.most_common() if item[0] not in [\"other\", \"missing\"]]\n",
        "        if top_conventions:\n",
        "            dominant_convention = top_conventions[0][0]\n",
        "        else: # Handle case where all names are 'other' or 'missing'\n",
        "            dominant_convention = convention_counts.most_common(1)[0][0]\n",
        "\n",
        "    # 5. Identify outliers and calculate consistency score\n",
        "    outliers = []\n",
        "    consistent_elements = 0\n",
        "    for i, element in enumerate(elements_to_check):\n",
        "        if classifications[i] == dominant_convention:\n",
        "            consistent_elements += 1\n",
        "        else:\n",
        "            outliers.append({\n",
        "                \"name\": element.get(\"name\", \"[MISSING NAME]\"),\n",
        "                \"type\": element[\"type\"],\n",
        "                \"detected_convention\": classifications[i],\n",
        "                \"reason\": f\"Deviates from the dominant convention: '{dominant_convention}'\"\n",
        "            })\n",
        "\n",
        "    # 6. Calculate Final Score\n",
        "    score = (consistent_elements / total_elements) * 100\n",
        "\n",
        "    return {\n",
        "        \"metric\": \"Emergent Naming Consistency\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"dominantConvention\": {\n",
        "            \"name\": dominant_convention,\n",
        "            \"count\": convention_counts.get(dominant_convention, 0),\n",
        "            \"total\": total_elements\n",
        "        },\n",
        "        \"allConventionCounts\": dict(convention_counts),\n",
        "        \"details\": {\"outliers\": outliers} if outliers else \"All names are internally consistent.\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_pmZaLEjUFD"
      },
      "source": [
        "#### Architect critique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "OrVLnWhwLSub"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "def evaluate_architect_critique(system_brief: str, c4_model: Dict, judge_llm) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a structured, qualitative critique of the entire C4 model from the\n",
        "    perspective of a Principal Software Architect.\n",
        "    \"\"\"\n",
        "    print(\"âš–ï¸ Evaluating Metric 7: Principal Architect's Critique...\")\n",
        "\n",
        "    # --- 1. Define the JSON Schema for a structured, comparable output ---\n",
        "    architect_critique_schema = {\n",
        "        \"title\": \"PrincipalArchitectCritique\",\n",
        "        \"description\": \"A senior-level review of a software architecture, providing both quantitative ratings and qualitative, narrative feedback.\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"executiveSummary\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"A high-level, one-paragraph summary of the architect's overall opinion of the design.\"\n",
        "            },\n",
        "            \"feasibilityAndSoundness\": {\n",
        "                \"type\": \"object\",\n",
        "                \"description\": \"Evaluation of the architecture's technical viability and alignment with non-functional requirements.\",\n",
        "                \"properties\": {\n",
        "                    \"rating\": {\"type\": \"integer\", \"description\": \"Overall rating for this category from 1 (poor) to 5 (excellent).\"},\n",
        "                    \"critique\": {\"type\": \"string\", \"description\": \"Detailed narrative explaining the rating, covering technology choices, scalability, and performance.\"},\n",
        "                    \"identifiedRisks\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"string\"},\n",
        "                        \"description\": \"A specific list of the top 2-3 architectural risks or potential bottlenecks.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"rating\", \"critique\", \"identifiedRisks\"]\n",
        "            },\n",
        "            \"clarityAndCommunication\": {\n",
        "                \"type\": \"object\",\n",
        "                \"description\": \"Evaluation of how well the diagrams communicate the architecture to different audiences.\",\n",
        "                \"properties\": {\n",
        "                    \"rating\": {\"type\": \"integer\", \"description\": \"Overall rating for this category from 1 (poor) to 5 (excellent).\"},\n",
        "                    \"critique\": {\"type\": \"string\", \"description\": \"Detailed narrative explaining the rating, commenting on the clarity of each diagram for its intended audience.\"}\n",
        "                },\n",
        "                \"required\": [\"rating\", \"critique\"]\n",
        "            },\n",
        "            \"actionableRecommendation\": {\n",
        "                \"type\": \"object\",\n",
        "                \"description\": \"The single most important recommendation for improving the design.\",\n",
        "                \"properties\": {\n",
        "                    \"recommendation\": {\"type\": \"string\", \"description\": \"A clear, concise statement of the primary recommendation.\"},\n",
        "                    \"justification\": {\"type\": \"string\", \"description\": \"The reasoning behind why this recommendation is critical.\"},\n",
        "                    \"priority\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"Critical\", \"High\", \"Medium\"],\n",
        "                        \"description\": \"The priority level of this recommendation.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"recommendation\", \"justification\", \"priority\"]\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"executiveSummary\", \"feasibilityAndSoundness\", \"clarityAndCommunication\", \"actionableRecommendation\"]\n",
        "    }\n",
        "\n",
        "    # --- 2. Aggregate ALL relevant input documents for the judge ---\n",
        "    # <<< CHANGED: Now includes both YAML definitions and PlantUML diagrams >>>\n",
        "    architecture_documents = []\n",
        "\n",
        "    # Context Level\n",
        "    architecture_documents.append(\"## Context Level Definition (YAML)\\n```yaml\\n\" + c4_model.get(\"context\", {}).get(\"yaml_definition\", \"Not available\") + \"\\n```\")\n",
        "    architecture_documents.append(\"\\n## Context Level Diagram (PlantUML)\\n```puml\\n\" + c4_model.get(\"context\", {}).get(\"diagram\", \"Not available\") + \"\\n```\")\n",
        "\n",
        "    # Container Level\n",
        "    architecture_documents.append(\"\\n\\n## Container Level Definition (YAML)\\n```yaml\\n\" + c4_model.get(\"containers\", {}).get(\"yaml_definition\", \"Not available\") + \"\\n```\")\n",
        "    architecture_documents.append(\"\\n## Container Level Diagram (PlantUML)\\n```puml\\n\" + c4_model.get(\"containers\", {}).get(\"diagram\", \"Not available\") + \"\\n```\")\n",
        "\n",
        "    # Component Level (include up to 2 for brevity)\n",
        "    component_count = 0\n",
        "    for name, data in c4_model.get(\"components\", {}).items():\n",
        "        if component_count < 2:\n",
        "            architecture_documents.append(f\"\\n\\n## Component Level: {name} (YAML)\\n```yaml\\n\" + data.get(\"yaml_definition\", \"Not available\") + \"\\n```\")\n",
        "            architecture_documents.append(f\"\\n## Component Level: {name} (PlantUML)\\n```puml\\n\" + data.get(\"diagram\", \"Not available\") + \"\\n```\")\n",
        "            component_count += 1\n",
        "\n",
        "    full_context = \"\\n\".join(architecture_documents)\n",
        "\n",
        "    # --- 3. Update the prompt to reflect the new context ---\n",
        "    critique_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a pragmatic Principal Software Architect... (persona is unchanged)\"\"\"),\n",
        "        # <<< CHANGED: Prompt now acknowledges both definitions and diagrams >>>\n",
        "        (\"human\", \"\"\"Please review the following architecture.\n",
        "\n",
        "        **Guiding Questions for Your Analysis:**\n",
        "        1.  **Feasibility & Soundness:** Based on the brief and the YAML definitions, are the technology choices realistic? Does the decomposition make sense for scalability and performance? Identify the biggest architectural risk.\n",
        "        2.  **Clarity & Communication:** Does this set of diagrams AND definitions effectively communicate the architecture? Is there a clear link between the definitions and the diagrams?\n",
        "        3.  **Actionable Recommendation:** What is the single most important change you would recommend to this design and why?\n",
        "\n",
        "        **System Design Brief:**\n",
        "        ```\n",
        "        {brief}\n",
        "        ```\n",
        "\n",
        "        **Generated C4 Architecture (Definitions & Diagrams):**\n",
        "        {architecture_docs}\n",
        "\n",
        "        Provide your structured JSON response now.\n",
        "        \"\"\")\n",
        "    ])\n",
        "\n",
        "    structured_judge_llm = judge_llm.with_structured_output(architect_critique_schema)\n",
        "    critique_chain = critique_prompt | structured_judge_llm\n",
        "\n",
        "    # --- 4. Invoke the chain with the enriched context ---\n",
        "    try:\n",
        "        critique = critique_chain.invoke({\n",
        "            \"brief\": system_brief,\n",
        "            \"architecture_docs\": full_context\n",
        "        })\n",
        "        return {\n",
        "            \"metric\": \"Principal Architect's Critique\",\n",
        "            \"critique\": critique\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to get architect's critique: {e}\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-aGOYFPjXBa"
      },
      "source": [
        "#### Security assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cBb3PCGXLwdr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "def evaluate_security_assessment(system_brief: str, c4_model: Dict, judge_llm) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a threat modeling assessment on the container diagram from the\n",
        "    perspective of a cybersecurity expert.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ›¡ï¸  Evaluating Metric 8: Security 'Red Team' Assessment...\")\n",
        "\n",
        "    container_diag = c4_model.get(\"containers\", {}).get(\"diagram\")\n",
        "    if not container_diag:\n",
        "        return {\"error\": \"Container diagram not found, cannot perform security assessment.\"}\n",
        "\n",
        "    # --- 1. Define the JSON Schema for a structured vulnerability report ---\n",
        "    security_assessment_schema = {\n",
        "        \"title\": \"SecurityThreatModel\",\n",
        "        \"description\": \"A threat model report identifying potential security vulnerabilities in a software architecture.\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"executiveSummary\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"A high-level summary of the overall security posture of the architecture.\"\n",
        "            },\n",
        "            \"vulnerabilities\": {\n",
        "                \"type\": \"array\",\n",
        "                \"description\": \"A list of identified potential vulnerabilities.\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"description\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"A clear, concise description of the potential vulnerability.\"\n",
        "                        },\n",
        "                        \"category\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The type of vulnerability.\",\n",
        "                            \"enum\": [\"Information Disclosure\", \"Insecure Data Flow\", \"Authentication Bypass\", \"Elevation of Privilege\", \"Denial of Service\", \"Missing Security Control\"]\n",
        "                        },\n",
        "                        \"severity\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The estimated severity of the vulnerability.\",\n",
        "                            \"enum\": [\"Critical\", \"High\", \"Medium\", \"Low\"]\n",
        "                        },\n",
        "                        \"recommendation\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"An actionable recommendation to mitigate the risk.\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"description\", \"category\", \"severity\", \"recommendation\"]\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"executiveSummary\", \"vulnerabilities\"]\n",
        "    }\n",
        "\n",
        "    # --- 2. Create the prompt and the structured LLM chain ---\n",
        "    security_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a cybersecurity expert specializing in threat modeling and architectural security reviews. Your call sign is 'Red Specter'. Your job is to think like an attacker and identify potential weaknesses in the proposed design.\n",
        "\n",
        "Your analysis should be based on the provided system brief and C4 Container diagram. You must format your entire response as a single JSON object that strictly adheres to the provided schema. Do not add any text outside the JSON object.\"\"\"),\n",
        "        (\"human\", \"\"\"Please perform a security review of the following architecture.\n",
        "\n",
        "        **Guiding Questions for Your Analysis:**\n",
        "        1.  **Attack Surface Analysis:** Based on the diagram, what are the primary entry points for an external attacker? Which containers are most exposed?\n",
        "        2.  **Data Flow Risks:** Where is sensitive patron data likely to be stored or processed? Are there any risky relationships shown, such as a public-facing container having direct access to the main database?\n",
        "        3.  **Missing Controls:** What critical security components or considerations (e.g., an API Gateway, a dedicated authentication service, firewalls, rate limiting) appear to be missing from this architecture?\n",
        "\n",
        "        **System Design Brief:**\n",
        "        ```yaml\n",
        "        {brief}\n",
        "        ```\n",
        "\n",
        "        **C4 Container Diagram:**\n",
        "        ```puml\n",
        "        {diagram}\n",
        "        ```\n",
        "\n",
        "        Provide your structured JSON threat model now.\n",
        "        \"\"\")\n",
        "    ])\n",
        "\n",
        "    structured_judge_llm = judge_llm.with_structured_output(security_assessment_schema)\n",
        "    security_chain = security_prompt | structured_judge_llm\n",
        "\n",
        "    # --- 3. Invoke the chain and process results ---\n",
        "    try:\n",
        "        assessment = security_chain.invoke({\n",
        "            \"brief\": system_brief,\n",
        "            \"diagram\": container_diag\n",
        "        })\n",
        "\n",
        "        # --- 4. Post-process to calculate a comparable risk score ---\n",
        "        # Lower score is better.\n",
        "        risk_weights = {\"Critical\": 10, \"High\": 5, \"Medium\": 2, \"Low\": 1}\n",
        "        total_risk_score = 0\n",
        "        for vuln in assessment.get(\"vulnerabilities\", []):\n",
        "            total_risk_score += risk_weights.get(vuln.get(\"severity\"), 0)\n",
        "\n",
        "        assessment['overallRiskScore'] = total_risk_score\n",
        "\n",
        "        return {\n",
        "            \"metric\": \"Security 'Red Team' Assessment\",\n",
        "            \"assessment\": assessment\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to get security assessment: {e}\"}\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Assuming 'final_c4_model' and 'system_brief_lms' exist from previous steps\n",
        "\n",
        "# security_report = evaluate_security_assessment(system_brief_lms, final_c4_model)\n",
        "# import json\n",
        "# print(json.dumps(security_report, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmTWNuuUJql3"
      },
      "source": [
        "#### C4 completeness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ngbafpYXK8c_"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Any, Optional\n",
        "import yaml # Assuming usage from previous context\n",
        "\n",
        "def check_c4_completeness(c4_model: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Checks for missing or empty C4 artifacts, respecting the sequential\n",
        "    dependency between C4 levels.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ¤– Evaluating Metric: C4 Model Completeness...\")\n",
        "\n",
        "    def is_missing(value: Optional[str]) -> bool:\n",
        "        \"\"\"Returns True if value is None, an empty string, or whitespace-only.\"\"\"\n",
        "        return not value or not value.strip()\n",
        "\n",
        "    results = {}\n",
        "    missing_count = 0\n",
        "    total_expected = 0\n",
        "    artifacts = [\"analysis\", \"yaml_definition\", \"diagram\"]\n",
        "\n",
        "    # --- 1. CONTEXT LEVEL CHECK ---\n",
        "    # The context level is always expected.\n",
        "    total_expected += 3\n",
        "    context_data = c4_model.get(\"context\", {})\n",
        "    context_status = {key: \"Present\" if not is_missing(context_data.get(key)) else \"Missing\" for key in artifacts}\n",
        "    missing_count += list(context_status.values()).count(\"Missing\")\n",
        "    results[\"Context\"] = context_status\n",
        "\n",
        "    # --- 2. CONTAINER LEVEL CHECK (Conditional) ---\n",
        "    # <<< REFINED LOGIC: Only expect container artifacts if the context was generated successfully. >>>\n",
        "    # We use the context's YAML definition as the key indicator of success.\n",
        "    if not is_missing(context_data.get(\"yaml_definition\")):\n",
        "        total_expected += 3\n",
        "        container_data = c4_model.get(\"containers\", {})\n",
        "        container_status = {key: \"Present\" if not is_missing(container_data.get(key)) else \"Missing\" for key in artifacts}\n",
        "        missing_count += list(container_status.values()).count(\"Missing\")\n",
        "        results[\"Containers\"] = container_status\n",
        "    else:\n",
        "        # If context failed, we explicitly state that containers were not expected.\n",
        "        results[\"Containers\"] = {key: \"Not Expected\" for key in artifacts}\n",
        "\n",
        "    # --- 3. COMPONENT LEVEL CHECK (Logic is already correct) ---\n",
        "    results[\"Components\"] = {}\n",
        "    components_data = c4_model.get(\"components\", {})\n",
        "    if components_data:\n",
        "        for comp_name, comp_data in components_data.items():\n",
        "            comp_status = {}\n",
        "            total_expected += 1 # Analysis is always expected if the component key exists.\n",
        "            if is_missing(comp_data.get(\"analysis\")):\n",
        "                comp_status[\"analysis\"] = \"Missing\"\n",
        "                missing_count += 1\n",
        "                comp_status[\"yaml_definition\"] = \"Not Expected\"\n",
        "                comp_status[\"diagram\"] = \"Not Expected\"\n",
        "            else:\n",
        "                comp_status[\"analysis\"] = \"Present\"\n",
        "                total_expected += 2 # YAML and diagram are now expected.\n",
        "                for key in [\"yaml_definition\", \"diagram\"]:\n",
        "                    if is_missing(comp_data.get(key)):\n",
        "                        comp_status[key] = \"Missing\"\n",
        "                        missing_count += 1\n",
        "                    else:\n",
        "                        comp_status[key] = \"Present\"\n",
        "            results[\"Components\"][comp_name] = comp_status\n",
        "\n",
        "    # --- 4. Final Score Calculation ---\n",
        "    score = 100.0\n",
        "    if total_expected > 0:\n",
        "        score = ((total_expected - missing_count) / total_expected) * 100\n",
        "\n",
        "    return {\n",
        "        \"metric\": \"C4 Model Completeness\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"missing_count\": missing_count,\n",
        "        \"total_expected_artifacts\": total_expected,\n",
        "        \"details\": results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7CO5PRpR8Ab"
      },
      "source": [
        "## Run evaluators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YEchvDuj1Zi"
      },
      "source": [
        "### Run full evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3lFwu2xVMZ17"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Assume all other functions (get_llm, evaluate_*, check_c4_completeness) are defined\n",
        "# and that ModelName is a defined type.\n",
        "\n",
        "def run_full_evaluation(\n",
        "    system_brief: str,\n",
        "    c4_model: Dict,\n",
        "    judge_model_name: Any, # Using Any to be compatible with ModelName type\n",
        "    temperature: float = 0.0\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs a structured, level-aware evaluation of a C4 model, providing the\n",
        "    correct context and source of truth to each metric.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"ðŸ STARTING FULL C4 MODEL EVALUATION (Judge: {judge_model_name}) ðŸ\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    judge_llm = get_llm(model_name=judge_model_name, temperature=temperature)\n",
        "    report = {\n",
        "        'evaluationMetadata': {\n",
        "            \"judgeModel\": judge_model_name,\n",
        "            \"judgeModelTemperature\": temperature,\n",
        "            \"evaluationTimestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Layer 1: Holistic Structural & Completeness Metrics ---\n",
        "    # These metrics correctly inspect the entire c4_model dictionary at once.\n",
        "    print(\"--- Running Holistic Structural Checks ---\")\n",
        "    report['compilationSuccess'] = evaluate_compilation_success(c4_model)\n",
        "    report['abstractionAdherence'] = evaluate_abstraction_adherence(c4_model)\n",
        "    report['missingInformation'] = check_c4_completeness(c4_model)\n",
        "    # Naming consistency can also be considered a holistic check\n",
        "    report['emergentNamingConsistency'] = evaluate_emergent_naming_consistency(c4_model)\n",
        "\n",
        "    # --- Layer 2: Level-Specific Semantic & Qualitative Evaluations ---\n",
        "    print(\"\\n--- Running Level-Specific Evaluations ---\")\n",
        "\n",
        "    # --- CONTEXT LEVEL EVALUATION ---\n",
        "    if \"context\" in c4_model:\n",
        "        print(\"  - Evaluating Context Level...\")\n",
        "        context_eval = {}\n",
        "        context_diag = c4_model[\"context\"].get(\"diagram\")\n",
        "        if context_diag:\n",
        "            # The brief IS the source of truth for the context diagram.\n",
        "            context_eval['semanticConsistency'] = evaluate_semantic_consistency(system_brief, c4_model, judge_llm)\n",
        "            # The qualitative rubric now gets the brief it needs to judge completeness.\n",
        "            context_eval['qualitativeRubric'] = evaluate_qualitative_rubric(context_diag, \"Context Diagram\", system_brief, judge_llm)\n",
        "        report['contextEvaluation'] = context_eval\n",
        "\n",
        "    # --- CONTAINER LEVEL EVALUATION ---\n",
        "    if \"containers\" in c4_model:\n",
        "        print(\"  - Evaluating Container Level...\")\n",
        "        container_eval = {}\n",
        "        container_diag = c4_model[\"containers\"].get(\"diagram\")\n",
        "        container_yaml = c4_model[\"containers\"].get(\"yaml_definition\")\n",
        "        if container_diag and container_yaml:\n",
        "            # The container YAML is the source of truth for the container diagram.\n",
        "            container_eval['definitionalConsistency'] = evaluate_definitional_consistency(container_yaml, container_diag, \"containers\")\n",
        "            # The qualitative rubric still needs the high-level brief for context.\n",
        "            container_eval['qualitativeRubric'] = evaluate_qualitative_rubric(container_diag, \"Container Diagram\", system_brief, judge_llm)\n",
        "        report['containerEvaluation'] = container_eval\n",
        "\n",
        "    # --- COMPONENT LEVEL EVALUATION ---\n",
        "    if \"components\" in c4_model:\n",
        "        print(\"  - Evaluating Component Level(s)...\")\n",
        "        component_evals = {}\n",
        "        for comp_name, comp_data in c4_model[\"components\"].items():\n",
        "            comp_diag = comp_data.get(\"diagram\")\n",
        "            comp_yaml = comp_data.get(\"yaml_definition\")\n",
        "            if comp_diag and comp_yaml:\n",
        "                # The component YAML is the source of truth for the component diagram.\n",
        "                consistency_result = evaluate_definitional_consistency(comp_yaml, comp_diag, \"components\")\n",
        "                # For component rubric, the system brief is still the best high-level context we have.\n",
        "                rubric_result = evaluate_qualitative_rubric(comp_diag, f\"Component: {comp_name}\", system_brief, judge_llm)\n",
        "                component_evals[comp_name] = {\n",
        "                    'definitionalConsistency': consistency_result,\n",
        "                    'qualitativeRubric': rubric_result\n",
        "                }\n",
        "        report['componentEvaluations'] = component_evals\n",
        "\n",
        "    # --- Layer 3: Holistic Expert Critiques ---\n",
        "    # These high-level critiques look at the model as a whole, so they run last.\n",
        "    print(\"\\n--- Running Holistic Expert Critiques ---\")\n",
        "    report['architectCritique'] = evaluate_architect_critique(system_brief, c4_model, judge_llm)\n",
        "    report['securityAssessment'] = evaluate_security_assessment(system_brief, c4_model, judge_llm)\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(f\"ðŸ“‹ FINAL EVALUATION REPORT (Judge: {judge_model_name}) ðŸ“‹\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    print(json.dumps(report, indent=2))\n",
        "\n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7QILPFmv9MH"
      },
      "source": [
        "### Save evalution report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "S_rdZhApfPeD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Assume the following are defined in your broader script:\n",
        "# - run_full_evaluation (your existing evaluation function)\n",
        "# - experiment_results (the list containing results from each run)\n",
        "\n",
        "def save_all_evaluation_reports(\n",
        "    all_reports: Dict[str, Dict[str, Any]],\n",
        "    output_filename: str = \"all_evaluation_summary.json\",\n",
        "    output_dir: str = \"c4_artifacts/evaluation_summaries\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Consolidates and saves all individual evaluation reports into a single JSON file.\n",
        "\n",
        "    Args:\n",
        "        all_reports (Dict[str, Dict[str, Any]]): A dictionary where keys are thread_ids\n",
        "                                                  and values are the full evaluation reports.\n",
        "        output_filename (str): The name of the file to save the consolidated report.\n",
        "        output_dir (str): The directory where the consolidated report will be saved.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filepath = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_reports, f, indent=2)\n",
        "        print(f\"\\n--- âœ… Consolidated evaluation reports saved to: {filepath} ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- âŒ Failed to save consolidated evaluation reports to {filepath}: {e} ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3e-bfpQjq_r"
      },
      "source": [
        "### Format evaluation reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "l4DaWat5ExD0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DEDICATED FORMATTING HELPER FUNCTIONS\n",
        "# Each function is small, self-contained, and easy to understand/modify.\n",
        "# ==============================================================================\n",
        "\n",
        "def _format_default(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Default formatter for any metric that doesn't have a custom one.\"\"\"\n",
        "    title = data.get('metric', 'Metric Details')\n",
        "    return (\n",
        "        f\"### {title}\\n\\n\"\n",
        "        \"**Details:**\\n\"\n",
        "        \"```json\\n\"\n",
        "        f\"{json.dumps(data.get('details', data), indent=2)}\\n\"\n",
        "        \"```\\n\\n\"\n",
        "    )\n",
        "\n",
        "def _format_compilation_success(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Formats the 'Compilation Success' metric.\"\"\"\n",
        "    score = data.get('score', 0)\n",
        "    status_text = \"Excellent\" if score >= 80 else (\"Good\" if score >= 50 else \"Needs Improvement\")\n",
        "    successful = data.get('successful', 0)\n",
        "    total = data.get('total', 0)\n",
        "    return (\n",
        "        f\"### {data.get('metric', 'Compilation Success')}\\n\\n\"\n",
        "        f\"**Overall Score:** {score:.2f}% ({status_text})\\n\"\n",
        "        f\"- **Successful:** {successful} / **Total:** {total}\\n\\n\"\n",
        "    )\n",
        "\n",
        "def _format_abstraction_adherence(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Formats the 'Abstraction Adherence' metric.\"\"\"\n",
        "    details = data.get('details', {})\n",
        "    lines = [f\"### {data.get('metric', 'Abstraction Adherence')}\\n\\n\"]\n",
        "    if not details:\n",
        "        lines.append(\"No details provided.\\n\\n\")\n",
        "    for diag, status in details.items():\n",
        "        lines.append(f\"- {diag}: **{status}**\\n\")\n",
        "    return \"\".join(lines) + \"\\n\"\n",
        "\n",
        "def _format_cross_level_consistency(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Formats the 'Cross-Level Consistency' metric.\"\"\"\n",
        "    details = data.get('details', {})\n",
        "    lines = [f\"### {data.get('metric', 'Cross-Level Consistency')}\\n\\n\"]\n",
        "    lines.append(f\"- **Passed Checks:** {data.get('passed', 0)} / **Total Checks:** {data.get('total', 0)}\\n\\n\")\n",
        "    if isinstance(details, dict):\n",
        "        for check, result in details.items():\n",
        "            status = result.get('status', 'Unknown')\n",
        "            reason = result.get('reason', '')\n",
        "            lines.append(f\"- {check}: **{status}**{f' - *{reason}*' if reason else ''}\\n\")\n",
        "    return \"\".join(lines) + \"\\n\"\n",
        "\n",
        "def _format_security_assessment(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Formats the 'Security Red Team Assessment' metric.\"\"\"\n",
        "    assessment = data.get('assessment', {})\n",
        "    lines = [f\"### {data.get('metric', 'Security Assessment')}\\n\\n\"]\n",
        "    lines.append(f\"**Executive Summary:** {assessment.get('executiveSummary', 'N/A')}\\n\")\n",
        "    lines.append(f\"**Overall Risk Score (Lower is better):** {assessment.get('overallRiskScore', 'N/A')}\\n\\n\")\n",
        "\n",
        "    vulnerabilities = assessment.get('vulnerabilities', [])\n",
        "    if vulnerabilities:\n",
        "        lines.append(\"**Identified Vulnerabilities:**\\n\\n\")\n",
        "        lines.append(\"| Description | Category | Severity | Recommendation |\\n\")\n",
        "        lines.append(\"|---|---|---|---|\\n\")\n",
        "        for vuln in vulnerabilities:\n",
        "            lines.append(f\"| {vuln.get('description', 'N/A')} | {vuln.get('category', 'N/A')} | **{vuln.get('severity', 'Low')}** | {vuln.get('recommendation', 'N/A')} |\\n\")\n",
        "    else:\n",
        "        lines.append(\"No specific vulnerabilities were identified.\\n\")\n",
        "    return \"\".join(lines) + \"\\n\"\n",
        "\n",
        "def _format_architect_critique(data: Dict[str, Any]) -> str:\n",
        "    \"\"\"Formats the 'Principal Architect's Critique'.\"\"\"\n",
        "    critique = data.get('critique', {})\n",
        "    lines = [f\"### {data.get('metric', 'Architectâ€™s Critique')}\\n\\n\"]\n",
        "    lines.append(f\"**Executive Summary:** {critique.get('executiveSummary', 'N/A')}\\n\\n\")\n",
        "    # Feasibility and Soundness\n",
        "    fs = critique.get('feasibilityAndSoundness', {})\n",
        "    lines.append(f\"**Feasibility & Soundness ({fs.get('rating', '-')}/5):** {fs.get('critique', 'N/A')}\\n\")\n",
        "    if fs.get('identifiedRisks'):\n",
        "        lines.append(\"**Identified Risks:**\\n\" + \"\".join(f\"- {risk}\\n\" for risk in fs['identifiedRisks']))\n",
        "    # Clarity and Communication\n",
        "    cc = critique.get('clarityAndCommunication', {})\n",
        "    lines.append(f\"\\n**Clarity & Communication ({cc.get('rating', '-')}/5):** {cc.get('critique', 'N/A')}\\n\\n\")\n",
        "    # Recommendation\n",
        "    ar = critique.get('actionableRecommendation', {})\n",
        "    lines.append(f\"**Actionable Recommendation (Priority: {ar.get('priority', 'N/A')}):**\\n\")\n",
        "    lines.append(f\"- **Recommendation:** {ar.get('recommendation', 'N/A')}\\n\")\n",
        "    lines.append(f\"- **Justification:** {ar.get('justification', 'N/A')}\\n\")\n",
        "    return \"\".join(lines)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. MAIN FORMATTING FUNCTION (Now much cleaner)\n",
        "# ==============================================================================\n",
        "\n",
        "def format_evaluation_report(report: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Formats the full evaluation report dictionary into a human-readable Markdown string\n",
        "    using a dispatcher pattern for maintainability.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- The Dispatcher: Maps metric keys to their formatting function ---\n",
        "    METRIC_FORMATTERS = {\n",
        "        'compilationSuccess': _format_compilation_success,\n",
        "        'abstractionAdherence': _format_abstraction_adherence,\n",
        "        'crossLevelConsistency': _format_cross_level_consistency,\n",
        "        'securityAssessment': _format_security_assessment,\n",
        "        'architectCritique': _format_architect_critique,\n",
        "        # Add other specific formatters here\n",
        "    }\n",
        "\n",
        "    parts = [\"# ðŸ C4 Model Evaluation Report ðŸ\\n\\n\"]\n",
        "\n",
        "    # --- Header Section ---\n",
        "    # (Your header logic for metadata, brief name, etc. is good and can be kept here)\n",
        "    if 'evaluationMetadata' in report:\n",
        "        # ... your metadata formatting logic ...\n",
        "        parts.append(\"## Evaluation Context\\n\\n\")\n",
        "    if 'brief_name' in report:\n",
        "        parts.append(f\"## System Brief: {report['brief_name']}\\n\\n\")\n",
        "\n",
        "    parts.append(\"---\\n\\n\")\n",
        "\n",
        "    # --- Metrics Section (uses the dispatcher) ---\n",
        "    ordered_metrics = [\n",
        "        'compilationSuccess', 'abstractionAdherence', 'crossLevelConsistency',\n",
        "        'emergentNamingConsistency', 'semanticConsistency', 'qualitativeRubric',\n",
        "        'architectCritique', 'securityAssessment'\n",
        "    ]\n",
        "\n",
        "    for metric_key in ordered_metrics:\n",
        "        if metric_key in report:\n",
        "            metric_data = report[metric_key]\n",
        "            # Look up the correct formatter, or use the default one\n",
        "            formatter = METRIC_FORMATTERS.get(metric_key, _format_default)\n",
        "            parts.append(formatter(metric_data))\n",
        "            parts.append(\"---\\n\\n\")\n",
        "\n",
        "    return \"\".join(parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReAZRMfmHLj"
      },
      "source": [
        "### Run all evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "COXfdxlxmJQb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def run_all_evaluations(\n",
        "    experiment_results: list,\n",
        "    experiment_config: dict,\n",
        "    save_c4_artifacts_func: callable,\n",
        "    run_full_evaluation_func: callable,\n",
        "    save_all_evaluation_reports_func: callable,\n",
        "    format_evaluation_report_func: callable,\n",
        "    judge_model_name: str = \"gemini-1.5-pro-latest\",\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Processes each experiment result completely (saves artifacts, runs evaluation,\n",
        "    saves reports) in a single, efficient loop.\n",
        "    \"\"\"\n",
        "    all_evaluation_reports = {}\n",
        "    experiment_name = experiment_config[\"name\"]\n",
        "\n",
        "    print(\"\\n\" + \"#\"*80)\n",
        "    print(f\"### âœ¨ PROCESSING ALL RESULTS FOR EXPERIMENT: {experiment_name} âœ¨ ###\")\n",
        "    print(f\"### Judge Model: {judge_model_name}\")\n",
        "    print(\"#\"*80 + \"\\n\")\n",
        "\n",
        "    # <<< STRUCTURED: Process each result completely in a single loop >>>\n",
        "    for result in experiment_results:\n",
        "        # 1. EXTRACT METADATA\n",
        "        brief_name = result['brief_name']\n",
        "        thread_id = result['thread_id']\n",
        "        system_brief = result['system_brief_content']\n",
        "        final_c4_model = result['final_c4_model']\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"--- Processing Run: '{brief_name}' (Thread: {thread_id}) ---\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # 2. DEFINE STRUCTURED OUTPUT PATHS\n",
        "        # Create a single, neat directory for this entire result\n",
        "        result_base_dir = f\"evaluation_results_openai/{experiment_name}/{brief_name}_{thread_id}\"\n",
        "        artifacts_output_dir = f\"{result_base_dir}/c4_artifacts\"\n",
        "        reports_output_dir = f\"{result_base_dir}/evaluation_reports\"\n",
        "        os.makedirs(artifacts_output_dir, exist_ok=True)\n",
        "        os.makedirs(reports_output_dir, exist_ok=True)\n",
        "\n",
        "        # 3. SAVE THE GENERATED C4 ARTIFACTS\n",
        "        print(f\"ðŸ’¾ Saving C4 model artifacts to: {artifacts_output_dir}\")\n",
        "        # <<< FIXED: The missing function call is now here >>>\n",
        "        save_c4_artifacts_func(artifacts_output_dir, final_c4_model)\n",
        "        print(\"--- âœ… C4 Artifacts saved. ---\")\n",
        "\n",
        "        # 4. RUN THE EVALUATION\n",
        "        print(f\"\\nðŸ”¬ Running evaluation with judge model '{judge_model_name}'...\")\n",
        "        current_report = run_full_evaluation_func(system_brief, final_c4_model, judge_model_name)\n",
        "        print(\"--- âœ… Evaluation complete. ---\")\n",
        "\n",
        "        # 5. ENRICH AND SAVE THE EVALUATION REPORT\n",
        "        # Add metadata for context\n",
        "        current_report['experiment_config'] = experiment_config\n",
        "        current_report['brief_name'] = brief_name\n",
        "        current_report['thread_id'] = thread_id\n",
        "        all_evaluation_reports[thread_id] = current_report\n",
        "\n",
        "        # Save the JSON report\n",
        "        report_json_filepath = f\"{reports_output_dir}/evaluation_report.json\"\n",
        "        print(f\"\\nðŸ’¾ Saving evaluation reports to: {reports_output_dir}\")\n",
        "        with open(report_json_filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(current_report, f, indent=2)\n",
        "        print(f\"  - Saved evaluation_report.json\")\n",
        "\n",
        "        # Save the formatted Markdown report\n",
        "        # ... (Your logic for formatted reports) ...\n",
        "        formatted_report_content = format_evaluation_report_func(current_report)\n",
        "        with open(f\"{reports_output_dir}/evaluation_report.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(formatted_report_content)\n",
        "        print(f\"  - Saved evaluation_report.md\")\n",
        "        print(\"--- âœ… Evaluation reports saved. ---\")\n",
        "\n",
        "    print(\"\\n\\n\" + \"#\"*80)\n",
        "    print(f\"### ðŸŽ‰ ALL RESULTS PROCESSED FOR: {experiment_name}! ###\")\n",
        "    print(\"#\"*80 + \"\\n\")\n",
        "\n",
        "    # --- SAVE THE CONSOLIDATED REPORT FOR THIS ENTIRE EXPERIMENT ---\n",
        "    consolidated_output_dir = f\"evaluation_results_openai/{experiment_name}\"\n",
        "    save_all_evaluation_reports_func(\n",
        "        all_evaluation_reports,\n",
        "        output_dir=consolidated_output_dir\n",
        "    )\n",
        "\n",
        "    return all_evaluation_reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-D4gMT_k0hL"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmdLOF6Ozygu"
      },
      "source": [
        "### Briefs examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWnORaK1vq6H"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Define your system briefs in a dictionary (same as before) ---\n",
        "system_briefs = {\n",
        "    \"Library Management System (LMS)\": \"\"\"\n",
        "title: Library Management System\n",
        "description: >\n",
        "  A solution for public and academic libraries to catalogue items,\n",
        "  manage circulation, and provide self-service portals for patrons.\n",
        "domain: Education / Library Services\n",
        "constraints:\n",
        "  - \"EU-only data residency\"\n",
        "  - \"Open-source tech stack preferred\"\n",
        "  - \"Must support bilingual UI (EN/FR)\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Catalogue physical & digital items with MARC metadata\"\n",
        "  - id: R-02\n",
        "    desc: \"Patron self-checkout & returns via kiosks or mobile app\"\n",
        "  - id: R-03\n",
        "    desc: \"Search & faceted browse with <1 s median latency\"\n",
        "  - id: R-04\n",
        "    desc: \"Automated overdue notices by email/SMS\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Performance\n",
        "    desc: \"99th-percentile search response < 800 ms\"\n",
        "  - id: R-06\n",
        "    quality: Availability\n",
        "    desc: \"Service uptime â‰¥ 99.5%\"\n",
        "  - id: R-07\n",
        "    quality: Security\n",
        "    desc: \"Role-based access; yearly PEN tests\"\n",
        "  - id: R-08\n",
        "    quality: Scalability\n",
        "    desc: \"Catalogue up to 1 million items\"\n",
        "target_cloud:\n",
        "  provider: Azure\n",
        "  regions:\n",
        "    - westeurope\n",
        "\"\"\",\n",
        "    \"NextGen Point-of-Sale (POS)\": \"\"\"\n",
        "title: NextGen Point-of-Sale\n",
        "description: >\n",
        "  Store-front POS inspired by Craig Larmanâ€™s case study; supports bar-code\n",
        "  scanning, promotions, and offline queueing when networks fails.\n",
        "domain: Retail / Point-of-Sale\n",
        "constraints:\n",
        "  - \"PCI-DSS Level 1 compliance\"\n",
        "  - \"Offline transaction buffering â‰¤ 24 h\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Scan items & compute totals with tax rules per locale\"\n",
        "  - id: R-02\n",
        "    desc: \"Apply promotions & loyalty points in real time\"\n",
        "  - id: R-03\n",
        "    desc: \"Process card payments via Stripe Terminal\"\n",
        "  - id: R-04\n",
        "    desc: \"Print or email receipt with QR-code\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Performance\n",
        "    desc: \"Complete sale in â‰¤ 500 ms P95\"\n",
        "  - id: R-06\n",
        "    quality: Availability\n",
        "    desc: \"Uptime â‰¥ 99.9%\"\n",
        "  - id: R-07\n",
        "    quality: Reliability\n",
        "    desc: \"No lost sales during network outages\"\n",
        "  - id: R-08\n",
        "    quality: Usability\n",
        "    desc: \"Cashier workflow â‰¤ 4 clicks\"\n",
        "target_cloud:\n",
        "  provider: AWS\n",
        "  regions:\n",
        "    - us-east-1\n",
        "    - eu-west-1\n",
        "\"\"\",\n",
        "    \"Online Bookstore (Mini-Amazon)\": \"\"\"\n",
        "title: Online Bookstore\n",
        "description: >\n",
        "  A scaled-down Amazon-style e-commerce site for buying physical and\n",
        "  electronic books with recommendations and reviews.\n",
        "domain: Retail / E-commerce\n",
        "constraints:\n",
        "  - \"GDPR & CCPA compliance\"\n",
        "  - \"Multi-currency (USD, EUR, GBP)\"\n",
        "  - \"Integrate with third-party shipping APIs\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Browse & keyword search the catalogue\"\n",
        "  - id: R-02\n",
        "    desc: \"Shopping cart & secure checkout\"\n",
        "  - id: R-03\n",
        "    desc: \"Customer reviews & 5-star ratings\"\n",
        "  - id: R-04\n",
        "    desc: \"â€˜Customers also boughtâ€™ recommendations\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Scalability\n",
        "    desc: \"Handle 1 M MAU without degradation\"\n",
        "  - id: R-06\n",
        "    quality: Performance\n",
        "    desc: \"Page load < 2 s on 3G\"\n",
        "  - id: R-07\n",
        "    quality: Availability\n",
        "    desc: \"99.95% uptime\"\n",
        "  - id: R-08\n",
        "    quality: Security\n",
        "    desc: \"OWASP Top-10 mitigations\"\n",
        "target_cloud:\n",
        "  provider: AWS\n",
        "  regions:\n",
        "    - us-east-1\n",
        "    - eu-west-1\n",
        "    - ap-southeast-1\n",
        "\"\"\",\n",
        "    \"Student Information System (SIS)\": \"\"\"\n",
        "title: Student Information System\n",
        "description: >\n",
        "  Central system for universities to manage student records, enrollment,\n",
        "  grades, and transcripts across multiple campuses.\n",
        "domain: Education / Administration\n",
        "constraints:\n",
        "  - \"FERPA compliance (US) & GDPR (EU)\"\n",
        "  - \"Multi-campus tenancy\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Maintain student demographic & academic records\"\n",
        "  - id: R-02\n",
        "    desc: \"Online course enrollment & wait-listing\"\n",
        "  - id: R-03\n",
        "    desc: \"Faculty grade submission & change history\"\n",
        "  - id: R-04\n",
        "    desc: \"Generate official transcripts (PDF)\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Security\n",
        "    desc: \"Field-level encryption for PII\"\n",
        "  - id: R-06\n",
        "    quality: Integrity\n",
        "    desc: \"Immutable audit logs for grade changes\"\n",
        "  - id: R-07\n",
        "    quality: Availability\n",
        "    desc: \"Uptime â‰¥ 99.8% during term\"\n",
        "  - id: R-08\n",
        "    quality: Maintainability\n",
        "    desc: \"â‰¤ 20% mean time to repair (MTTR) per incident\"\n",
        "target_cloud:\n",
        "  provider: GCP\n",
        "  regions:\n",
        "    - us-east1\n",
        "    - europe-west4\n",
        "\"\"\",\n",
        "    \"Clinic Management System\": \"\"\"\n",
        "title: Clinic Management System\n",
        "description: >\n",
        "  Manages patient admissions, electronic medical records, scheduling,\n",
        "  and billing for medium-sized hospitals and clinics.\n",
        "domain: Healthcare / Clinical IT\n",
        "constraints:\n",
        "  - \"HIPAA & GDPR compliance\"\n",
        "  - \"High availability 99.99%\"\n",
        "  - \"Data retention â‰¥ 10 years\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Patient registration & demographic capture\"\n",
        "  - id: R-02\n",
        "    desc: \"Appointment scheduling with resource clash checks\"\n",
        "  - id: R-03\n",
        "    desc: \"Electronic Medical Record (EMR) with audit trail\"\n",
        "  - id: R-04\n",
        "    desc: \"Billing & insurance claim submission\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Security\n",
        "    desc: \"Access via multi-factor auth; AES-256 at rest\"\n",
        "  - id: R-06\n",
        "    quality: Availability\n",
        "    desc: \"Uptime â‰¥ 99.99% (active-active)\"\n",
        "  - id: R-07\n",
        "    quality: Performance\n",
        "    desc: \"EMR screen load < 1 s P95\"\n",
        "  - id: R-08\n",
        "    quality: Interoperability\n",
        "    desc: \"HL7 FHIR APIs for lab & imaging systems\"\n",
        "target_cloud:\n",
        "  provider: Hybrid\n",
        "  regions:\n",
        "    - on-prem-k8s\n",
        "    - eu-central-1\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgK6jzGM1JcG"
      },
      "source": [
        "## Run all experiments and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "MPKFEzbV5NXS"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import platform\n",
        "import subprocess\n",
        "\n",
        "def zip_folder_with_increment(folder_to_zip: str, base_name: str = \"evaluation_results_openai\") -> str:\n",
        "    \"\"\"\n",
        "    Zips the specified folder and saves it with an incremented filename if needed.\n",
        "\n",
        "    Args:\n",
        "        folder_to_zip (str): Path to the folder to zip.\n",
        "        base_name (str): Base name for the zip file (default is 'evaluation_results').\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the created zip file.\n",
        "    \"\"\"\n",
        "    zip_filename = f\"{base_name}.zip\"\n",
        "    counter = 1\n",
        "\n",
        "    # Find a non-conflicting filename\n",
        "    while os.path.exists(zip_filename):\n",
        "        zip_filename = f\"{base_name}_{counter}.zip\"\n",
        "        counter += 1\n",
        "\n",
        "    zip_base_name = zip_filename.replace('.zip', '')\n",
        "\n",
        "    print(f\"Zipping the folder: '{folder_to_zip}' into '{zip_filename}'...\")\n",
        "\n",
        "    try:\n",
        "        shutil.make_archive(zip_base_name, 'zip', folder_to_zip)\n",
        "        print(f\"âœ… Successfully created zip file: '{zip_filename}'\")\n",
        "\n",
        "        abs_path = os.path.abspath(zip_filename)\n",
        "        print(f\"ðŸ“ You can find the zipped file here: {abs_path}\")\n",
        "\n",
        "        # Optional: open folder in file explorer\n",
        "        folder_path = os.path.dirname(abs_path)\n",
        "        if platform.system() == \"Windows\":\n",
        "            os.startfile(folder_path)\n",
        "        elif platform.system() == \"Darwin\":  # macOS\n",
        "            subprocess.run([\"open\", folder_path])\n",
        "        elif platform.system() == \"Linux\":\n",
        "            subprocess.run([\"xdg-open\", folder_path])\n",
        "\n",
        "        return abs_path\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: The directory '{folder_to_zip}' was not found.\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ An error occurred: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Example usage:\n",
        "# zip_folder_with_increment(\"evaluation_results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeLbt9xUd5mO"
      },
      "source": [
        "### GPT experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2xeBGfnxVYt"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "908dV28sk8IG",
        "outputId": "e47ef798-f165-484e-dda7-97f532db857a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸš€ Starting Experiment: GPT4o_Mini_Simple\n",
            "   - Model: gpt-4o-mini\n",
            "   - Method: simple\n",
            "============================================================\n",
            "--- ðŸ—ï¸ Building graph with model: 'gpt-4o-mini' and analysis: 'simple' ---\n",
            "--- âš™ï¸  Instantiating model: gpt-4o-mini ---\n",
            "âœ… LangGraph C4 Modeler compiled successfully with checkpointer!\n",
            "\n",
            "--- ðŸš€ Starting C4 Model Generation Experiments ---\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Processing: Library Management System (LMS) ---\n",
            "================================================================================\n",
            "\n",
            "\n",
            "--- LangGraph Thread ID: 20250612-141410-library-management-system-lms-a77be777 ---\n",
            "--- âœï¸ Generating Context Level Analysis ---\n",
            "\n",
            "========================================\n",
            "Node: analysis\n",
            "========================================\n",
            "--- ðŸ“ Generating Context Level YAML ---\n",
            "\n",
            "========================================\n",
            "Node: yaml\n",
            "========================================\n",
            "--- ðŸŽ¨ Generating Context Level Diagram ---\n",
            "\n",
            "========================================\n",
            "Node: diagram\n",
            "========================================\n",
            "--- âœï¸ Generating Container Level Analysis ---\n",
            "\n",
            "========================================\n",
            "Node: analysis\n",
            "========================================\n",
            "--- ðŸ“ Generating Container Level YAML ---\n",
            "\n",
            "========================================\n",
            "Node: yaml\n",
            "========================================\n",
            "--- ðŸŽ¨ Generating Container Level Diagram ---\n",
            "\n",
            "========================================\n",
            "Node: diagram\n",
            "========================================\n",
            "--- âš™ï¸ Populating Component Queue ---\n",
            "Found containers to process: ['Web Application Container', 'API Container', 'Database Container', 'Notification Service Container']\n",
            "--- ðŸ¤” Checking Component Queue ---\n",
            "Queue is not empty. Processing next component.\n",
            "\n",
            "========================================\n",
            "Node: populate_queue\n",
            "========================================\n",
            "--- âœï¸ Generating Component Level Analysis for 'Web Application Container' ---\n",
            "\n",
            "========================================\n",
            "Node: analysis\n",
            "========================================\n",
            "--- ðŸ“ Generating Component Level YAML for 'Web Application Container' ---\n",
            "\n",
            "========================================\n",
            "Node: yaml\n",
            "========================================\n",
            "--- ðŸŽ¨ Generating Component Level Diagram for 'Web Application Container' ---\n",
            "\n",
            "========================================\n",
            "Node: diagram\n",
            "========================================\n",
            "--- âœ… Completing Component Task ---\n",
            "Finished processing: Web Application Container\n",
            "--- ðŸ¤” Checking Component Queue ---\n",
            "Queue is not empty. Processing next component.\n",
            "\n",
            "========================================\n",
            "Node: complete_component\n",
            "========================================\n",
            "--- âœï¸ Generating Component Level Analysis for 'API Container' ---\n",
            "\n",
            "========================================\n",
            "Node: analysis\n",
            "========================================\n",
            "--- ðŸ“ Generating Component Level YAML for 'API Container' ---\n",
            "\n",
            "========================================\n",
            "Node: yaml\n",
            "========================================\n",
            "--- ðŸŽ¨ Generating Component Level Diagram for 'API Container' ---\n",
            "\n",
            "========================================\n",
            "Node: diagram\n",
            "========================================\n",
            "--- âœ… Completing Component Task ---\n",
            "Finished processing: API Container\n",
            "--- ðŸ¤” Checking Component Queue ---\n",
            "Queue is not empty. Processing next component.\n",
            "\n",
            "========================================\n",
            "Node: complete_component\n",
            "========================================\n",
            "--- âœï¸ Generating Component Level Analysis for 'Database Container' ---\n",
            "\n",
            "========================================\n",
            "Node: analysis\n",
            "========================================\n",
            "--- ðŸ“ Generating Component Level YAML for 'Database Container' ---\n",
            "\n",
            "========================================\n",
            "Node: yaml\n",
            "========================================\n",
            "--- ðŸŽ¨ Generating Component Level Diagram for 'Database Container' ---\n",
            "\n",
            "========================================\n",
            "Node: diagram\n",
            "========================================\n",
            "--- âœ… Completing Component Task ---\n",
            "Finished processing: Database Container\n",
            "--- ðŸ¤” Checking Component Queue ---\n",
            "Queue is not empty. Processing next component.\n",
            "\n",
            "========================================\n",
            "Node: complete_component\n",
            "========================================\n",
            "--- âœï¸ Generating Component Level Analysis for 'Notification Service Container' ---\n",
            "\n",
            "========================================\n",
            "Node: analysis\n",
            "========================================\n",
            "--- ðŸ“ Generating Component Level YAML for 'Notification Service Container' ---\n",
            "\n",
            "========================================\n",
            "Node: yaml\n",
            "========================================\n",
            "--- ðŸŽ¨ Generating Component Level Diagram for 'Notification Service Container' ---\n",
            "\n",
            "========================================\n",
            "Node: diagram\n",
            "========================================\n",
            "--- âœ… Completing Component Task ---\n",
            "Finished processing: Notification Service Container\n",
            "--- ðŸ¤” Checking Component Queue ---\n",
            "Queue is empty. Finishing workflow.\n",
            "\n",
            "========================================\n",
            "Node: complete_component\n",
            "========================================\n",
            "\n",
            "--- ðŸŽ‰ C4 Model Generation Complete for Library Management System (LMS)! ---\n",
            "Final state for thread '20250612-141410-library-management-system-lms-a77be777' retrieved and stored.\n",
            "\n",
            "\n",
            "--- âœ… All C4 Model Generation Experiments Complete! ---\n",
            "âœ… Finished experiment: GPT4o_Mini_Simple.\n",
            "\n",
            "\n",
            "ðŸŽ‰ All experiments completed for all configurations.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Define the configurations for your experiments ---\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "\n",
        "\n",
        "# Instead of a simple list of models, we create a list of dictionaries.\n",
        "# Each dictionary is a complete configuration for a single experiment run.\n",
        "# This makes it easy to test different models, methods, and parameters.\n",
        "EXPERIMENT_CONFIGS = [\n",
        "    {\n",
        "        \"name\": \"GPT4omini_Collaborative_3_Rounds\",\n",
        "        \"model_name\": \"gpt-4o-mini\",\n",
        "        \"analysis_method\": \"collaborative\",\n",
        "        \"collab_rounds\": 3,\n",
        "    },\n",
        "    # {\n",
        "    #     \"name\": \"GPT4omini_Collaborative_1_Rounds\",\n",
        "    #     \"model_name\": \"gpt-4o-mini\",\n",
        "    #     \"analysis_method\": \"collaborative\",\n",
        "    #     \"collab_rounds\": 1,\n",
        "    # },\n",
        "    # {\n",
        "    #     \"name\": \"GPT4o_Collaborative_3_Rounds\",\n",
        "    #     \"model_name\": \"gpt-4o\",\n",
        "    #     \"analysis_method\": \"collaborative\",\n",
        "    #     \"collab_rounds\": 3,\n",
        "    # },\n",
        "    # {\n",
        "    #     \"name\": \"GPT4o_Collaborative_1_Rounds\",\n",
        "    #     \"model_name\": \"gpt-4o\",\n",
        "    #     \"analysis_method\": \"collaborative\",\n",
        "    #     \"collab_rounds\": 1,\n",
        "    # },\n",
        "    # {\n",
        "    #     \"name\": \"GPT4o_Mini_Simple\",\n",
        "    #     \"model_name\": \"gpt-4o-mini\",\n",
        "    #     \"analysis_method\": \"simple\",\n",
        "    #     \"collab_rounds\": None,  # Not applicable for the 'simple' method\n",
        "    # },\n",
        "    # {\n",
        "    #     \"name\": \"GPT4o_Simple\",\n",
        "    #     \"model_name\": \"gpt-4o\",\n",
        "    #     \"analysis_method\": \"simple\",\n",
        "    #     \"collab_rounds\": None,  # Not applicable for the 'simple' method\n",
        "    # },\n",
        "]\n",
        "\n",
        "# --- 2. Run the experiments by looping through the configurations ---\n",
        "all_experiment_results = {}\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# We now loop through our list of configuration dictionaries.\n",
        "for config in EXPERIMENT_CONFIGS:\n",
        "    experiment_name = config[\"name\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"ðŸš€ Starting Experiment: {experiment_name}\")\n",
        "    print(f\"   - Model: {config['model_name']}\")\n",
        "    print(f\"   - Method: {config['analysis_method']}\")\n",
        "    if config['analysis_method'] == 'collaborative':\n",
        "        print(f\"   - Rounds: {config['collab_rounds']}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # A. Create a dedicated app instance using the full configuration.\n",
        "    # We pass all the parameters from our config dictionary.\n",
        "    current_app = create_c4_modeler_graph(\n",
        "        checkpointer=checkpointer,\n",
        "        model_name=config[\"model_name\"],\n",
        "        analysis_method=config[\"analysis_method\"],\n",
        "        collab_rounds=config[\"collab_rounds\"]\n",
        "    )\n",
        "\n",
        "    # B. Run your existing experiment function against this specific app.\n",
        "    model_specific_results = run_all_experiments(\n",
        "        app_instance=current_app,\n",
        "        system_briefs_data=system_briefs\n",
        "    )\n",
        "\n",
        "    # C. Store the results using the unique experiment name as the key.\n",
        "    all_experiment_results[experiment_name] = model_specific_results\n",
        "\n",
        "    print(f\"âœ… Finished experiment: {experiment_name}.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\nðŸŽ‰ All experiments completed for all configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3sBfmSnQBYGY"
      },
      "outputs": [],
      "source": [
        "# all_experiment_results['Gemini_Flash_Collaborative_2_Rounds'][0]['final_c4_model']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9revLYEAxY3t"
      },
      "source": [
        "#### Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oZb6FzlxzUIG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ”¬ Running All Evaluations (Judge: gemini-2.5-flash-preview-05-20)\n",
            "============================================================\n",
            "\n",
            "--- Evaluating Experiment: GPT4o_Mini_Simple ---\n",
            "\n",
            "################################################################################\n",
            "### âœ¨ PROCESSING ALL RESULTS FOR EXPERIMENT: GPT4o_Mini_Simple âœ¨ ###\n",
            "### Judge Model: gemini-2.5-flash-preview-05-20\n",
            "################################################################################\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Processing Run: 'Library Management System (LMS)' (Thread: 20250612-141410-library-management-system-lms-a77be777) ---\n",
            "================================================================================\n",
            "\n",
            "ðŸ’¾ Saving C4 model artifacts to: evaluation_results_openai/GPT4o_Mini_Simple/Library Management System (LMS)_20250612-141410-library-management-system-lms-a77be777/c4_artifacts\n",
            "  - Saved 1_context_analysis.md\n",
            "  - Saved 1_context_definition.yaml\n",
            "  - Saved 1_context_diagram.puml\n",
            "  - Saved 2_container_analysis.md\n",
            "  - Saved 2_container_definition.yaml\n",
            "  - Saved 2_container_diagram.puml\n",
            "  - Saved web_application_container_analysis.md\n",
            "  - Saved web_application_container_definition.yaml\n",
            "  - Saved web_application_container_diagram.puml\n",
            "  - Saved api_container_analysis.md\n",
            "  - Saved api_container_definition.yaml\n",
            "  - Saved api_container_diagram.puml\n",
            "  - Saved database_container_analysis.md\n",
            "  - Saved database_container_definition.yaml\n",
            "  - Saved database_container_diagram.puml\n",
            "  - Saved notification_service_container_analysis.md\n",
            "  - Saved notification_service_container_definition.yaml\n",
            "  - Saved notification_service_container_diagram.puml\n",
            "--- âœ… C4 Artifacts saved. ---\n",
            "\n",
            "ðŸ”¬ Running evaluation with judge model 'gemini-2.5-flash-preview-05-20'...\n",
            "\n",
            "==================================================\n",
            "ðŸ STARTING FULL C4 MODEL EVALUATION (Judge: gemini-2.5-flash-preview-05-20) ðŸ\n",
            "==================================================\n",
            "\n",
            "--- âš™ï¸  Instantiating model: gemini-2.5-flash-preview-05-20 ---\n",
            "--- Running Holistic Structural Checks ---\n",
            "ðŸ¤– Evaluating Metric: PlantUML Compilation Success...\n",
            "  - âœ… SUCCESS: '1_Context' compiled.\n",
            "  - âœ… SUCCESS: '2_Containers' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_Web Application Container' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_API Container' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_Database Container' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_Notification Service Container' compiled.\n",
            "ðŸ¤– Evaluating Metric: C4 Abstraction Adherence...\n",
            "ðŸ¤– Evaluating Metric: C4 Model Completeness...\n",
            "ðŸ¤– Evaluating Metric 6 (New): Emergent Naming Consistency...\n",
            "\n",
            "--- Running Level-Specific Evaluations ---\n",
            "  - Evaluating Context Level...\n",
            "âš–ï¸ Evaluating Metric 3: Semantic Consistency...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Context Diagram...\n",
            "  - Evaluating Container Level...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Container Diagram...\n",
            "  - Evaluating Component Level(s)...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Component: Web Application Container...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Component: API Container...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Component: Database Container...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Component: Notification Service Container...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Holistic Expert Critiques ---\n",
            "âš–ï¸ Evaluating Metric 7: Principal Architect's Critique...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ›¡ï¸  Evaluating Metric 8: Security 'Red Team' Assessment...\n",
            "\n",
            "\n",
            "==================================================\n",
            "ðŸ“‹ FINAL EVALUATION REPORT (Judge: gemini-2.5-flash-preview-05-20) ðŸ“‹\n",
            "==================================================\n",
            "\n",
            "{\n",
            "  \"evaluationMetadata\": {\n",
            "    \"judgeModel\": \"gemini-2.5-flash-preview-05-20\",\n",
            "    \"judgeModelTemperature\": 0.0,\n",
            "    \"evaluationTimestamp\": \"2025-06-12T14:17:32.895762\"\n",
            "  },\n",
            "  \"compilationSuccess\": {\n",
            "    \"metric\": \"Compilation Success Rate\",\n",
            "    \"score\": 100.0,\n",
            "    \"successful\": 6,\n",
            "    \"total\": 6,\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"source\": \"1_Context\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"2_Containers\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_Web Application Container\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_API Container\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_Database Container\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_Notification Service Container\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"abstractionAdherence\": {\n",
            "    \"metric\": \"Abstraction Adherence\",\n",
            "    \"score\": 33.33,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Component: Web Application Container\": {\n",
            "        \"status\": \"Fail\",\n",
            "        \"reason\": \"Required 'Container_Boundary' element appears to be missing.\"\n",
            "      },\n",
            "      \"Component: API Container\": {\n",
            "        \"status\": \"Fail\",\n",
            "        \"reason\": \"Required 'Container_Boundary' element appears to be missing.\"\n",
            "      },\n",
            "      \"Component: Database Container\": {\n",
            "        \"status\": \"Fail\",\n",
            "        \"reason\": \"Required 'Container_Boundary' element appears to be missing.\"\n",
            "      },\n",
            "      \"Component: Notification Service Container\": {\n",
            "        \"status\": \"Fail\",\n",
            "        \"reason\": \"Required 'Container_Boundary' element appears to be missing.\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"missingInformation\": {\n",
            "    \"metric\": \"C4 Model Completeness\",\n",
            "    \"score\": 100.0,\n",
            "    \"missing_count\": 0,\n",
            "    \"total_expected_artifacts\": 18,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Components\": {\n",
            "        \"Web Application Container\": {\n",
            "          \"analysis\": \"Present\",\n",
            "          \"yaml_definition\": \"Present\",\n",
            "          \"diagram\": \"Present\"\n",
            "        },\n",
            "        \"API Container\": {\n",
            "          \"analysis\": \"Present\",\n",
            "          \"yaml_definition\": \"Present\",\n",
            "          \"diagram\": \"Present\"\n",
            "        },\n",
            "        \"Database Container\": {\n",
            "          \"analysis\": \"Present\",\n",
            "          \"yaml_definition\": \"Present\",\n",
            "          \"diagram\": \"Present\"\n",
            "        },\n",
            "        \"Notification Service Container\": {\n",
            "          \"analysis\": \"Present\",\n",
            "          \"yaml_definition\": \"Present\",\n",
            "          \"diagram\": \"Present\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"emergentNamingConsistency\": {\n",
            "    \"metric\": \"Emergent Naming Consistency\",\n",
            "    \"score\": 100.0,\n",
            "    \"dominantConvention\": {\n",
            "      \"name\": \"other\",\n",
            "      \"count\": 24,\n",
            "      \"total\": 24\n",
            "    },\n",
            "    \"allConventionCounts\": {\n",
            "      \"other\": 24\n",
            "    },\n",
            "    \"details\": \"All names are internally consistent.\"\n",
            "  },\n",
            "  \"contextEvaluation\": {\n",
            "    \"semanticConsistency\": {\n",
            "      \"metric\": \"Semantic Consistency\",\n",
            "      \"score\": 60.0,\n",
            "      \"verified_items\": 6,\n",
            "      \"total_items\": 10\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Context Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"containerEvaluation\": {\n",
            "    \"definitionalConsistency\": {\n",
            "      \"score\": 100,\n",
            "      \"details\": {\n",
            "        \"message\": \"No containerss with a 'name' key found in the YAML definition.\"\n",
            "      }\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Container Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"componentEvaluations\": {\n",
            "    \"Web Application Container\": {\n",
            "      \"definitionalConsistency\": {\n",
            "        \"score\": 100,\n",
            "        \"details\": {\n",
            "          \"message\": \"No componentss with a 'name' key found in the YAML definition.\"\n",
            "        }\n",
            "      },\n",
            "      \"qualitativeRubric\": {\n",
            "        \"diagram_name\": \"Component: Web Application Container\",\n",
            "        \"average_score\": 0,\n",
            "        \"details\": {}\n",
            "      }\n",
            "    },\n",
            "    \"API Container\": {\n",
            "      \"definitionalConsistency\": {\n",
            "        \"score\": 100,\n",
            "        \"details\": {\n",
            "          \"message\": \"No componentss with a 'name' key found in the YAML definition.\"\n",
            "        }\n",
            "      },\n",
            "      \"qualitativeRubric\": {\n",
            "        \"diagram_name\": \"Component: API Container\",\n",
            "        \"average_score\": 0,\n",
            "        \"details\": {}\n",
            "      }\n",
            "    },\n",
            "    \"Database Container\": {\n",
            "      \"definitionalConsistency\": {\n",
            "        \"score\": 100,\n",
            "        \"details\": {\n",
            "          \"message\": \"No componentss with a 'name' key found in the YAML definition.\"\n",
            "        }\n",
            "      },\n",
            "      \"qualitativeRubric\": {\n",
            "        \"diagram_name\": \"Component: Database Container\",\n",
            "        \"average_score\": 0,\n",
            "        \"details\": {}\n",
            "      }\n",
            "    },\n",
            "    \"Notification Service Container\": {\n",
            "      \"definitionalConsistency\": {\n",
            "        \"score\": 100,\n",
            "        \"details\": {\n",
            "          \"message\": \"No componentss with a 'name' key found in the YAML definition.\"\n",
            "        }\n",
            "      },\n",
            "      \"qualitativeRubric\": {\n",
            "        \"diagram_name\": \"Component: Notification Service Container\",\n",
            "        \"average_score\": 0,\n",
            "        \"details\": {}\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"architectCritique\": {\n",
            "    \"metric\": \"Principal Architect's Critique\",\n",
            "    \"critique\": {\n",
            "      \"actionableRecommendation\": {\n",
            "        \"priority\": \"Critical\",\n",
            "        \"recommendation\": \"Introduce a dedicated search engine (e.g., Elasticsearch or Apache Solr) as a new container, integrated with the API Container, specifically for handling all catalog search and faceted browsing functionalities.\",\n",
            "        \"justification\": \"The current design implies that search and faceted browsing for up to 1 million items will be handled directly by the primary database (PostgreSQL or MongoDB). While these databases can perform searches, they are generally not optimized for the stringent performance requirements (median latency < 1s, 99th-percentile < 800ms) of complex, faceted searches on large datasets. A dedicated search engine is purpose-built for these scenarios, offering superior indexing, query performance, and scalability, and offloading this intensive workload from the transactional database.\"\n",
            "      },\n",
            "      \"feasibilityAndSoundness\": {\n",
            "        \"identifiedRisks\": [\n",
            "          \"The primary architectural risk is the ability to meet the stringent search performance non-functional requirements (R-03: <1s median latency, R-05: 99th-percentile < 800ms) for a catalog of up to 1 million items without a dedicated search engine. A general-purpose database is unlikely to provide this level of performance for complex, faceted queries.\",\n",
            "          \"The integration and ongoing management of 'Data Residency Compliance' and 'Bilingual UI Support' are vaguely defined as 'external systems' and 'compliance services'/'localization API'. While identified, the concrete architectural patterns or specific services/libraries to ensure these are met effectively and efficiently are not detailed, potentially leading to implementation complexities or compliance gaps.\",\n",
            "          \"Scalability of the database layer for 1 million items, especially under high concurrent read/write loads from cataloging and circulation, will require careful consideration of database sharding, replication, and indexing strategies, which are not detailed at this level but represent a potential future bottleneck.\"\n",
            "        ],\n",
            "        \"critique\": \"The technology choices (e.g., PostgreSQL/MongoDB, Spring Boot/Flask, React/Vue.js, RabbitMQ/Kafka) are realistic, align with the open-source preference, and are well-suited for a scalable web application. Hosting on Azure West Europe directly addresses the EU data residency constraint. The decomposition into distinct containers (Web App, API, Database, Notification Service) is logical and supports scalability by allowing independent scaling of components. The use of a message queue for notifications is a sound design pattern for asynchronous processing and reliability. However, the architecture's ability to meet the aggressive search performance requirements (R-03, R-05) for a catalog of up to 1 million items is questionable without a dedicated search engine. Relying solely on a general-purpose database for complex, faceted searches at sub-second latencies is a significant technical challenge and a potential bottleneck.\",\n",
            "        \"rating\": 3.0\n",
            "      },\n",
            "      \"executiveSummary\": \"The proposed Library Management System architecture, presented using the C4 model, provides a solid foundation with clear decomposition into containers and components. It demonstrates a good understanding of the functional and non-functional requirements, including EU data residency and open-source preferences. The design is generally sound and well-documented, but a critical gap exists in the approach to meeting stringent search performance requirements for a large catalog, which poses the most significant architectural risk.\",\n",
            "      \"clarityAndCommunication\": {\n",
            "        \"critique\": \"The C4 diagrams and accompanying YAML definitions are generally clear, consistent, and effectively communicate the architecture at each level. The linkage between the YAML definitions and the PlantUML diagrams is strong, making it easy to understand the elements and their relationships. However, the classification of 'Bilingual UI Support', 'Data Residency Compliance', and 'Cloud Infrastructure (Azure)' as distinct 'external systems' at the Context and Container levels is somewhat unconventional. While these are crucial concerns and dependencies, they are more accurately cross-cutting requirements, hosting environments, or integration points with specific services/libraries rather than standalone external systems that the LMS directly interacts with via a defined API in the same manner as an Email/SMS service. This slightly blurs the typical C4 definition of an external system.\",\n",
            "        \"rating\": 4.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"securityAssessment\": {\n",
            "    \"metric\": \"Security 'Red Team' Assessment\",\n",
            "    \"assessment\": {\n",
            "      \"vulnerabilities\": [\n",
            "        {\n",
            "          \"recommendation\": \"Implement an API Gateway (e.g., Azure API Management) in front of the API Container to centralize security controls, enforce policies, and provide a single entry point for API traffic.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"The architecture lacks an explicit API Gateway or a centralized component for API security enforcement (e.g., authentication, authorization, rate limiting, input validation). This pushes security responsibilities to the API Container, increasing its complexity and potential for misconfigurations or vulnerabilities.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Introduce a dedicated Identity Provider (IdP) or an IAM service (e.g., Azure AD B2C for patrons, Azure AD for staff/admins) to centralize user authentication, authorization, and session management.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"The diagram does not depict a dedicated authentication or authorization service. This suggests that the API Container might be handling user authentication and role-based access control, which can lead to dispersed security logic, potential for insecure implementations, and difficulty in managing user identities across the system.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement robust, centralized authorization checks at the API layer, ideally integrated with a dedicated IAM service. Ensure all API endpoints validate user permissions against the requested resource and action.\",\n",
            "          \"category\": \"Elevation of Privilege\",\n",
            "          \"severity\": \"Critical\",\n",
            "          \"description\": \"Without a clear, centralized authorization mechanism (as per the missing IAM service), there's a risk that the API Container might not properly validate if a user is authorized to access specific resources or perform certain actions, leading to Insecure Direct Object References (IDOR) or horizontal/vertical privilege escalation. Sensitive patron data in the Database Container is particularly vulnerable.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Deploy a WAF (e.g., Azure Application Gateway with WAF) in front of the Web Application. Implement strict input validation and output encoding in both Web Application and API. Ensure proper use of parameterized queries for database interactions. Implement security headers (CSP, X-Frame-Options, etc.) and CSRF tokens.\",\n",
            "          \"category\": \"Insecure Data Flow\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"The Web Application is directly exposed to patrons and staff. Without explicit mention of a Web Application Firewall (WAF), robust input validation, output encoding, and security headers, it is susceptible to common web vulnerabilities like Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and SQL Injection (if not properly parameterized queries are used by the API).\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement strict network segmentation using Virtual Networks (VNets) and Network Security Groups (NSGs) in Azure. Restrict ingress/egress traffic between containers to only necessary ports and protocols (e.g., Web Application can only talk to API on HTTPS, API can only talk to Database on PostgreSQL/MongoDB port).\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"The diagram does not explicitly show network segmentation or firewall rules between containers (e.g., Web Application to API, API to Database). This means if one container is compromised, an attacker could potentially move laterally to other containers without significant network-level barriers.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Ensure all communication with external services uses strong encryption (e.g., TLS 1.2+). Vet the security posture of the chosen external service provider. Implement strict data minimization, sending only necessary data. Consider anonymizing data where possible. Implement rate limiting for notification sending to prevent abuse.\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"category\": \"Information Disclosure\",\n",
            "          \"description\": \"The API sends patron contact information (email/SMS) to an external Email/SMS Notification Services. If this external service is compromised, or the communication channel is not adequately secured (e.g., mutual TLS), sensitive patron data could be intercepted or exposed. The external service itself could be a target for phishing campaigns using the system's identity.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement a centralized logging and monitoring solution (e.g., Azure Monitor, Azure Log Analytics, SIEM integration). Ensure all containers log relevant security events, access attempts, and errors. Configure alerts for suspicious activities.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"The architecture does not explicitly include components for centralized logging, monitoring, and alerting. Without these, detecting security incidents, identifying anomalous behavior, or performing forensic analysis after a breach would be significantly challenging.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement a dedicated secrets management solution (e.g., Azure Key Vault) to securely store and retrieve sensitive credentials and API keys. Ensure applications retrieve secrets at runtime and do not hardcode them.\",\n",
            "          \"severity\": \"High\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"description\": \"The diagram does not show a dedicated secrets management solution. Database credentials, API keys for external services (email, localization), and other sensitive configuration data might be stored insecurely (e.g., in environment variables, configuration files within containers), making them vulnerable to compromise if a container is breached.\"\n",
            "        }\n",
            "      ],\n",
            "      \"executiveSummary\": \"The Library Management System architecture, while following a standard multi-tiered design, exhibits several critical security weaknesses due to the absence of key security controls and explicit defense-in-depth measures. The direct exposure of the Web Application and the central role of the API Container, without an API Gateway or dedicated Identity and Access Management (IAM) service, significantly broaden the attack surface. Sensitive patron data stored in the Database Container is a primary target, and its protection relies heavily on the security posture of the API layer. External integrations, particularly with the Email/SMS Notification Services, introduce additional vectors for information disclosure. Overall, the system lacks robust security hardening, centralized security enforcement, and comprehensive monitoring capabilities, making it susceptible to various common web attacks, unauthorized access, and data breaches.\",\n",
            "      \"overallRiskScore\": 39\n",
            "    }\n",
            "  }\n",
            "}\n",
            "--- âœ… Evaluation complete. ---\n",
            "\n",
            "ðŸ’¾ Saving evaluation reports to: evaluation_results_openai/GPT4o_Mini_Simple/Library Management System (LMS)_20250612-141410-library-management-system-lms-a77be777/evaluation_reports\n",
            "  - Saved evaluation_report.json\n",
            "  - Saved evaluation_report.md\n",
            "--- âœ… Evaluation reports saved. ---\n",
            "\n",
            "\n",
            "################################################################################\n",
            "### ðŸŽ‰ ALL RESULTS PROCESSED FOR: GPT4o_Mini_Simple! ###\n",
            "################################################################################\n",
            "\n",
            "\n",
            "--- âœ… Consolidated evaluation reports saved to: evaluation_results_openai/GPT4o_Mini_Simple\\all_evaluation_summary.json ---\n",
            "Zipping the folder: 'evaluation_results_openai' into 'evaluation_results.zip'...\n",
            "âœ… Successfully created zip file: 'evaluation_results.zip'\n",
            "ðŸ“ You can find the zipped file here: c:\\Users\\kamil\\Documents\\Kamil\\HICSS\\experiments\\evaluation_results.zip\n",
            "\n",
            "\n",
            "ðŸŽ‰ All evaluations completed.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Define Your Judge Model ---\n",
        "JUDGE_MODEL = \"gemini-2.5-flash-preview-05-20\"\n",
        "\n",
        "\n",
        "# --- 2. Run Evaluations by Looping Through Experiment Configs ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸ”¬ Running All Evaluations (Judge: {JUDGE_MODEL})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_evaluation_summaries = {}\n",
        "\n",
        "# <<< CHANGED: Loop through the full configs, not just model names >>>\n",
        "for config in EXPERIMENT_CONFIGS:\n",
        "    experiment_name = config[\"name\"]\n",
        "    print(f\"\\n--- Evaluating Experiment: {experiment_name} ---\")\n",
        "\n",
        "    # Get the corresponding results for this specific experiment run\n",
        "    experiment_results = all_experiment_results.get(experiment_name)\n",
        "\n",
        "    if not experiment_results:\n",
        "        print(f\"âš ï¸  Warning: No results found for '{experiment_name}'. Skipping evaluation.\")\n",
        "        continue\n",
        "\n",
        "    # <<< CHANGED: Pass the entire config dictionary to the evaluator >>>\n",
        "    # This gives the function all the context it needs.\n",
        "    final_evaluation_summary = run_all_evaluations(\n",
        "        experiment_results=experiment_results,\n",
        "        experiment_config=config, # Pass the whole config\n",
        "        save_c4_artifacts_func=save_c4_artifacts,\n",
        "        run_full_evaluation_func=run_full_evaluation,\n",
        "        save_all_evaluation_reports_func=save_all_evaluation_reports,\n",
        "        format_evaluation_report_func=format_evaluation_report,\n",
        "        judge_model_name=JUDGE_MODEL\n",
        "    )\n",
        "\n",
        "    final_evaluation_summaries[experiment_name] = final_evaluation_summary\n",
        "\n",
        "    \n",
        "    zip_folder_with_increment(\"evaluation_results_openai\")\n",
        "\n",
        "print(\"\\n\\nðŸŽ‰ All evaluations completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47vKRhQueDf8"
      },
      "source": [
        "### Google experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3hQWfD5dTf0"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define the configurations for your experiments ---\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "\n",
        "# Instead of a simple list of models, we create a list of dictionaries.\n",
        "# Each dictionary is a complete configuration for a single experiment run.\n",
        "# This makes it easy to test different models, methods, and parameters.\n",
        "EXPERIMENT_CONFIGS = [\n",
        "    {\n",
        "        \"name\": \"Gemini_Flash_Collaborative_3_Rounds\",\n",
        "        \"model_name\": \"gemini-2.5-flash-preview-05-20\",\n",
        "        \"analysis_method\": \"collaborative\",\n",
        "        \"collab_rounds\": 3,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Gemini_Flash_Collaborative_1_Rounds\",\n",
        "        \"model_name\": \"gemini-2.5-flash-preview-05-20\",\n",
        "        \"analysis_method\": \"collaborative\",\n",
        "        \"collab_rounds\": 1,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Gemini_Flash_Simple\",\n",
        "        \"model_name\": \"gemini-2.5-flash-preview-05-20\",\n",
        "        \"analysis_method\": \"simple\",\n",
        "        \"collab_rounds\": None,  # Not applicable for the 'simple' method\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- 2. Run the experiments by looping through the configurations ---\n",
        "all_experiment_results = {}\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# We now loop through our list of configuration dictionaries.\n",
        "for config in EXPERIMENT_CONFIGS:\n",
        "    experiment_name = config[\"name\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"ðŸš€ Starting Experiment: {experiment_name}\")\n",
        "    print(f\"   - Model: {config['model_name']}\")\n",
        "    print(f\"   - Method: {config['analysis_method']}\")\n",
        "    if config['analysis_method'] == 'collaborative':\n",
        "        print(f\"   - Rounds: {config['collab_rounds']}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # A. Create a dedicated app instance using the full configuration.\n",
        "    # We pass all the parameters from our config dictionary.\n",
        "    current_app = create_c4_modeler_graph(\n",
        "        checkpointer=checkpointer,\n",
        "        model_name=config[\"model_name\"],\n",
        "        analysis_method=config[\"analysis_method\"],\n",
        "        collab_rounds=config[\"collab_rounds\"]\n",
        "    )\n",
        "\n",
        "    # B. Run your existing experiment function against this specific app.\n",
        "    model_specific_results = run_all_experiments(\n",
        "        app_instance=current_app,\n",
        "        system_briefs_data=system_briefs\n",
        "    )\n",
        "\n",
        "    # C. Store the results using the unique experiment name as the key.\n",
        "    all_experiment_results[experiment_name] = model_specific_results\n",
        "\n",
        "    print(f\"âœ… Finished experiment: {experiment_name}.\")\n",
        "\n",
        "print(\"\\n\\nðŸŽ‰ All experiments completed for all configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDOiD63pxbk2"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define Your Judge Model ---\n",
        "JUDGE_MODEL = \"gemini-2.5-flash-preview-05-20\"\n",
        "\n",
        "\n",
        "# --- 2. Run Evaluations by Looping Through Experiment Configs ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸ”¬ Running All Evaluations (Judge: {JUDGE_MODEL})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_evaluation_summaries = {}\n",
        "\n",
        "# <<< CHANGED: Loop through the full configs, not just model names >>>\n",
        "for config in EXPERIMENT_CONFIGS:\n",
        "    experiment_name = config[\"name\"]\n",
        "    print(f\"\\n--- Evaluating Experiment: {experiment_name} ---\")\n",
        "\n",
        "    # Get the corresponding results for this specific experiment run\n",
        "    experiment_results = all_experiment_results.get(experiment_name)\n",
        "\n",
        "    if not experiment_results:\n",
        "        print(f\"âš ï¸  Warning: No results found for '{experiment_name}'. Skipping evaluation.\")\n",
        "        continue\n",
        "\n",
        "    # <<< CHANGED: Pass the entire config dictionary to the evaluator >>>\n",
        "    # This gives the function all the context it needs.\n",
        "    final_evaluation_summary = run_all_evaluations(\n",
        "        experiment_results=experiment_results,\n",
        "        experiment_config=config, # Pass the whole config\n",
        "        save_c4_artifacts_func=save_c4_artifacts,\n",
        "        run_full_evaluation_func=run_full_evaluation,\n",
        "        save_all_evaluation_reports_func=save_all_evaluation_reports,\n",
        "        format_evaluation_report_func=format_evaluation_report,\n",
        "        judge_model_name=JUDGE_MODEL\n",
        "    )\n",
        "\n",
        "    final_evaluation_summaries[experiment_name] = final_evaluation_summary\n",
        "\n",
        "print(\"\\n\\nðŸŽ‰ All evaluations completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlMX3RqNxgeX"
      },
      "outputs": [],
      "source": [
        "# prompt: write a code to zip and download evaluation_results folder\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "folder_to_zip = 'evaluation_results'\n",
        "zip_filename = 'evaluation_results.zip'\n",
        "\n",
        "print(f\"Zipping the folder: '{folder_to_zip}'...\")\n",
        "\n",
        "try:\n",
        "    shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "    print(f\"Successfully created zip file: '{zip_filename}'\")\n",
        "    print(\"Starting download... Please wait.\")\n",
        "    print(\"Note: If the download doesn't start, check if your browser is blocking pop-ups for this site.\")\n",
        "\n",
        "    files.download(zip_filename)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The directory '{folder_to_zip}' was not found. Please ensure the previous script ran successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuDnc4QrxlF3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQbQ4JFheJrU"
      },
      "source": [
        "### DeepSeek experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6DlTj2bxnAI"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define the configurations for your experiments ---\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "\n",
        "\n",
        "# Instead of a simple list of models, we create a list of dictionaries.\n",
        "# Each dictionary is a complete configuration for a single experiment run.\n",
        "# This makes it easy to test different models, methods, and parameters.\n",
        "EXPERIMENT_CONFIGS = [\n",
        "    {\n",
        "        \"name\": \"DeepSeek_Collaborative_3_Rounds\",\n",
        "        \"model_name\": \"deepseek-chat\",\n",
        "        \"analysis_method\": \"collaborative\",\n",
        "        \"collab_rounds\": 3,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"DeepSeek_Collaborative_1_Rounds\",\n",
        "        \"model_name\": \"deepseek-chat\",\n",
        "        \"analysis_method\": \"collaborative\",\n",
        "        \"collab_rounds\": 1,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"DeepSeek_Simple\",\n",
        "        \"model_name\": \"deepseek-chat\",\n",
        "        \"analysis_method\": \"simple\",\n",
        "        \"collab_rounds\": None,  # Not applicable for the 'simple' method\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- 2. Run the experiments by looping through the configurations ---\n",
        "all_experiment_results = {}\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# We now loop through our list of configuration dictionaries.\n",
        "for config in EXPERIMENT_CONFIGS:\n",
        "    experiment_name = config[\"name\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"ðŸš€ Starting Experiment: {experiment_name}\")\n",
        "    print(f\"   - Model: {config['model_name']}\")\n",
        "    print(f\"   - Method: {config['analysis_method']}\")\n",
        "    if config['analysis_method'] == 'collaborative':\n",
        "        print(f\"   - Rounds: {config['collab_rounds']}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # A. Create a dedicated app instance using the full configuration.\n",
        "    # We pass all the parameters from our config dictionary.\n",
        "    current_app = create_c4_modeler_graph(\n",
        "        checkpointer=checkpointer,\n",
        "        model_name=config[\"model_name\"],\n",
        "        analysis_method=config[\"analysis_method\"],\n",
        "        collab_rounds=config[\"collab_rounds\"]\n",
        "    )\n",
        "\n",
        "    # B. Run your existing experiment function against this specific app.\n",
        "    model_specific_results = run_all_experiments(\n",
        "        app_instance=current_app,\n",
        "        system_briefs_data=system_briefs\n",
        "    )\n",
        "\n",
        "    # C. Store the results using the unique experiment name as the key.\n",
        "    all_experiment_results[experiment_name] = model_specific_results\n",
        "\n",
        "    print(f\"âœ… Finished experiment: {experiment_name}.\")\n",
        "\n",
        "print(\"\\n\\nðŸŽ‰ All experiments completed for all configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFD6xejExnAI"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define Your Judge Model ---\n",
        "JUDGE_MODEL = \"gemini-2.5-flash-preview-05-20\"\n",
        "\n",
        "\n",
        "# --- 2. Run Evaluations by Looping Through Experiment Configs ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸ”¬ Running All Evaluations (Judge: {JUDGE_MODEL})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_evaluation_summaries = {}\n",
        "\n",
        "# <<< CHANGED: Loop through the full configs, not just model names >>>\n",
        "for config in EXPERIMENT_CONFIGS:\n",
        "    experiment_name = config[\"name\"]\n",
        "    print(f\"\\n--- Evaluating Experiment: {experiment_name} ---\")\n",
        "\n",
        "    # Get the corresponding results for this specific experiment run\n",
        "    experiment_results = all_experiment_results.get(experiment_name)\n",
        "\n",
        "    if not experiment_results:\n",
        "        print(f\"âš ï¸  Warning: No results found for '{experiment_name}'. Skipping evaluation.\")\n",
        "        continue\n",
        "\n",
        "    # <<< CHANGED: Pass the entire config dictionary to the evaluator >>>\n",
        "    # This gives the function all the context it needs.\n",
        "    final_evaluation_summary = run_all_evaluations(\n",
        "        experiment_results=experiment_results,\n",
        "        experiment_config=config, # Pass the whole config\n",
        "        save_c4_artifacts_func=save_c4_artifacts,\n",
        "        run_full_evaluation_func=run_full_evaluation,\n",
        "        save_all_evaluation_reports_func=save_all_evaluation_reports,\n",
        "        format_evaluation_report_func=format_evaluation_report,\n",
        "        judge_model_name=JUDGE_MODEL\n",
        "    )\n",
        "\n",
        "    final_evaluation_summaries[experiment_name] = final_evaluation_summary\n",
        "\n",
        "print(\"\\n\\nðŸŽ‰ All evaluations completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JMWDIp0yFlx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gathering results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.3.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\kamil\\documents\\kamil\\hicss\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kamil\\documents\\kamil\\hicss\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\kamil\\documents\\kamil\\hicss\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/11.1 MB 1.2 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.8/11.1 MB 1.2 MB/s eta 0:00:09\n",
            "   --- ------------------------------------ 1.0/11.1 MB 1.4 MB/s eta 0:00:08\n",
            "   ----- ---------------------------------- 1.6/11.1 MB 1.5 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 2.1/11.1 MB 1.7 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 2.6/11.1 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 3.4/11.1 MB 2.1 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.2/11.1 MB 2.3 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 5.2/11.1 MB 2.5 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 6.0/11.1 MB 2.7 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 7.3/11.1 MB 3.0 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 8.4/11.1 MB 3.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 9.2/11.1 MB 3.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.0/11.1 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 3.4 MB/s eta 0:00:00\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   ---------------------------------------- 3/3 [pandas]\n",
            "\n",
            "Successfully installed pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12 experiment summary files to process.\n",
            "\n",
            "--- Average Scores Across All Test Cases ---\n",
            "                                    compilation_success  abstraction_adherence  completeness  naming_consistency  semantic_consistency  architect_feasibility_rating  security_risk_score\n",
            "model         method        rounds                                                                                                                                                       \n",
            "GPT4o         Collaborative 1                     89.78                 100.00        100.00               16.46                 25.62                           2.8                 31.6\n",
            "                            3                    100.00                 100.00        100.00                8.24                 34.12                           3.6                 36.6\n",
            "              Simple        N/A                   80.11                  97.50        100.00               60.85                 51.71                           4.0                 30.2\n",
            "GPT4omini     Collaborative 1                     92.14                  17.04        100.00                8.84                 48.07                           3.4                 33.4\n",
            "                            3                     88.96                  22.08        100.00               30.02                 41.58                           3.6                 32.6\n",
            "              Simple        N/A                   97.14                  28.25        100.00               42.91                 49.89                           3.2                 36.0\n",
            "Gemini15Flash Collaborative 1                    100.00                  95.00         81.46               82.00                 29.52                           2.4                 34.0\n",
            "                            3                     95.00                  78.33         77.10               66.52                 30.64                           1.8                 36.5\n",
            "              Simple        N/A                   66.19                  78.86        100.00               80.42                 48.68                           3.4                 33.2\n",
            "Grok3mini     Collaborative 1                     72.14                 100.00        100.00               24.74                 43.82                           3.4                 30.6\n",
            "                            3                     73.21                 100.00         98.33               14.14                 41.86                           3.2                 27.4\n",
            "              Simple        N/A                   58.10                 100.00        100.00               61.36                 48.98                           3.8                 36.8\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def extract_scores_from_report(report: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extracts all relevant metric scores from a single, complex report dictionary,\n",
        "    handling nested structures and averaging where necessary.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    # Layer 1: Holistic Metrics\n",
        "    scores['compilation_success'] = report.get('compilationSuccess', {}).get('score')\n",
        "    scores['abstraction_adherence'] = report.get('abstractionAdherence', {}).get('score')\n",
        "    scores['completeness'] = report.get('missingInformation', {}).get('score')\n",
        "    scores['naming_consistency'] = report.get('emergentNamingConsistency', {}).get('score')\n",
        "\n",
        "    # Layer 2: Level-Specific Metrics\n",
        "    scores['semantic_consistency'] = report.get('contextEvaluation', {}).get('semanticConsistency', {}).get('score')\n",
        "    scores['definitional_consistency_container'] = report.get('containerEvaluation', {}).get('definitionalConsistency', {}).get('score')\n",
        "\n",
        "    # Average the qualitative rubrics across all levels\n",
        "    qualitative_scores = []\n",
        "    for eval_key in ['contextEvaluation', 'containerEvaluation']:\n",
        "        if report.get(eval_key, {}).get('qualitativeRubric', {}).get('average_score') is not None:\n",
        "            qualitative_scores.append(report[eval_key]['qualitativeRubric']['average_score'])\n",
        "\n",
        "    for comp_eval in report.get('componentEvaluations', {}).values():\n",
        "        if comp_eval.get('qualitativeRubric', {}).get('average_score') is not None:\n",
        "            qualitative_scores.append(comp_eval['qualitativeRubric']['average_score'])\n",
        "    \n",
        "    if qualitative_scores:\n",
        "        scores['qualitative_rubric_avg'] = sum(qualitative_scores) / len(qualitative_scores)\n",
        "    else:\n",
        "        scores['qualitative_rubric_avg'] = None\n",
        "\n",
        "    # Layer 3: Expert Critiques (extracting a key rating)\n",
        "    scores['architect_feasibility_rating'] = report.get('architectCritique', {}).get('critique', {}).get('feasibilityAndSoundness', {}).get('rating')\n",
        "    scores['security_risk_score'] = report.get('securityAssessment', {}).get('assessment', {}).get('overallRiskScore')\n",
        "\n",
        "    # Clean up None values\n",
        "    return {k: v for k, v in scores.items() if v is not None}\n",
        "\n",
        "\n",
        "def gather_and_process_results(base_dir: str = \".\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Finds all experiment summary files, parses them, and aggregates the results\n",
        "    into a single pandas DataFrame.\n",
        "    \"\"\"\n",
        "    all_flat_results: List[Dict[str, Any]] = []\n",
        "    \n",
        "    # Use rglob to recursively find all summary files\n",
        "    summary_files = list(Path(base_dir).rglob('all_evaluation_summary.json'))\n",
        "    print(f\"Found {len(summary_files)} experiment summary files to process.\\n\")\n",
        "\n",
        "    for file_path in summary_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            \n",
        "            # The experiment name is the name of the parent directory\n",
        "            experiment_name = file_path.parent.name\n",
        "            \n",
        "            # Extract metadata from the experiment name\n",
        "            parts = experiment_name.split('_')\n",
        "            model = parts[0]\n",
        "            method = parts[1]\n",
        "            rounds = parts[2] if len(parts) > 3 else 'N/A'\n",
        "\n",
        "            # Process each test case within the file\n",
        "            for run_id, report in data.items():\n",
        "                flat_result = {\n",
        "                    \"experiment_name\": experiment_name,\n",
        "                    \"model\": model,\n",
        "                    \"method\": method,\n",
        "                    \"rounds\": rounds,\n",
        "                    \"run_id\": run_id\n",
        "                }\n",
        "                \n",
        "                # Extract scores from the report\n",
        "                scores = extract_scores_from_report(report)\n",
        "                flat_result.update(scores)\n",
        "                \n",
        "                all_flat_results.append(flat_result)\n",
        "\n",
        "        except (json.JSONDecodeError, KeyError) as e:\n",
        "            print(f\"Could not process file {file_path}: {e}\")\n",
        "    \n",
        "    return pd.DataFrame(all_flat_results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Process all results from the current directory downwards\n",
        "    results_df = gather_and_process_results()\n",
        "\n",
        "    if not results_df.empty:\n",
        "        # Define the columns we want to average\n",
        "        score_columns = [\n",
        "            'compilation_success',\n",
        "            'abstraction_adherence',\n",
        "            'completeness',\n",
        "            'naming_consistency',\n",
        "            'semantic_consistency',\n",
        "            # 'definitional_consistency_container',\n",
        "            # 'qualitative_rubric_avg',\n",
        "            'architect_feasibility_rating',\n",
        "            'security_risk_score'\n",
        "        ]\n",
        "        \n",
        "        # Ensure all score columns exist, fill missing with NaN\n",
        "        for col in score_columns:\n",
        "            if col not in results_df.columns:\n",
        "                results_df[col] = pd.NA\n",
        "\n",
        "        # Group by the experiment configuration and calculate the mean for score columns\n",
        "        summary = results_df.groupby(['model', 'method', 'rounds'])[score_columns].mean().round(2)\n",
        "\n",
        "        print(\"--- Average Scores Across All Test Cases ---\")\n",
        "        # Set pandas display options for better viewing\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        pd.set_option('display.width', 200)\n",
        "\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"No results were processed. Please check the directory structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12 experiment summary files to process.\n",
            "\n",
            "--- Average Scores Across All Test Cases ---\n",
            "                                    compilation_success  abstraction_adherence  completeness  naming_consistency  semantic_consistency  architect_feasibility_rating  \\\n",
            "model         method        rounds                                                                                                                                     \n",
            "GPT4o         Collaborative 1                     89.78                 100.00        100.00               16.46                 25.62                           2.8   \n",
            "                            3                    100.00                 100.00        100.00                8.24                 34.12                           3.6   \n",
            "              Simple        N/A                   80.11                  97.50        100.00               60.85                 51.71                           4.0   \n",
            "GPT4omini     Collaborative 1                     92.14                  17.04        100.00                8.84                 48.07                           3.4   \n",
            "                            3                     88.96                  22.08        100.00               30.02                 41.58                           3.6   \n",
            "              Simple        N/A                   97.14                  28.25        100.00               42.91                 49.89                           3.2   \n",
            "Gemini15Flash Collaborative 1                    100.00                  95.00         81.46               82.00                 29.52                           2.4   \n",
            "                            3                     95.00                  78.33         77.10               66.52                 30.64                           1.8   \n",
            "              Simple        N/A                   66.19                  78.86        100.00               80.42                 48.68                           3.4   \n",
            "Grok3mini     Collaborative 1                     72.14                 100.00        100.00               24.74                 43.82                           3.4   \n",
            "                            3                     73.21                 100.00         98.33               14.14                 41.86                           3.2   \n",
            "              Simple        N/A                   58.10                 100.00        100.00               61.36                 48.98                           3.8   \n",
            "\n",
            "                                    architect_clarity_communication_rating  security_risk_score  \n",
            "model         method        rounds                                                               \n",
            "GPT4o         Collaborative 1                                          2.4                 31.6  \n",
            "                            3                                          3.2                 36.6  \n",
            "              Simple        N/A                                        4.8                 30.2  \n",
            "GPT4omini     Collaborative 1                                          3.8                 33.4  \n",
            "                            3                                          3.8                 32.6  \n",
            "              Simple        N/A                                        4.2                 36.0  \n",
            "Gemini15Flash Collaborative 1                                          3.0                 34.0  \n",
            "                            3                                          2.8                 36.5  \n",
            "              Simple        N/A                                        4.0                 33.2  \n",
            "Grok3mini     Collaborative 1                                          3.6                 30.6  \n",
            "                            3                                          3.4                 27.4  \n",
            "              Simple        N/A                                        4.8                 36.8  \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def extract_scores_from_report(report: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extracts all relevant metric scores from a single, complex report dictionary,\n",
        "    handling nested structures and averaging where necessary.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    # Layer 1: Holistic Metrics\n",
        "    scores['compilation_success'] = report.get('compilationSuccess', {}).get('score')\n",
        "    scores['abstraction_adherence'] = report.get('abstractionAdherence', {}).get('score')\n",
        "    scores['completeness'] = report.get('missingInformation', {}).get('score')\n",
        "    scores['naming_consistency'] = report.get('emergentNamingConsistency', {}).get('score')\n",
        "\n",
        "    # Layer 2: Level-Specific Metrics\n",
        "    scores['semantic_consistency'] = report.get('contextEvaluation', {}).get('semanticConsistency', {}).get('score')\n",
        "    scores['definitional_consistency_container'] = report.get('containerEvaluation', {}).get('definitionalConsistency', {}).get('score')\n",
        "\n",
        "    # Average the qualitative rubrics across all levels\n",
        "    qualitative_scores = []\n",
        "    for eval_key in ['contextEvaluation', 'containerEvaluation']:\n",
        "        if report.get(eval_key, {}).get('qualitativeRubric', {}).get('average_score') is not None:\n",
        "            qualitative_scores.append(report[eval_key]['qualitativeRubric']['average_score'])\n",
        "\n",
        "    for comp_eval in report.get('componentEvaluations', {}).values():\n",
        "        if comp_eval.get('qualitativeRubric', {}).get('average_score') is not None:\n",
        "            qualitative_scores.append(comp_eval['qualitativeRubric']['average_score'])\n",
        "    \n",
        "    if qualitative_scores:\n",
        "        scores['qualitative_rubric_avg'] = sum(qualitative_scores) / len(qualitative_scores)\n",
        "    else:\n",
        "        scores['qualitative_rubric_avg'] = None\n",
        "\n",
        "    # Layer 3: Expert Critiques (extracting key ratings)\n",
        "    scores['architect_feasibility_rating'] = report.get('architectCritique', {}).get('critique', {}).get('feasibilityAndSoundness', {}).get('rating')\n",
        "    scores['architect_clarity_communication_rating'] = report.get('architectCritique', {}).get('critique', {}).get('clarityAndCommunication', {}).get('rating')\n",
        "    scores['security_risk_score'] = report.get('securityAssessment', {}).get('assessment', {}).get('overallRiskScore')\n",
        "\n",
        "    # Clean up None values\n",
        "    return {k: v for k, v in scores.items() if v is not None}\n",
        "\n",
        "\n",
        "def gather_and_process_results(base_dir: str = \".\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Finds all experiment summary files, parses them, and aggregates the results\n",
        "    into a single pandas DataFrame.\n",
        "    \"\"\"\n",
        "    all_flat_results: List[Dict[str, Any]] = []\n",
        "    \n",
        "    # Use rglob to recursively find all summary files\n",
        "    summary_files = list(Path(base_dir).rglob('all_evaluation_summary.json'))\n",
        "    print(f\"Found {len(summary_files)} experiment summary files to process.\\n\")\n",
        "\n",
        "    for file_path in summary_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            \n",
        "            # The experiment name is the name of the parent directory\n",
        "            experiment_name = file_path.parent.name\n",
        "            \n",
        "            # Extract metadata from the experiment name\n",
        "            parts = experiment_name.split('_')\n",
        "            model = parts[0]\n",
        "            method = parts[1]\n",
        "            rounds = parts[2] if len(parts) > 3 else 'N/A'\n",
        "\n",
        "            # Process each test case within the file\n",
        "            for run_id, report in data.items():\n",
        "                flat_result = {\n",
        "                    \"experiment_name\": experiment_name,\n",
        "                    \"model\": model,\n",
        "                    \"method\": method,\n",
        "                    \"rounds\": rounds,\n",
        "                    \"run_id\": run_id\n",
        "                }\n",
        "                \n",
        "                # Extract scores from the report\n",
        "                scores = extract_scores_from_report(report)\n",
        "                flat_result.update(scores)\n",
        "                \n",
        "                all_flat_results.append(flat_result)\n",
        "\n",
        "        except (json.JSONDecodeError, KeyError) as e:\n",
        "            print(f\"Could not process file {file_path}: {e}\")\n",
        "    \n",
        "    return pd.DataFrame(all_flat_results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Process all results from the current directory downwards\n",
        "    results_df = gather_and_process_results()\n",
        "\n",
        "    if not results_df.empty:\n",
        "        # Define the columns we want to average, including the new one\n",
        "        score_columns = [\n",
        "            'compilation_success',\n",
        "            'abstraction_adherence',\n",
        "            'completeness',\n",
        "            'naming_consistency',\n",
        "            'semantic_consistency',\n",
        "            # 'definitional_consistency_container', # Uncomment if you want to include this in the average\n",
        "            # 'qualitative_rubric_avg', # Uncomment if you want to include this in the average\n",
        "            'architect_feasibility_rating',\n",
        "            'architect_clarity_communication_rating', # Added new metric\n",
        "            'security_risk_score'\n",
        "        ]\n",
        "        \n",
        "        # Ensure all score columns exist, fill missing with NaN\n",
        "        for col in score_columns:\n",
        "            if col not in results_df.columns:\n",
        "                results_df[col] = pd.NA\n",
        "\n",
        "        # Group by the experiment configuration and calculate the mean for score columns\n",
        "        summary = results_df.groupby(['model', 'method', 'rounds'])[score_columns].mean().round(2)\n",
        "\n",
        "        print(\"--- Average Scores Across All Test Cases ---\")\n",
        "        # Set pandas display options for better viewing\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        pd.set_option('display.width', 200)\n",
        "\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"No results were processed. Please check the directory structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## C4 Model Evaluation Summary\n",
            "Below is a summary table of evaluation metrics for each experiment configuration. Each row represents a unique combination of model, method, and rounds. Scores are percentages unless otherwise noted (higher is better, except for Security Risk Score where lower is better).\n",
            "\n",
            "| Model         | Method        | Rounds   |   Compilation Success |   Abstraction Adherence |   Completeness |   Naming Consistency |   Semantic Consistency |   Architect Feasibility |   Security Risk Score |\n",
            "|:--------------|:--------------|:---------|----------------------:|------------------------:|---------------:|---------------------:|-----------------------:|------------------------:|----------------------:|\n",
            "| DeepSeek      | Collaborative | 1        |                100    |                  100    |         100    |                84.17 |                  45.01 |                     3.4 |                  30.4 |\n",
            "| DeepSeek      | Simple        | N/A      |                 97.14 |                  100    |         100    |                62.65 |                  50.5  |                     3.8 |                  32.8 |\n",
            "| GPT4o         | Collaborative | 1        |                 89.78 |                  100    |         100    |                16.46 |                  25.62 |                     2.8 |                  31.6 |\n",
            "| GPT4o         | Collaborative | 3        |                100    |                  100    |         100    |                 8.24 |                  34.12 |                     3.6 |                  36.6 |\n",
            "| GPT4o         | Simple        | N/A      |                 80.11 |                   97.5  |         100    |                60.85 |                  51.71 |                     4   |                  30.2 |\n",
            "| GPT4omini     | Collaborative | 1        |                 92.14 |                   17.04 |         100    |                 8.84 |                  48.07 |                     3.4 |                  33.4 |\n",
            "| GPT4omini     | Collaborative | 3        |                 88.96 |                   22.08 |         100    |                30.02 |                  41.58 |                     3.6 |                  32.6 |\n",
            "| GPT4omini     | Simple        | N/A      |                 97.14 |                   28.25 |         100    |                42.91 |                  49.89 |                     3.2 |                  36   |\n",
            "| Gemini15Flash | Collaborative | 1        |                100    |                   95    |          81.46 |                82    |                  29.52 |                     2.4 |                  34   |\n",
            "| Gemini15Flash | Collaborative | 3        |                 95    |                   78.33 |          77.1  |                66.52 |                  30.64 |                     1.8 |                  36.5 |\n",
            "| Gemini15Flash | Simple        | N/A      |                 66.19 |                   78.86 |         100    |                80.42 |                  48.68 |                     3.4 |                  33.2 |\n",
            "| Grok3mini     | Collaborative | 1        |                 72.14 |                  100    |         100    |                24.74 |                  43.82 |                     3.4 |                  30.6 |\n",
            "| Grok3mini     | Collaborative | 3        |                 73.21 |                  100    |          98.33 |                14.14 |                  41.86 |                     3.2 |                  27.4 |\n",
            "| Grok3mini     | Simple        | N/A      |                 58.1  |                  100    |         100    |                61.36 |                  48.98 |                     3.8 |                  36.8 |\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def summary_to_llm_readable(summary: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Converts the summary DataFrame into a markdown table string suitable for LLM input.\n",
        "    \"\"\"\n",
        "    # Reset index to get model/method/rounds as columns\n",
        "    df = summary.reset_index()\n",
        "    # Optionally, rename columns for clarity\n",
        "    df = df.rename(columns={\n",
        "        \"model\": \"Model\",\n",
        "        \"method\": \"Method\",\n",
        "        \"rounds\": \"Rounds\",\n",
        "        \"compilation_success\": \"Compilation Success\",\n",
        "        \"abstraction_adherence\": \"Abstraction Adherence\",\n",
        "        \"completeness\": \"Completeness\",\n",
        "        \"naming_consistency\": \"Naming Consistency\",\n",
        "        \"semantic_consistency\": \"Semantic Consistency\",\n",
        "        \"architect_feasibility_rating\": \"Architect Feasibility\",\n",
        "        \"security_risk_score\": \"Security Risk Score\"\n",
        "    })\n",
        "    # Format floats to 2 decimals\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == float:\n",
        "            df[col] = df[col].map(lambda x: f\"{x:.2f}\")\n",
        "    # Convert to markdown table\n",
        "    markdown_table = df.to_markdown(index=False)\n",
        "    # Add a short intro for LLM context\n",
        "    intro = (\n",
        "        \"## C4 Model Evaluation Summary\\n\"\n",
        "        \"Below is a summary table of evaluation metrics for each experiment configuration. \"\n",
        "        \"Each row represents a unique combination of model, method, and rounds. \"\n",
        "        \"Scores are percentages unless otherwise noted (higher is better, except for Security Risk Score where lower is better).\\n\\n\"\n",
        "    )\n",
        "    return intro + markdown_table\n",
        "\n",
        "# Example usage:\n",
        "llm_summary = summary_to_llm_readable(summary)\n",
        "print(llm_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Main Evaluation Function (Now much cleaner) ---\n",
        "\n",
        "def evaluate_cross_level_consistency(c4_model: Dict) -> Dict[str, Any]:\n",
        "    \"\"\"Measures two-way consistency of elements across C4 levels.\"\"\"\n",
        "    print(\"ðŸ¤– Evaluating Metric: Cross-Level Consistency Check...\")\n",
        "\n",
        "    # 1. Parse all relevant YAML definitions\n",
        "    context_data = parse_yaml_safe(c4_model.get(\"context\", {}).get(\"yaml_definition\"))\n",
        "    container_data = parse_yaml_safe(c4_model.get(\"containers\", {}).get(\"yaml_definition\"))\n",
        "    component_data_map = {\n",
        "        name: parse_yaml_safe(data.get(\"yaml_definition\", \"\"))\n",
        "        for name, data in c4_model.get(\"components\", {}).items()\n",
        "    }\n",
        "\n",
        "    all_details = {}\n",
        "\n",
        "    # 2. Run Context -> Container Check\n",
        "    if context_data and container_data:\n",
        "        is_consistent, reason = _check_context_to_container(context_data, container_data)\n",
        "        all_details[\"Context->Container\"] = {\"status\": \"Pass\" if is_consistent else \"Fail\", \"reason\": reason}\n",
        "\n",
        "    # 3. Run Container -> Component Checks\n",
        "    if container_data and component_data_map:\n",
        "        component_results = _check_container_to_components(container_data, component_data_map)\n",
        "        all_details.update(component_results)\n",
        "\n",
        "    # 4. Calculate Final Score\n",
        "    total_checks = len(all_details)\n",
        "    passed_checks = sum(1 for result in all_details.values() if result[\"status\"] == \"Pass\")\n",
        "    score = (passed_checks / total_checks) * 100 if total_checks > 0 else 100\n",
        "\n",
        "    return {\n",
        "        \"metric\": \"Cross-Level Consistency\",\n",
        "        \"score\": round(score, 2),\n",
        "        \"passed\": passed_checks,\n",
        "        \"total\": total_checks,\n",
        "        \"details\": all_details\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eval again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Define your system briefs in a dictionary (same as before) ---\n",
        "system_briefs = {\n",
        "    \"Library Management System (LMS)\": \"\"\"\n",
        "title: Library Management System\n",
        "description: >\n",
        "  A solution for public and academic libraries to catalogue items,\n",
        "  manage circulation, and provide self-service portals for patrons.\n",
        "domain: Education / Library Services\n",
        "constraints:\n",
        "  - \"EU-only data residency\"\n",
        "  - \"Open-source tech stack preferred\"\n",
        "  - \"Must support bilingual UI (EN/FR)\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Catalogue physical & digital items with MARC metadata\"\n",
        "  - id: R-02\n",
        "    desc: \"Patron self-checkout & returns via kiosks or mobile app\"\n",
        "  - id: R-03\n",
        "    desc: \"Search & faceted browse with <1 s median latency\"\n",
        "  - id: R-04\n",
        "    desc: \"Automated overdue notices by email/SMS\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Performance\n",
        "    desc: \"99th-percentile search response < 800 ms\"\n",
        "  - id: R-06\n",
        "    quality: Availability\n",
        "    desc: \"Service uptime â‰¥ 99.5%\"\n",
        "  - id: R-07\n",
        "    quality: Security\n",
        "    desc: \"Role-based access; yearly PEN tests\"\n",
        "  - id: R-08\n",
        "    quality: Scalability\n",
        "    desc: \"Catalogue up to 1 million items\"\n",
        "target_cloud:\n",
        "  provider: Azure\n",
        "  regions:\n",
        "    - westeurope\n",
        "\"\"\",\n",
        "    \"NextGen Point-of-Sale (POS)\": \"\"\"\n",
        "title: NextGen Point-of-Sale\n",
        "description: >\n",
        "  Store-front POS inspired by Craig Larmanâ€™s case study; supports bar-code\n",
        "  scanning, promotions, and offline queueing when networks fails.\n",
        "domain: Retail / Point-of-Sale\n",
        "constraints:\n",
        "  - \"PCI-DSS Level 1 compliance\"\n",
        "  - \"Offline transaction buffering â‰¤ 24 h\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Scan items & compute totals with tax rules per locale\"\n",
        "  - id: R-02\n",
        "    desc: \"Apply promotions & loyalty points in real time\"\n",
        "  - id: R-03\n",
        "    desc: \"Process card payments via Stripe Terminal\"\n",
        "  - id: R-04\n",
        "    desc: \"Print or email receipt with QR-code\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Performance\n",
        "    desc: \"Complete sale in â‰¤ 500 ms P95\"\n",
        "  - id: R-06\n",
        "    quality: Availability\n",
        "    desc: \"Uptime â‰¥ 99.9%\"\n",
        "  - id: R-07\n",
        "    quality: Reliability\n",
        "    desc: \"No lost sales during network outages\"\n",
        "  - id: R-08\n",
        "    quality: Usability\n",
        "    desc: \"Cashier workflow â‰¤ 4 clicks\"\n",
        "target_cloud:\n",
        "  provider: AWS\n",
        "  regions:\n",
        "    - us-east-1\n",
        "    - eu-west-1\n",
        "\"\"\",\n",
        "    \"Online Bookstore (Mini-Amazon)\": \"\"\"\n",
        "title: Online Bookstore\n",
        "description: >\n",
        "  A scaled-down Amazon-style e-commerce site for buying physical and\n",
        "  electronic books with recommendations and reviews.\n",
        "domain: Retail / E-commerce\n",
        "constraints:\n",
        "  - \"GDPR & CCPA compliance\"\n",
        "  - \"Multi-currency (USD, EUR, GBP)\"\n",
        "  - \"Integrate with third-party shipping APIs\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Browse & keyword search the catalogue\"\n",
        "  - id: R-02\n",
        "    desc: \"Shopping cart & secure checkout\"\n",
        "  - id: R-03\n",
        "    desc: \"Customer reviews & 5-star ratings\"\n",
        "  - id: R-04\n",
        "    desc: \"â€˜Customers also boughtâ€™ recommendations\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Scalability\n",
        "    desc: \"Handle 1 M MAU without degradation\"\n",
        "  - id: R-06\n",
        "    quality: Performance\n",
        "    desc: \"Page load < 2 s on 3G\"\n",
        "  - id: R-07\n",
        "    quality: Availability\n",
        "    desc: \"99.95% uptime\"\n",
        "  - id: R-08\n",
        "    quality: Security\n",
        "    desc: \"OWASP Top-10 mitigations\"\n",
        "target_cloud:\n",
        "  provider: AWS\n",
        "  regions:\n",
        "    - us-east-1\n",
        "    - eu-west-1\n",
        "    - ap-southeast-1\n",
        "\"\"\",\n",
        "    \"Student Information System (SIS)\": \"\"\"\n",
        "title: Student Information System\n",
        "description: >\n",
        "  Central system for universities to manage student records, enrollment,\n",
        "  grades, and transcripts across multiple campuses.\n",
        "domain: Education / Administration\n",
        "constraints:\n",
        "  - \"FERPA compliance (US) & GDPR (EU)\"\n",
        "  - \"Multi-campus tenancy\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Maintain student demographic & academic records\"\n",
        "  - id: R-02\n",
        "    desc: \"Online course enrollment & wait-listing\"\n",
        "  - id: R-03\n",
        "    desc: \"Faculty grade submission & change history\"\n",
        "  - id: R-04\n",
        "    desc: \"Generate official transcripts (PDF)\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Security\n",
        "    desc: \"Field-level encryption for PII\"\n",
        "  - id: R-06\n",
        "    quality: Integrity\n",
        "    desc: \"Immutable audit logs for grade changes\"\n",
        "  - id: R-07\n",
        "    quality: Availability\n",
        "    desc: \"Uptime â‰¥ 99.8% during term\"\n",
        "  - id: R-08\n",
        "    quality: Maintainability\n",
        "    desc: \"â‰¤ 20% mean time to repair (MTTR) per incident\"\n",
        "target_cloud:\n",
        "  provider: GCP\n",
        "  regions:\n",
        "    - us-east1\n",
        "    - europe-west4\n",
        "\"\"\",\n",
        "    \"Clinic Management System\": \"\"\"\n",
        "title: Clinic Management System\n",
        "description: >\n",
        "  Manages patient admissions, electronic medical records, scheduling,\n",
        "  and billing for medium-sized hospitals and clinics.\n",
        "domain: Healthcare / Clinical IT\n",
        "constraints:\n",
        "  - \"HIPAA & GDPR compliance\"\n",
        "  - \"High availability 99.99%\"\n",
        "  - \"Data retention â‰¥ 10 years\"\n",
        "functional_requirements:\n",
        "  - id: R-01\n",
        "    desc: \"Patient registration & demographic capture\"\n",
        "  - id: R-02\n",
        "    desc: \"Appointment scheduling with resource clash checks\"\n",
        "  - id: R-03\n",
        "    desc: \"Electronic Medical Record (EMR) with audit trail\"\n",
        "  - id: R-04\n",
        "    desc: \"Billing & insurance claim submission\"\n",
        "nonfunctional_requirements:\n",
        "  - id: R-05\n",
        "    quality: Security\n",
        "    desc: \"Access via multi-factor auth; AES-256 at rest\"\n",
        "  - id: R-06\n",
        "    quality: Availability\n",
        "    desc: \"Uptime â‰¥ 99.99% (active-active)\"\n",
        "  - id: R-07\n",
        "    quality: Performance\n",
        "    desc: \"EMR screen load < 1 s P95\"\n",
        "  - id: R-08\n",
        "    quality: Interoperability\n",
        "    desc: \"HL7 FHIR APIs for lab & imaging systems\"\n",
        "target_cloud:\n",
        "  provider: Hybrid\n",
        "  regions:\n",
        "    - on-prem-k8s\n",
        "    - eu-central-1\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Assume all other functions (get_llm, evaluate_*, check_c4_completeness) are defined\n",
        "# and that ModelName is a defined type.\n",
        "\n",
        "def run_full_evaluation(\n",
        "    system_brief: str,\n",
        "    c4_model: Dict,\n",
        "    judge_model_name: Any,\n",
        "    temperature: float = 0.0\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs a structured, level-aware evaluation of a C4 model.\n",
        "    \"\"\"\n",
        "    # ... (setup code is unchanged) ...\n",
        "    judge_llm = get_llm(model_name=judge_model_name, temperature=temperature)\n",
        "    report = {\n",
        "        'evaluationMetadata': {\n",
        "            \"judgeModel\": judge_model_name,\n",
        "            \"judgeModelTemperature\": temperature,\n",
        "            \"evaluationTimestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- Layer 1: Holistic Structural & Consistency Metrics ---\n",
        "    print(\"--- Running Holistic Structural Checks ---\")\n",
        "    report['compilationSuccess'] = evaluate_compilation_success(c4_model)\n",
        "    report['abstractionAdherence'] = evaluate_abstraction_adherence(c4_model)\n",
        "    report['missingInformation'] = check_c4_completeness(c4_model)\n",
        "    report['emergentNamingConsistency'] = evaluate_emergent_naming_consistency(c4_model)\n",
        "    # <<< ADDED: The missing cross-level consistency check >>>\n",
        "    report['crossLevelConsistency'] = evaluate_cross_level_consistency(c4_model)\n",
        "\n",
        "    # --- Layer 2: Level-Specific Semantic & Qualitative Evaluations ---\n",
        "    print(\"\\n--- Running Level-Specific Evaluations ---\")\n",
        "\n",
        "    # --- CONTEXT LEVEL EVALUATION ---\n",
        "    if \"context\" in c4_model:\n",
        "        print(\"  - Evaluating Context Level...\")\n",
        "        context_eval = {}\n",
        "        context_diag = c4_model[\"context\"].get(\"diagram\")\n",
        "        if context_diag:\n",
        "            # The brief IS the source of truth for the context diagram.\n",
        "            context_eval['semanticConsistency'] = evaluate_semantic_consistency(system_brief, c4_model, judge_llm)\n",
        "            # The qualitative rubric now gets the brief it needs to judge completeness.\n",
        "            context_eval['qualitativeRubric'] = evaluate_qualitative_rubric(context_diag, \"Context Diagram\", system_brief, judge_llm)\n",
        "        report['contextEvaluation'] = context_eval\n",
        "\n",
        "    # --- CONTAINER LEVEL EVALUATION ---\n",
        "    if \"containers\" in c4_model:\n",
        "        print(\"  - Evaluating Container Level...\")\n",
        "        container_eval = {}\n",
        "        container_diag = c4_model[\"containers\"].get(\"diagram\")\n",
        "        container_yaml = c4_model[\"containers\"].get(\"yaml_definition\")\n",
        "        if container_diag and container_yaml:\n",
        "            # The container YAML is the source of truth for the container diagram.\n",
        "            container_eval['definitionalConsistency'] = evaluate_definitional_consistency(container_yaml, container_diag, \"containers\")\n",
        "            # The qualitative rubric still needs the high-level brief for context.\n",
        "            container_eval['qualitativeRubric'] = evaluate_qualitative_rubric(container_diag, \"Container Diagram\", system_brief, judge_llm)\n",
        "        report['containerEvaluation'] = container_eval\n",
        "\n",
        "    # --- COMPONENT LEVEL EVALUATION ---\n",
        "    if \"components\" in c4_model:\n",
        "        print(\"  - Evaluating Component Level(s)...\")\n",
        "        component_evals = {}\n",
        "        for comp_name, comp_data in c4_model[\"components\"].items():\n",
        "            comp_diag = comp_data.get(\"diagram\")\n",
        "            comp_yaml = comp_data.get(\"yaml_definition\")\n",
        "            if comp_diag and comp_yaml:\n",
        "                # The component YAML is the source of truth for the component diagram.\n",
        "                consistency_result = evaluate_definitional_consistency(comp_yaml, comp_diag, \"components\")\n",
        "                # For component rubric, the system brief is still the best high-level context we have.\n",
        "                rubric_result = evaluate_qualitative_rubric(comp_diag, f\"Component: {comp_name}\", system_brief, judge_llm)\n",
        "                component_evals[comp_name] = {\n",
        "                    'definitionalConsistency': consistency_result,\n",
        "                    'qualitativeRubric': rubric_result\n",
        "                }\n",
        "        report['componentEvaluations'] = component_evals\n",
        "\n",
        "    # --- Layer 3: Holistic Expert Critiques ---\n",
        "    # These high-level critiques look at the model as a whole, so they run last.\n",
        "    print(\"\\n--- Running Holistic Expert Critiques ---\")\n",
        "    report['architectCritique'] = evaluate_architect_critique(system_brief, c4_model, judge_llm)\n",
        "    report['securityAssessment'] = evaluate_security_assessment(system_brief, c4_model, judge_llm)\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(f\"ðŸ“‹ FINAL EVALUATION REPORT (Judge: {judge_model_name}) ðŸ“‹\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    print(json.dumps(report, indent=2))\n",
        "\n",
        "    return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import yaml # You might need to run: pip install pyyaml\n",
        "\n",
        "def load_c4_model_from_artifacts(artifacts_dir: str) -> dict:\n",
        "    \"\"\"\n",
        "    Loads C4 model artifacts from a directory into a dictionary.\n",
        "\n",
        "    Args:\n",
        "        artifacts_dir (str): The path to the 'c4_artifacts' directory.\n",
        "\n",
        "    Returns:\n",
        "        dict: The reconstructed C4 model dictionary.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(artifacts_dir):\n",
        "        print(f\"Warning: Artifacts directory not found at {artifacts_dir}\")\n",
        "        return {}\n",
        "\n",
        "    c4_model = {\"context\": {}, \"containers\": {}, \"components\": {}}\n",
        "\n",
        "    def read_file_content(filepath):\n",
        "        \"\"\"Safely reads content from a file.\"\"\"\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except FileNotFoundError:\n",
        "            return None # Return None if a file doesn't exist\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filepath}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # --- Load Context Level ---\n",
        "    c4_model[\"context\"][\"analysis\"] = read_file_content(os.path.join(artifacts_dir, \"1_context_analysis.md\"))\n",
        "    c4_model[\"context\"][\"yaml_definition\"] = read_file_content(os.path.join(artifacts_dir, \"1_context_definition.yaml\"))\n",
        "    c4_model[\"context\"][\"diagram\"] = read_file_content(os.path.join(artifacts_dir, \"1_context_diagram.puml\"))\n",
        "\n",
        "    # --- Load Container Level ---\n",
        "    c4_model[\"containers\"][\"analysis\"] = read_file_content(os.path.join(artifacts_dir, \"2_container_analysis.md\"))\n",
        "    c4_model[\"containers\"][\"yaml_definition\"] = read_file_content(os.path.join(artifacts_dir, \"2_container_definition.yaml\"))\n",
        "    c4_model[\"containers\"][\"diagram\"] = read_file_content(os.path.join(artifacts_dir, \"2_container_diagram.puml\"))\n",
        "\n",
        "    # --- Load Component Level ---\n",
        "    component_dir = os.path.join(artifacts_dir, \"3_components\")\n",
        "    if os.path.isdir(component_dir):\n",
        "        # Find all unique component names by looking at the definition files\n",
        "        yaml_files = glob.glob(os.path.join(component_dir, \"*_definition.yaml\"))\n",
        "        \n",
        "        for yaml_path in yaml_files:\n",
        "            # Extract the sanitized container name from the filename\n",
        "            base_name = os.path.basename(yaml_path)\n",
        "            # e.g., \"my_api_container_definition.yaml\" -> \"my_api_container\"\n",
        "            safe_container_name = base_name.replace('_definition.yaml', '')\n",
        "            \n",
        "            # The original container name should be inside the yaml definition itself.\n",
        "            # This is a more robust way to get the original key.\n",
        "            original_container_name = safe_container_name # Fallback\n",
        "            try:\n",
        "                with open(yaml_path, 'r', encoding='utf-8') as f:\n",
        "                    # Assuming the YAML has a root key that is the container name\n",
        "                    comp_data = yaml.safe_load(f)\n",
        "                    if isinstance(comp_data, dict) and 'container' in comp_data:\n",
        "                         original_container_name = comp_data['container']\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not parse container name from {yaml_path}: {e}\")\n",
        "\n",
        "\n",
        "            c4_model[\"components\"][original_container_name] = {\n",
        "                \"analysis\": read_file_content(os.path.join(component_dir, f\"{safe_container_name}_analysis.md\")),\n",
        "                \"yaml_definition\": read_file_content(yaml_path),\n",
        "                \"diagram\": read_file_content(os.path.join(component_dir, f\"{safe_container_name}_diagram.puml\"))\n",
        "            }\n",
        "            \n",
        "    # Clean up empty keys for tidiness\n",
        "    c4_model = {k: v for k, v in c4_model.items() if v and any(val is not None for val in v.values())}\n",
        "\n",
        "    return c4_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "################################################################################\n",
            "# Starting provider: GEMINI\n",
            "################################################################################\n",
            "ðŸš€ Starting re-evaluation process with judge: gemini-2.5-flash-preview-05-20 ðŸš€\n",
            "Found 15 test cases to re-evaluate.\n",
            "\n",
            "============================================================\n",
            "Processing Test Case: Clinic Management System_20250613-115753-clinic-management-system-35ef7531\n",
            "============================================================\n",
            "âœ… Found matching system brief: 'Clinic Management System'\n",
            "âœ… C4 model loaded successfully.\n",
            "--- âš™ï¸  Instantiating model: gemini-2.5-flash-preview-05-20 ---\n",
            "--- Running Holistic Structural Checks ---\n",
            "ðŸ¤– Evaluating Metric: PlantUML Compilation Success...\n",
            "  - âœ… SUCCESS: '1_Context' compiled.\n",
            "  - âœ… SUCCESS: '2_Containers' compiled.\n",
            "ðŸ¤– Evaluating Metric: C4 Abstraction Adherence...\n",
            "ðŸ¤– Evaluating Metric: C4 Model Completeness...\n",
            "ðŸ¤– Evaluating Metric 6 (New): Emergent Naming Consistency...\n",
            "ðŸ¤– Evaluating Metric: Cross-Level Consistency Check...\n",
            "\n",
            "--- Running Level-Specific Evaluations ---\n",
            "  - Evaluating Context Level...\n",
            "âš–ï¸ Evaluating Metric 3: Semantic Consistency...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Context Diagram...\n",
            "  - Evaluating Container Level...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Container Diagram...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Holistic Expert Critiques ---\n",
            "âš–ï¸ Evaluating Metric 7: Principal Architect's Critique...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ›¡ï¸  Evaluating Metric 8: Security 'Red Team' Assessment...\n",
            "\n",
            "\n",
            "==================================================\n",
            "ðŸ“‹ FINAL EVALUATION REPORT (Judge: gemini-2.5-flash-preview-05-20) ðŸ“‹\n",
            "==================================================\n",
            "\n",
            "{\n",
            "  \"evaluationMetadata\": {\n",
            "    \"judgeModel\": \"gemini-2.5-flash-preview-05-20\",\n",
            "    \"judgeModelTemperature\": 0.0,\n",
            "    \"evaluationTimestamp\": \"2025-06-13T17:08:50.534119\"\n",
            "  },\n",
            "  \"compilationSuccess\": {\n",
            "    \"metric\": \"Compilation Success Rate\",\n",
            "    \"score\": 100.0,\n",
            "    \"successful\": 2,\n",
            "    \"total\": 2,\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"source\": \"1_Context\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"2_Containers\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"abstractionAdherence\": {\n",
            "    \"metric\": \"Abstraction Adherence\",\n",
            "    \"score\": 100.0,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"missingInformation\": {\n",
            "    \"metric\": \"C4 Model Completeness\",\n",
            "    \"score\": 100.0,\n",
            "    \"missing_count\": 0,\n",
            "    \"total_expected_artifacts\": 6,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Components\": {}\n",
            "    }\n",
            "  },\n",
            "  \"emergentNamingConsistency\": {\n",
            "    \"metric\": \"Emergent Naming Consistency\",\n",
            "    \"score\": 100.0,\n",
            "    \"dominantConvention\": {\n",
            "      \"name\": \"other\",\n",
            "      \"count\": 1,\n",
            "      \"total\": 1\n",
            "    },\n",
            "    \"allConventionCounts\": {\n",
            "      \"other\": 1\n",
            "    },\n",
            "    \"details\": \"All names are internally consistent.\"\n",
            "  },\n",
            "  \"crossLevelConsistency\": {\n",
            "    \"metric\": \"Cross-Level Consistency\",\n",
            "    \"score\": 100.0,\n",
            "    \"passed\": 1,\n",
            "    \"total\": 1,\n",
            "    \"details\": {\n",
            "      \"Context->Container\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"External elements are consistent with the Context level.\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"contextEvaluation\": {\n",
            "    \"semanticConsistency\": {\n",
            "      \"metric\": \"Semantic Consistency\",\n",
            "      \"score\": 36.36,\n",
            "      \"verified_items\": 4,\n",
            "      \"total_items\": 11\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Context Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"containerEvaluation\": {\n",
            "    \"definitionalConsistency\": {\n",
            "      \"score\": 100,\n",
            "      \"details\": {\n",
            "        \"message\": \"No containerss with a 'name' key found in the YAML definition.\"\n",
            "      }\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Container Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"architectCritique\": {\n",
            "    \"metric\": \"Principal Architect's Critique\",\n",
            "    \"critique\": {\n",
            "      \"actionableRecommendation\": {\n",
            "        \"priority\": \"Critical\",\n",
            "        \"recommendation\": \"Detail the internal components (containers/services) of the Clinic Management System, including core services (e.g., Patient Management, EMR, Scheduling, Billing) and their associated data stores, along with their interactions.\",\n",
            "        \"justification\": \"The current Container level diagram and definition are severely lacking in detail, showing only an API Gateway and external systems. Without a clear decomposition into internal services and data stores, it's impossible to assess the architecture's feasibility, scalability, performance, security, or compliance with critical non-functional requirements like HIPAA/GDPR, 99.99% availability, and EMR screen load times. This is the foundational next step required to validate the design.\"\n",
            "      },\n",
            "      \"feasibilityAndSoundness\": {\n",
            "        \"identifiedRisks\": [\n",
            "          \"Lack of internal system decomposition: Without defined internal services and data stores, it's impossible to validate how functional and non-functional requirements (especially EMR performance, high availability, and compliance) will be met.\",\n",
            "          \"Data management and compliance: The architecture provides no details on how 'Data retention \\u2265 10 years' and 'AES-256 at rest' will be implemented, or how HIPAA/GDPR compliance will be ensured across data stores and services.\",\n",
            "          \"Scalability and high availability implementation: The 'active-active' 99.99% availability and performance requirements are significant, but the current design offers no insight into how these will be achieved across internal components.\"\n",
            "        ],\n",
            "        \"critique\": \"The technology choices are currently too abstract to fully assess. The mention of 'on-prem-k8s' and 'eu-central-1' suggests a Kubernetes-centric hybrid cloud approach, which is generally sound for scalability and availability, but no internal services are defined to leverage this. The decomposition is virtually non-existent at the Container level, making it impossible to evaluate how well it supports scalability, performance (e.g., EMR screen load < 1s P95), or the 'active-active' 99.99% availability requirement. The critical non-functional requirements (HIPAA/GDPR, data retention, security) are stated but not addressed or demonstrated in the current architectural representation.\",\n",
            "        \"rating\": 2.0\n",
            "      },\n",
            "      \"executiveSummary\": \"The proposed Clinic Management System architecture provides a clear Context Level view, but the Container Level is critically underdeveloped. While the brief outlines important functional and non-functional requirements, the current architectural diagrams lack the necessary internal decomposition and detail to assess feasibility, scalability, performance, or compliance with critical requirements like HIPAA/GDPR and high availability. The absence of internal service and data store definitions is the primary gap.\",\n",
            "      \"clarityAndCommunication\": {\n",
            "        \"critique\": \"The Context Level diagram and its YAML definition are clear, concise, and effectively communicate the system's boundaries and external interactions. They provide a good high-level overview. However, the Container Level diagram and its corresponding YAML definition are extremely sparse. They only show the API Gateway and external systems, completely omitting any internal containers (services, databases, message queues, etc.) that would represent the actual decomposition of the 'Clinic Management System'. This makes the Container level almost entirely ineffective at communicating the internal architecture and the link between the definitions and diagrams is broken at this level due to the missing internal components.\",\n",
            "        \"rating\": 2.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"securityAssessment\": {\n",
            "    \"metric\": \"Security 'Red Team' Assessment\",\n",
            "    \"assessment\": {\n",
            "      \"vulnerabilities\": [\n",
            "        {\n",
            "          \"recommendation\": \"Expand the C4 diagram to include all internal containers (e.g., EMR Service, Scheduling Service, User Service, Database containers) and their relationships. Clearly define data flows, especially for sensitive patient data.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Critical\",\n",
            "          \"description\": \"The C4 Container diagram lacks detail on internal services, databases, and their interconnections. This obscurity prevents a thorough analysis of internal data flows, access controls, and potential lateral movement paths for an attacker.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Specify the communication protocols and security mechanisms (e.g., mTLS, internal network segmentation, least privilege access) for all internal data flows, particularly those involving sensitive patient information.\",\n",
            "          \"severity\": \"High\",\n",
            "          \"category\": \"Insecure Data Flow\",\n",
            "          \"description\": \"While the API Gateway handles external HTTPS, the internal data flows between the API Gateway and backend services/databases (which are not shown) are undefined. This poses a risk of insecure communication channels or improper access controls between internal components, especially for sensitive patient data.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Introduce a dedicated Identity and Access Management (IAM) service or component responsible for user authentication (including MFA), authorization, and session management. Ensure this service enforces least privilege principles.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Critical\",\n",
            "          \"description\": \"The system brief mentions multi-factor authentication (MFA) as a security requirement, but the architecture diagram does not depict a dedicated authentication and authorization service. Without a clear component responsible for identity management, session management, and access control, there's a high risk of authentication bypasses or unauthorized access.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Include database containers in the diagram. Detail the mechanisms for data encryption at rest (e.g., disk encryption, transparent data encryption, application-level encryption) and how encryption keys are managed and protected.\",\n",
            "          \"severity\": \"High\",\n",
            "          \"category\": \"Information Disclosure\",\n",
            "          \"description\": \"The system handles highly sensitive patient data (EMR, billing). While AES-256 at rest is a requirement, the diagram does not show any database containers or explicit components responsible for data storage and encryption. This lack of visibility raises concerns about how data at rest is protected and who has access to encryption keys.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement robust network segmentation (e.g., using VPCs, subnets, security groups, network policies in Kubernetes) to isolate components based on their sensitivity and function. Apply strict firewall rules to enforce least-privilege network access.\",\n",
            "          \"severity\": \"High\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"description\": \"The diagram shows an API Gateway as the sole entry point, but there's no indication of network segmentation or internal firewalls protecting backend services and databases from each other or from potential compromise of the API Gateway.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement comprehensive rate limiting, API throttling, and Web Application Firewall (WAF) capabilities at the API Gateway level. Consider integrating with cloud-native DDoS protection services.\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"category\": \"Denial of Service\",\n",
            "          \"description\": \"The API Gateway is the single point of entry for all users. Without explicit mention of rate limiting, WAF, or DDoS protection mechanisms, it is highly susceptible to Denial of Service (DoS) attacks, which could impact system availability (a critical NFR).\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Integrate a centralized logging system, robust monitoring, and a SIEM solution to collect, analyze, and alert on security-relevant events across all components. Ensure audit trails are immutable and protected.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"The EMR system requires an audit trail (R-03). However, the architecture diagram does not depict any dedicated logging, monitoring, or security information and event management (SIEM) components. Without these, detecting and responding to security incidents will be severely hampered.\"\n",
            "        }\n",
            "      ],\n",
            "      \"executiveSummary\": \"The current C4 Container diagram provides a high-level view, primarily focusing on external interactions via an API Gateway. While an API Gateway is a good starting point for external access control, the complete internal architecture, including data stores, internal services, and critical security controls like dedicated authentication/authorization services, firewalls, and detailed data flow for sensitive information, is missing. This lack of detail makes a comprehensive threat model challenging and indicates significant architectural gaps that need to be addressed to meet HIPAA/GDPR compliance and security non-functional requirements. The primary attack surface is the API Gateway, and without internal segmentation and explicit security controls, any compromise of this gateway could lead to widespread data breaches.\",\n",
            "      \"overallRiskScore\": 39\n",
            "    }\n",
            "  }\n",
            "}\n",
            "âœ… Successfully saved new evaluation report to: evaluation_results_gemini\\Gemini15Flash_Collaborative_1_Rounds\\Clinic Management System_20250613-115753-clinic-management-system-35ef7531\\evaluation_report_judged_by_gemini_2.5_flash_preview_05_20.json\n",
            "\n",
            "============================================================\n",
            "Processing Test Case: Library Management System (LMS)_20250613-115339-library-management-system-lms-a5b26a86\n",
            "============================================================\n",
            "âœ… Found matching system brief: 'Library Management System (LMS)'\n",
            "âœ… C4 model loaded successfully.\n",
            "--- âš™ï¸  Instantiating model: gemini-2.5-flash-preview-05-20 ---\n",
            "--- Running Holistic Structural Checks ---\n",
            "ðŸ¤– Evaluating Metric: PlantUML Compilation Success...\n",
            "  - âœ… SUCCESS: '1_Context' compiled.\n",
            "  - âœ… SUCCESS: '2_Containers' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_account_management' compiled.\n",
            "  - âŒ FAILED: '3_Component_catalog_search' content is empty.\n",
            "ðŸ¤– Evaluating Metric: C4 Abstraction Adherence...\n",
            "ðŸ¤– Evaluating Metric: C4 Model Completeness...\n",
            "ðŸ¤– Evaluating Metric 6 (New): Emergent Naming Consistency...\n",
            "ðŸ¤– Evaluating Metric: Cross-Level Consistency Check...\n",
            "\n",
            "--- Running Level-Specific Evaluations ---\n",
            "  - Evaluating Context Level...\n",
            "âš–ï¸ Evaluating Metric 3: Semantic Consistency...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Context Diagram...\n",
            "  - Evaluating Container Level...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Container Diagram...\n",
            "  - Evaluating Component Level(s)...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Component: account_management...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Holistic Expert Critiques ---\n",
            "âš–ï¸ Evaluating Metric 7: Principal Architect's Critique...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ›¡ï¸  Evaluating Metric 8: Security 'Red Team' Assessment...\n",
            "\n",
            "\n",
            "==================================================\n",
            "ðŸ“‹ FINAL EVALUATION REPORT (Judge: gemini-2.5-flash-preview-05-20) ðŸ“‹\n",
            "==================================================\n",
            "\n",
            "{\n",
            "  \"evaluationMetadata\": {\n",
            "    \"judgeModel\": \"gemini-2.5-flash-preview-05-20\",\n",
            "    \"judgeModelTemperature\": 0.0,\n",
            "    \"evaluationTimestamp\": \"2025-06-13T17:09:46.598520\"\n",
            "  },\n",
            "  \"compilationSuccess\": {\n",
            "    \"metric\": \"Compilation Success Rate\",\n",
            "    \"score\": 75.0,\n",
            "    \"successful\": 3,\n",
            "    \"total\": 4,\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"source\": \"1_Context\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"2_Containers\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_account_management\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_catalog_search\",\n",
            "        \"status\": \"Failed - Empty\",\n",
            "        \"error\": \"Diagram content was empty or whitespace.\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"abstractionAdherence\": {\n",
            "    \"metric\": \"Abstraction Adherence\",\n",
            "    \"score\": 100.0,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Component: account_management\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"missingInformation\": {\n",
            "    \"metric\": \"C4 Model Completeness\",\n",
            "    \"score\": 90.0,\n",
            "    \"missing_count\": 1,\n",
            "    \"total_expected_artifacts\": 10,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Components\": {\n",
            "        \"account_management\": {\n",
            "          \"analysis\": \"Present\",\n",
            "          \"yaml_definition\": \"Present\",\n",
            "          \"diagram\": \"Present\"\n",
            "        },\n",
            "        \"catalog_search\": {\n",
            "          \"analysis\": \"Missing\",\n",
            "          \"yaml_definition\": \"Not Expected\",\n",
            "          \"diagram\": \"Not Expected\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"emergentNamingConsistency\": {\n",
            "    \"metric\": \"Emergent Naming Consistency\",\n",
            "    \"score\": 100.0,\n",
            "    \"dominantConvention\": {\n",
            "      \"name\": \"other\",\n",
            "      \"count\": 6,\n",
            "      \"total\": 6\n",
            "    },\n",
            "    \"allConventionCounts\": {\n",
            "      \"other\": 6\n",
            "    },\n",
            "    \"details\": \"All names are internally consistent.\"\n",
            "  },\n",
            "  \"crossLevelConsistency\": {\n",
            "    \"metric\": \"Cross-Level Consistency\",\n",
            "    \"score\": 50.0,\n",
            "    \"passed\": 1,\n",
            "    \"total\": 2,\n",
            "    \"details\": {\n",
            "      \"Context->Container\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"External elements are consistent with the Context level.\"\n",
            "      },\n",
            "      \"Container->Component (account_management)\": {\n",
            "        \"status\": \"Fail\",\n",
            "        \"reason\": \"References elements not found in Container scope: ['Password Management Component', 'Authentication Component', 'Dependency Management Component']\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"contextEvaluation\": {\n",
            "    \"semanticConsistency\": {\n",
            "      \"metric\": \"Semantic Consistency\",\n",
            "      \"score\": 30.0,\n",
            "      \"verified_items\": 3,\n",
            "      \"total_items\": 10\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Context Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"containerEvaluation\": {\n",
            "    \"definitionalConsistency\": {\n",
            "      \"score\": 100,\n",
            "      \"details\": {\n",
            "        \"message\": \"No containerss with a 'name' key found in the YAML definition.\"\n",
            "      }\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Container Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"componentEvaluations\": {\n",
            "    \"account_management\": {\n",
            "      \"definitionalConsistency\": {\n",
            "        \"score\": 100,\n",
            "        \"details\": {\n",
            "          \"message\": \"No componentss with a 'name' key found in the YAML definition.\"\n",
            "        }\n",
            "      },\n",
            "      \"qualitativeRubric\": {\n",
            "        \"diagram_name\": \"Component: account_management\",\n",
            "        \"average_score\": 0,\n",
            "        \"details\": {}\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"architectCritique\": {\n",
            "    \"metric\": \"Principal Architect's Critique\",\n",
            "    \"critique\": {\n",
            "      \"actionableRecommendation\": {\n",
            "        \"priority\": \"Critical\",\n",
            "        \"recommendation\": \"Standardize and clearly define the core technology stack across all architectural levels, and complete the container-level decomposition to include all primary functional areas of the Library Management System.\",\n",
            "        \"justification\": \"The current architecture presents conflicting technology choices (Python/Django/React Native vs. Java Spring Boot) which makes the design fundamentally unclear and unfeasible. Furthermore, critical functional requirements such as item cataloging, circulation management, and notifications are not represented as distinct containers, indicating an incomplete and unsound decomposition that will lead to significant architectural gaps and scalability challenges for core business operations. Resolving these issues is paramount for a coherent and viable design.\"\n",
            "      },\n",
            "      \"feasibilityAndSoundness\": {\n",
            "        \"identifiedRisks\": [\n",
            "          \"Inconsistent Technology Stack: The conflicting declaration of Python/Django/React Native at the context level and Java Spring Boot at the container/component levels creates ambiguity and suggests a lack of a cohesive technical vision.\",\n",
            "          \"Incomplete Functional Decomposition: Key functional areas such as item cataloging, circulation management, and overdue notices are not represented as distinct containers, leading to an incomplete and potentially unscalable design for core library operations.\",\n",
            "          \"Data Residency Enforcement: While Azure West Europe is chosen, the architecture lacks details on how EU-only data residency is strictly enforced at the application and data storage layers, especially concerning third-party integrations or data replication.\"\n",
            "        ],\n",
            "        \"critique\": \"The initial technology choices (Python, Django, PostgreSQL, Elasticsearch, Redis, React Native) are generally sound for a library management system of this scale, aligning with the open-source preference. Elasticsearch is a strong choice for the search performance requirement. However, the subsequent container-level diagrams introduce Java Spring Boot, creating a major inconsistency in the core technology stack. The decomposition into 'Catalog Search' and 'Account Management' is a reasonable start for microservices, but it critically omits containers for core library functions like 'Item Cataloging', 'Circulation Management' (for R-01, R-02), and 'Notifications' (for R-04). This omission makes the architecture incomplete and raises concerns about how core business logic would be handled and scaled. The 'Dependency Management Component' using 'pip' within a Java Spring Boot context further highlights the technology confusion. While Azure West Europe addresses data residency, the architecture doesn't detail how this is enforced at the application or data layer.\",\n",
            "        \"rating\": 2.0\n",
            "      },\n",
            "      \"executiveSummary\": \"The architecture provides a basic C4 model representation but suffers from significant inconsistencies in the declared technology stack and an incomplete decomposition of core functional areas at the container level, making it difficult to assess its full feasibility and soundness.\",\n",
            "      \"clarityAndCommunication\": {\n",
            "        \"critique\": \"The use of C4 PlantUML and accompanying YAML definitions is a good approach for communication. However, the content suffers from critical inconsistencies and omissions. At the Context Level, including 'Azure' as an ExternalSystem interacting with the LMS is conceptually incorrect in a C4 context diagram; Azure is the platform. The PlantUML also lists specific technologies for the LMS (Python, Django, etc.) which are absent from the YAML definition, creating a minor inconsistency in detail. At the Container Level, the PlantUML diagram incorrectly places 'Librarian' and 'Patron' inside the System_Boundary; C4 defines people as external to the system. The most significant clarity issue is the introduction of Java Spring Boot as the technology for containers, directly contradicting the Python/Django stack mentioned in the Context diagram. This creates major confusion. The diagram also fails to represent containers for all core functional requirements, making the overall system incomplete. At the Component Level for Account Management, the 'Dependency Management Component' using 'pip' is inconsistent with the declared 'Java Spring Boot' technology for the container, further muddying the technology stack. The complete absence of both YAML and PlantUML for the 'Catalog Search' container is a significant gap in communication, leaving a core part of the system undefined. Overall, while the C4 structure is followed, the content suffers from critical inconsistencies in technology stack declarations, conceptual errors in C4 diagramming, and significant omissions, severely hindering effective communication.\",\n",
            "        \"rating\": 2.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"securityAssessment\": {\n",
            "    \"metric\": \"Security 'Red Team' Assessment\",\n",
            "    \"assessment\": {\n",
            "      \"vulnerabilities\": [\n",
            "        {\n",
            "          \"recommendation\": \"Introduce an API Gateway (e.g., Azure Application Gateway) as the single entry point for all external traffic. This gateway should handle SSL termination, WAF, rate limiting, and integrate with an authentication service.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"The diagram shows direct access from external users (Patron, Librarian) to 'catalog_search' and 'account_management' containers via HTTPS, without an intervening API Gateway or load balancer. This exposes the backend services directly, increasing the attack surface and making it harder to implement centralized security controls like WAF, rate limiting, and unified authentication.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement a centralized Identity and Access Management (IAM) service or integrate with an existing identity provider (e.g., Azure AD B2C for patrons, Azure AD for librarians). All services should delegate authentication and authorization decisions to this central service.\",\n",
            "          \"severity\": \"High\",\n",
            "          \"category\": \"Authentication Bypass\",\n",
            "          \"description\": \"While 'Role-based access' is a non-functional requirement (R-07), the architecture diagram does not depict a dedicated Authentication and Authorization service. This implies that 'catalog_search' and 'account_management' containers would each implement their own authentication and authorization logic, leading to potential inconsistencies, vulnerabilities, and increased development/maintenance overhead.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Explicitly include database components (e.g., Azure SQL Database, Azure Cosmos DB) in the architecture. Detail how data is encrypted at rest and in transit, and ensure strict network access controls (e.g., private endpoints, service endpoints) are applied to database instances, allowing access only from authorized application containers.\",\n",
            "          \"category\": \"Information Disclosure\",\n",
            "          \"severity\": \"Critical\",\n",
            "          \"description\": \"The diagram omits any database or data storage component. Sensitive patron data (account details, borrowing history) and library item metadata are critical assets. Without a clear representation of data storage, it's impossible to assess data-at-rest encryption, access controls, or potential direct database exposure from application containers.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement robust network segmentation using Virtual Networks (VNets) and Subnets in Azure. Utilize Network Security Groups (NSGs) to control traffic flow between subnets and to/from the internet. Deploy Azure DDoS Protection Standard for critical public-facing endpoints.\",\n",
            "          \"category\": \"Denial of Service\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"There is no explicit mention or component for network segmentation, firewalls, or DDoS protection. The direct exposure of 'catalog_search' and 'account_management' to the internet without these layers makes the system highly vulnerable to network-based attacks, including Denial of Service (DoS) and unauthorized access attempts.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Integrate Azure Monitor, Azure Log Analytics, and Azure Security Center for comprehensive logging, monitoring, and threat detection across all components. Implement security information and event management (SIEM) capabilities for real-time analysis and alerting.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"The architecture does not explicitly show components for centralized logging, monitoring, and alerting. Without these, detecting and responding to security incidents (e.g., brute-force attempts, unauthorized access, unusual activity) becomes significantly challenging, increasing the dwell time of attackers.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Designate a secure, dedicated service or module for external communication (e.g., Azure Logic Apps, Azure Functions integrating with Azure Communication Services or a secure third-party provider). Ensure credentials for these services are securely managed (e.g., Azure Key Vault) and communication is encrypted.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"The system requires sending automated overdue notices by email/SMS (R-04), but no specific component or secure integration pattern for this is shown. This implies potential direct integration with external email/SMS gateways, which could introduce risks related to credential management, secure communication, and potential for abuse.\"\n",
            "        }\n",
            "      ],\n",
            "      \"executiveSummary\": \"The proposed Library Management System architecture, while outlining core functional components, lacks critical security infrastructure and detailed data flow representations. The direct exposure of core services to external users without explicit intermediate security layers (like API Gateways, dedicated authentication services, or robust network segmentation) significantly broadens the attack surface. The absence of a database component in the diagram also prevents a thorough assessment of data-at-rest security and potential direct database access vulnerabilities.\",\n",
            "      \"overallRiskScore\": 29\n",
            "    }\n",
            "  }\n",
            "}\n",
            "âœ… Successfully saved new evaluation report to: evaluation_results_gemini\\Gemini15Flash_Collaborative_1_Rounds\\Library Management System (LMS)_20250613-115339-library-management-system-lms-a5b26a86\\evaluation_report_judged_by_gemini_2.5_flash_preview_05_20.json\n",
            "\n",
            "============================================================\n",
            "Processing Test Case: NextGen Point-of-Sale (POS)_20250613-115438-nextgen-point-of-sale-pos-fc5925d2\n",
            "============================================================\n",
            "âœ… Found matching system brief: 'NextGen Point-of-Sale (POS)'\n",
            "âœ… C4 model loaded successfully.\n",
            "--- âš™ï¸  Instantiating model: gemini-2.5-flash-preview-05-20 ---\n",
            "--- Running Holistic Structural Checks ---\n",
            "ðŸ¤– Evaluating Metric: PlantUML Compilation Success...\n",
            "  - âœ… SUCCESS: '1_Context' compiled.\n",
            "  - âœ… SUCCESS: '2_Containers' compiled.\n",
            "  - âŒ FAILED: '3_Component_inventory_management' content is empty.\n",
            "  - âŒ FAILED: '3_Component_reporting' content is empty.\n",
            "  - âœ… SUCCESS: '3_Component_transaction_processing' compiled.\n",
            "  - âŒ FAILED: '3_Component_user_interface' content is empty.\n",
            "ðŸ¤– Evaluating Metric: C4 Abstraction Adherence...\n",
            "ðŸ¤– Evaluating Metric: C4 Model Completeness...\n",
            "ðŸ¤– Evaluating Metric 6 (New): Emergent Naming Consistency...\n",
            "ðŸ¤– Evaluating Metric: Cross-Level Consistency Check...\n",
            "\n",
            "--- Running Level-Specific Evaluations ---\n",
            "  - Evaluating Context Level...\n",
            "âš–ï¸ Evaluating Metric 3: Semantic Consistency...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Context Diagram...\n",
            "  - Evaluating Container Level...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Container Diagram...\n",
            "  - Evaluating Component Level(s)...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Component: transaction_processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Holistic Expert Critiques ---\n",
            "âš–ï¸ Evaluating Metric 7: Principal Architect's Critique...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Key 'parameters' is not supported in schema, ignoring\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ›¡ï¸  Evaluating Metric 8: Security 'Red Team' Assessment...\n",
            "\n",
            "\n",
            "==================================================\n",
            "ðŸ“‹ FINAL EVALUATION REPORT (Judge: gemini-2.5-flash-preview-05-20) ðŸ“‹\n",
            "==================================================\n",
            "\n",
            "{\n",
            "  \"evaluationMetadata\": {\n",
            "    \"judgeModel\": \"gemini-2.5-flash-preview-05-20\",\n",
            "    \"judgeModelTemperature\": 0.0,\n",
            "    \"evaluationTimestamp\": \"2025-06-13T17:11:04.728735\"\n",
            "  },\n",
            "  \"compilationSuccess\": {\n",
            "    \"metric\": \"Compilation Success Rate\",\n",
            "    \"score\": 50.0,\n",
            "    \"successful\": 3,\n",
            "    \"total\": 6,\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"source\": \"1_Context\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"2_Containers\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_inventory_management\",\n",
            "        \"status\": \"Failed - Empty\",\n",
            "        \"error\": \"Diagram content was empty or whitespace.\"\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_reporting\",\n",
            "        \"status\": \"Failed - Empty\",\n",
            "        \"error\": \"Diagram content was empty or whitespace.\"\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_transaction_processing\",\n",
            "        \"status\": \"Compiled\",\n",
            "        \"error\": null\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"3_Component_user_interface\",\n",
            "        \"status\": \"Failed - Empty\",\n",
            "        \"error\": \"Diagram content was empty or whitespace.\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"abstractionAdherence\": {\n",
            "    \"metric\": \"Abstraction Adherence\",\n",
            "    \"score\": 100.0,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      },\n",
            "      \"Component: transaction_processing\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"Adheres to abstraction level.\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"missingInformation\": {\n",
            "    \"metric\": \"C4 Model Completeness\",\n",
            "    \"score\": 75.0,\n",
            "    \"missing_count\": 3,\n",
            "    \"total_expected_artifacts\": 12,\n",
            "    \"details\": {\n",
            "      \"Context\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Containers\": {\n",
            "        \"analysis\": \"Present\",\n",
            "        \"yaml_definition\": \"Present\",\n",
            "        \"diagram\": \"Present\"\n",
            "      },\n",
            "      \"Components\": {\n",
            "        \"inventory_management\": {\n",
            "          \"analysis\": \"Missing\",\n",
            "          \"yaml_definition\": \"Not Expected\",\n",
            "          \"diagram\": \"Not Expected\"\n",
            "        },\n",
            "        \"reporting\": {\n",
            "          \"analysis\": \"Missing\",\n",
            "          \"yaml_definition\": \"Not Expected\",\n",
            "          \"diagram\": \"Not Expected\"\n",
            "        },\n",
            "        \"transaction_processing\": {\n",
            "          \"analysis\": \"Present\",\n",
            "          \"yaml_definition\": \"Present\",\n",
            "          \"diagram\": \"Present\"\n",
            "        },\n",
            "        \"user_interface\": {\n",
            "          \"analysis\": \"Missing\",\n",
            "          \"yaml_definition\": \"Not Expected\",\n",
            "          \"diagram\": \"Not Expected\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"emergentNamingConsistency\": {\n",
            "    \"metric\": \"Emergent Naming Consistency\",\n",
            "    \"score\": 10.0,\n",
            "    \"dominantConvention\": {\n",
            "      \"name\": \"PascalCase\",\n",
            "      \"count\": 1,\n",
            "      \"total\": 10\n",
            "    },\n",
            "    \"allConventionCounts\": {\n",
            "      \"other\": 9,\n",
            "      \"PascalCase\": 1\n",
            "    },\n",
            "    \"details\": {\n",
            "      \"outliers\": [\n",
            "        {\n",
            "          \"name\": \"NextGen Point-of-Sale System\",\n",
            "          \"type\": \"system\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Transaction Processing\",\n",
            "          \"type\": \"container\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Inventory Management\",\n",
            "          \"type\": \"container\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"User Interface\",\n",
            "          \"type\": \"container\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Payment Processor\",\n",
            "          \"type\": \"component (in transaction_processing)\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Access Control Manager\",\n",
            "          \"type\": \"component (in transaction_processing)\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Security Assessment Module\",\n",
            "          \"type\": \"component (in transaction_processing)\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Vulnerability Manager\",\n",
            "          \"type\": \"component (in transaction_processing)\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        },\n",
            "        {\n",
            "          \"name\": \"Incident Response Handler\",\n",
            "          \"type\": \"component (in transaction_processing)\",\n",
            "          \"detected_convention\": \"other\",\n",
            "          \"reason\": \"Deviates from the dominant convention: 'PascalCase'\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"crossLevelConsistency\": {\n",
            "    \"metric\": \"Cross-Level Consistency\",\n",
            "    \"score\": 50.0,\n",
            "    \"passed\": 1,\n",
            "    \"total\": 2,\n",
            "    \"details\": {\n",
            "      \"Context->Container\": {\n",
            "        \"status\": \"Pass\",\n",
            "        \"reason\": \"External elements are consistent with the Context level.\"\n",
            "      },\n",
            "      \"Container->Component (transaction_processing)\": {\n",
            "        \"status\": \"Fail\",\n",
            "        \"reason\": \"References elements not found in Container scope: ['Security Assessment Module', 'Vulnerability Manager', 'Payment Processor', 'Access Control Manager', 'Incident Response Handler']\"\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"contextEvaluation\": {\n",
            "    \"semanticConsistency\": {\n",
            "      \"metric\": \"Semantic Consistency\",\n",
            "      \"score\": 18.18,\n",
            "      \"verified_items\": 2,\n",
            "      \"total_items\": 11\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Context Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"containerEvaluation\": {\n",
            "    \"definitionalConsistency\": {\n",
            "      \"score\": 100,\n",
            "      \"details\": {\n",
            "        \"message\": \"No containerss with a 'name' key found in the YAML definition.\"\n",
            "      }\n",
            "    },\n",
            "    \"qualitativeRubric\": {\n",
            "      \"diagram_name\": \"Container Diagram\",\n",
            "      \"average_score\": 0,\n",
            "      \"details\": {}\n",
            "    }\n",
            "  },\n",
            "  \"componentEvaluations\": {\n",
            "    \"transaction_processing\": {\n",
            "      \"definitionalConsistency\": {\n",
            "        \"score\": 100,\n",
            "        \"details\": {\n",
            "          \"message\": \"No componentss with a 'name' key found in the YAML definition.\"\n",
            "        }\n",
            "      },\n",
            "      \"qualitativeRubric\": {\n",
            "        \"diagram_name\": \"Component: transaction_processing\",\n",
            "        \"average_score\": 0,\n",
            "        \"details\": {}\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"architectCritique\": {\n",
            "    \"metric\": \"Principal Architect's Critique\",\n",
            "    \"critique\": {\n",
            "      \"actionableRecommendation\": {\n",
            "        \"recommendation\": \"Introduce a dedicated 'POS Terminal Application' or 'Edge Application' container responsible for local data storage, transaction buffering, and synchronization with the backend 'Transaction Processing' service. This container should manage the offline queueing and ensure data integrity during network interruptions.\",\n",
            "        \"priority\": \"Critical\",\n",
            "        \"justification\": \"The current architecture lacks explicit design for handling offline transactions and ensuring no lost sales during network outages, which are critical non-functional requirements (R-07 and 'Offline transaction buffering \\u2264 24 h' constraint). A robust POS system must be resilient to network failures at the edge.\"\n",
            "      },\n",
            "      \"feasibilityAndSoundness\": {\n",
            "        \"identifiedRisks\": [\n",
            "          \"Lack of explicit design for offline transaction buffering and resilience against network outages, directly violating critical non-functional requirements.\",\n",
            "          \"Potential for data inconsistencies or loss if network connectivity is intermittent or lost, due to the absence of a robust edge-level data synchronization strategy.\",\n",
            "          \"Reliance on external systems (Payment Gateway, Inventory Management System) without detailed strategies for handling their potential unavailability or latency, which could impact overall system performance and reliability.\"\n",
            "        ],\n",
            "        \"critique\": \"The technology choices (Java Spring Boot for Transaction Processing, Node.js for Inventory Management, Python for Reporting, React for UI) are realistic and generally well-suited for their respective domains, supporting a microservices-oriented approach. The decomposition into distinct containers (Transaction Processing, Inventory Management, Reporting, UI) makes sense for scalability and independent deployment. However, the architecture does not explicitly address the critical non-functional requirements of 'Offline transaction buffering \\u2264 24 h' and 'No lost sales during network outages' (R-07). The current design implies a purely online system, which is a significant oversight for a POS. Performance (R-05) and Availability (R-06) are generally supported by the chosen technologies and decomposition, but the lack of offline capability is a major flaw.\",\n",
            "        \"rating\": 3.0\n",
            "      },\n",
            "      \"executiveSummary\": \"The proposed NextGen Point-of-Sale architecture presents a sound foundational decomposition into logical containers with appropriate technology choices. The Context and Container level diagrams are well-defined and clear. However, the design critically overlooks explicit mechanisms for handling offline operations and ensuring data resilience during network outages, which are fundamental requirements for a modern POS system. The lack of detailed component-level diagrams also leaves significant gaps in understanding the internal workings of key modules.\",\n",
            "      \"clarityAndCommunication\": {\n",
            "        \"critique\": \"The Context and Container level diagrams, along with their YAML definitions, are clear, consistent, and effectively communicate the architecture at those levels. The link between the definitions and diagrams is strong. However, the absence of any Component Level diagrams and definitions, especially for critical modules like 'inventory_management' and 'reporting' (which were explicitly requested but left blank), significantly hinders the overall completeness and detailed understanding of the architecture. Key aspects like 'offline queueing' mentioned in the brief are not visually represented or detailed in any of the provided diagrams.\",\n",
            "        \"rating\": 3.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"securityAssessment\": {\n",
            "    \"metric\": \"Security 'Red Team' Assessment\",\n",
            "    \"assessment\": {\n",
            "      \"vulnerabilities\": [\n",
            "        {\n",
            "          \"recommendation\": \"Implement an API Gateway (e.g., AWS API Gateway) in front of the backend services (`transactionprocessing`, `inventorymanagement`, `reporting`). This gateway should enforce authentication, authorization, rate limiting, and provide WAF protection.\",\n",
            "          \"severity\": \"Critical\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"description\": \"The `userinterface` directly communicates with `transactionprocessing`, `inventorymanagement`, and `reporting` via REST. There is no explicit API Gateway or centralized security layer shown to handle authentication, authorization, rate limiting, input validation, or WAF capabilities.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Explicitly define the database/storage solution. Implement strong encryption for data at rest (e.g., AWS KMS for EBS/RDS encryption). Ensure strict access controls are in place for the database. Detail how offline buffered data, especially cardholder data, is secured and purged according to PCI-DSS requirements.\",\n",
            "          \"category\": \"Insecure Data Flow\",\n",
            "          \"severity\": \"Critical\",\n",
            "          \"description\": \"The diagram does not show any database or persistent storage component. The system brief mentions \\\"offline transaction buffering \\u2264 24 h\\\" and \\\"PCI-DSS Level 1 compliance,\\\" implying sensitive transaction data (potentially including payment details or PII) will be stored locally. Without a defined database and explicit controls for data at rest encryption, access control, and secure deletion, this poses a significant PCI-DSS compliance and data breach risk.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement a dedicated Identity and Access Management (IAM) service (e.g., AWS Cognito, Okta, or an internal service) to handle user authentication and authorization. Integrate this service with the API Gateway and backend services to enforce granular access control based on user roles.\",\n",
            "          \"severity\": \"High\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"description\": \"While `cashier` and `manager` interact with the `userinterface`, there is no explicit authentication and authorization service shown. This raises concerns about how user identities are managed, how roles (`cashier` vs. `manager`) are enforced, and how access to different functionalities (e.g., manager reports vs. cashier transactions) is controlled.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement network segmentation (e.g., AWS VPCs, Security Groups, Network ACLs) to isolate containers from each other based on their function and sensitivity. Apply strict firewall rules to allow only necessary communication paths.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"High\",\n",
            "          \"description\": \"The C4 diagram does not depict any network segmentation or firewall rules between the containers within the `NextGen Point-of-Sale System` boundary or between the system and external entities. This lack of internal segmentation means that if one container is compromised, an attacker could easily move laterally to other containers.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement a comprehensive logging and monitoring solution (e.g., AWS CloudWatch, ELK stack). Ensure all security-relevant events, API calls, and data access attempts are logged. Establish alerting mechanisms for suspicious activities and integrate with a Security Information and Event Management (SIEM) system if available.\",\n",
            "          \"category\": \"Information Disclosure\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"The architecture does not explicitly show components for logging, monitoring, or alerting. Without robust logging of security events, access attempts, and system activities, detecting and responding to security incidents (e.g., unauthorized access, data exfiltration attempts) becomes extremely difficult or impossible.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement a dedicated secrets management solution (e.g., AWS Secrets Manager, HashiCorp Vault) to securely store, retrieve, and rotate all application secrets. Avoid hardcoding credentials in code or configuration files.\",\n",
            "          \"category\": \"Missing Security Control\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"The diagram does not indicate how sensitive credentials (e.g., API keys for `paymentgateway`, database credentials, internal service-to-service authentication tokens) are stored and managed. Hardcoding or insecure storage of secrets can lead to compromise.\"\n",
            "        },\n",
            "        {\n",
            "          \"recommendation\": \"Implement rate limiting at the API Gateway level and potentially at the individual service level to prevent abuse and DoS attacks. Utilize AWS WAF and Shield for DDoS protection.\",\n",
            "          \"category\": \"Denial of Service\",\n",
            "          \"severity\": \"Medium\",\n",
            "          \"description\": \"Without an API Gateway or explicit rate-limiting mechanisms, the backend services (`transactionprocessing`, `inventorymanagement`, `reporting`) are vulnerable to abuse, including denial-of-service attacks through excessive requests from the `userinterface` or a compromised client.\"\n",
            "        }\n",
            "      ],\n",
            "      \"executiveSummary\": \"The NextGen Point-of-Sale system, while aiming for PCI-DSS Level 1 compliance, exhibits several architectural weaknesses that could expose sensitive patron data and lead to various security breaches. The primary concerns revolve around the lack of explicit security controls for API access, internal data storage (especially for offline buffering), and a general absence of defense-in-depth mechanisms like network segmentation and dedicated authentication services. The direct exposure of backend services to the UI without an API Gateway is a significant attack surface.\",\n",
            "      \"overallRiskScore\": 36\n",
            "    }\n",
            "  }\n",
            "}\n",
            "âœ… Successfully saved new evaluation report to: evaluation_results_gemini\\Gemini15Flash_Collaborative_1_Rounds\\NextGen Point-of-Sale (POS)_20250613-115438-nextgen-point-of-sale-pos-fc5925d2\\evaluation_report_judged_by_gemini_2.5_flash_preview_05_20.json\n",
            "\n",
            "============================================================\n",
            "Processing Test Case: Online Bookstore (Mini-Amazon)_20250613-115602-online-bookstore-mini-amazon-833c2c28\n",
            "============================================================\n",
            "âœ… Found matching system brief: 'Online Bookstore (Mini-Amazon)'\n",
            "âœ… C4 model loaded successfully.\n",
            "--- âš™ï¸  Instantiating model: gemini-2.5-flash-preview-05-20 ---\n",
            "--- Running Holistic Structural Checks ---\n",
            "ðŸ¤– Evaluating Metric: PlantUML Compilation Success...\n",
            "  - âœ… SUCCESS: '1_Context' compiled.\n",
            "  - âœ… SUCCESS: '2_Containers' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_data_management' compiled.\n",
            "  - âœ… SUCCESS: '3_Component_order_processing' compiled.\n",
            "  - âŒ FAILED: '3_Component_web_application' content is empty.\n",
            "ðŸ¤– Evaluating Metric: C4 Abstraction Adherence...\n",
            "ðŸ¤– Evaluating Metric: C4 Model Completeness...\n",
            "ðŸ¤– Evaluating Metric 6 (New): Emergent Naming Consistency...\n",
            "ðŸ¤– Evaluating Metric: Cross-Level Consistency Check...\n",
            "\n",
            "--- Running Level-Specific Evaluations ---\n",
            "  - Evaluating Context Level...\n",
            "âš–ï¸ Evaluating Metric 3: Semantic Consistency...\n",
            "âš–ï¸ Evaluating Metric: Qualitative Rubric for Context Diagram...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[87], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(base_dir):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Starting provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mre_evaluate_all_test_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJUDGE_MODEL_FOR_REEVALUATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_briefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mINFO: Directory for provider \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[87], line 61\u001b[0m, in \u001b[0;36mre_evaluate_all_test_cases\u001b[1;34m(base_results_dir, judge_model, briefs_dict)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… C4 model loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 3. Run the evaluation\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m new_evaluation_report \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_brief\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_brief\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc4_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc4_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjudge_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjudge_model\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 4. Save the new report\u001b[39;00m\n\u001b[0;32m     68\u001b[0m report_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_report_judged_by_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(judge_model)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[1;32mIn[85], line 48\u001b[0m, in \u001b[0;36mrun_full_evaluation\u001b[1;34m(system_brief, c4_model, judge_model_name, temperature)\u001b[0m\n\u001b[0;32m     46\u001b[0m         context_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemanticConsistency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_semantic_consistency(system_brief, c4_model, judge_llm)\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;66;03m# The qualitative rubric now gets the brief it needs to judge completeness.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m         context_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqualitativeRubric\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_qualitative_rubric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_diag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContext Diagram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_brief\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudge_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontextEvaluation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m context_eval\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# --- CONTAINER LEVEL EVALUATION ---\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 55\u001b[0m, in \u001b[0;36mevaluate_qualitative_rubric\u001b[1;34m(diagram_code, diagram_name, system_brief, judge_llm)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# --- 4. Invoke the chain with the new variable ---\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# <<< CHANGED: Pass the system_brief into the invoke call >>>\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrubric_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiagram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagram_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem_brief\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_brief\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[0;32m     60\u001b[0m     average_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores) \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3045\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3046\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3047\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3048\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5431\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5424\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5432\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5433\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5434\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5435\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1255\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         )\n\u001b[1;32m-> 1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    777\u001b[0m                 m,\n\u001b[0;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    781\u001b[0m             )\n\u001b[0;32m    782\u001b[0m         )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:1342\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1318\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1330\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m   1331\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1332\u001b[0m         messages,\n\u001b[0;32m   1333\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   1341\u001b[0m     )\n\u001b[1;32m-> 1342\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1343\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1344\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1345\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m   1346\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:210\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:192\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:78\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 78\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\grpc\\_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m   1192\u001b[0m     (\n\u001b[0;32m   1193\u001b[0m         state,\n\u001b[0;32m   1194\u001b[0m         call,\n\u001b[1;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\kamil\\Documents\\Kamil\\HICSS\\.venv\\lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
            "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def re_evaluate_all_test_cases(base_results_dir: str, judge_model: Any, briefs_dict: Dict[str, str]):\n",
        "    \"\"\"\n",
        "    Finds all C4 artifacts, loads them, and runs a new evaluation using a robust matching logic.\n",
        "    \"\"\"\n",
        "    print(f\"ðŸš€ Starting re-evaluation process with judge: {judge_model} ðŸš€\")\n",
        "\n",
        "    def is_match(brief_key: str, dir_name: str) -> bool:\n",
        "        \"\"\"\n",
        "        Robustly checks if the directory name corresponds to the brief key.\n",
        "        - Makes comparison case-insensitive.\n",
        "        - Removes all non-alphanumeric characters.\n",
        "        \"\"\"\n",
        "        # \"NextGen Point-of-Sale (POS)\" -> \"nextgenpointofsalepos\"\n",
        "        normalized_key = re.sub(r'[^a-zA-Z0-9]', '', brief_key).lower()\n",
        "        \n",
        "        # \"NextGen_Point-of-Sale_POS_thread_xyz\" -> \"nextgenpointofsaleposthreadxyz\"\n",
        "        normalized_dir = re.sub(r'[^a-zA-Z0-9]', '', dir_name).lower()\n",
        "        \n",
        "        return normalized_key in normalized_dir\n",
        "\n",
        "    search_pattern = os.path.join(base_results_dir, \"**\", \"c4_artifacts\")\n",
        "    artifact_dirs = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "    if not artifact_dirs:\n",
        "        print(f\"âŒ No 'c4_artifacts' directories found in '{base_results_dir}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(artifact_dirs)} test cases to re-evaluate.\")\n",
        "\n",
        "    for artifacts_dir in artifact_dirs:\n",
        "        test_case_path = os.path.dirname(artifacts_dir)\n",
        "        test_case_name = os.path.basename(test_case_path)\n",
        "        \n",
        "        print(f\"\\n{'='*60}\\nProcessing Test Case: {test_case_name}\\n{'='*60}\")\n",
        "\n",
        "        # 1. Find the corresponding system brief using the robust matching function\n",
        "        system_brief = None\n",
        "        matched_brief_name = None\n",
        "        for brief_name, brief_content in briefs_dict.items():\n",
        "            if is_match(brief_name, test_case_name):\n",
        "                system_brief = brief_content\n",
        "                matched_brief_name = brief_name\n",
        "                break\n",
        "        \n",
        "        if system_brief:\n",
        "            print(f\"âœ… Found matching system brief: '{matched_brief_name}'\")\n",
        "        else:\n",
        "            print(f\"âŒ CRITICAL: No matching system brief found for test case '{test_case_name}'. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        # ... (The rest of the script is unchanged) ...\n",
        "\n",
        "        # 2. Load the C4 model\n",
        "        c4_model = load_c4_model_from_artifacts(artifacts_dir)\n",
        "        if not c4_model:\n",
        "            print(f\"âŒ CRITICAL: Could not load C4 model from {artifacts_dir}. Skipping.\")\n",
        "            continue\n",
        "        print(\"âœ… C4 model loaded successfully.\")\n",
        "\n",
        "        # 3. Run the evaluation\n",
        "        new_evaluation_report = run_full_evaluation(\n",
        "            system_brief=system_brief,\n",
        "            c4_model=c4_model,\n",
        "            judge_model_name=judge_model\n",
        "        )\n",
        "\n",
        "        # 4. Save the new report\n",
        "        report_filename = f\"evaluation_report_judged_by_{str(judge_model).replace('-', '_')}.json\"\n",
        "        report_path = os.path.join(test_case_path, report_filename)\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(new_evaluation_report, f, indent=4)\n",
        "        print(f\"âœ… Successfully saved new evaluation report to: {report_path}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Your System Briefs ---\n",
        "\n",
        "\n",
        "    # --- Configuration ---\n",
        "    providers = ['gemini', 'openai', 'grok']\n",
        "    JUDGE_MODEL_FOR_REEVALUATION = \"gemini-2.5-flash-preview-05-20\"  # The judge model to use for re-evaluation\n",
        "\n",
        "    # --- Run the Process ---\n",
        "    for provider in providers:\n",
        "        base_dir = f\"evaluation_results_{provider}\"\n",
        "        if os.path.isdir(base_dir):\n",
        "            print(f\"\\n\\n{'#'*80}\\n# Starting provider: {provider.upper()}\\n{'#'*80}\")\n",
        "            re_evaluate_all_test_cases(base_dir, JUDGE_MODEL_FOR_REEVALUATION, system_briefs)\n",
        "        else:\n",
        "            print(f\"\\nINFO: Directory for provider '{provider}' not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting analysis... Will save results to a pandas DataFrame.\n",
            "Found 60 artifact directories to analyze.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š Component Generation Analysis (Pandas DataFrame) ðŸ“Š\n",
            "================================================================================\n",
            "   provider                            experiment  mean_components  std_dev_components  min_components  \\\n",
            "0    gemini  Gemini15Flash_Collaborative_1_Rounds              1.8            1.600000               0   \n",
            "1    gemini  Gemini15Flash_Collaborative_3_Rounds              2.8            2.039608               0   \n",
            "2    gemini                  Gemini15Flash_Simple              5.4            1.356466               4   \n",
            "3      grok      Grok3mini_Collaborative_1_Rounds              5.2            0.400000               5   \n",
            "4      grok      Grok3mini_Collaborative_3_Rounds              5.4            0.489898               5   \n",
            "5      grok                      Grok3mini_Simple              4.6            0.489898               4   \n",
            "6    openai          GPT4o_Collaborative_1_Rounds              7.2            1.720465               4   \n",
            "7    openai          GPT4o_Collaborative_3_Rounds              8.2            0.748331               7   \n",
            "8    openai                          GPT4o_Simple              7.0            0.632456               6   \n",
            "9    openai      GPT4omini_Collaborative_1_Rounds              7.6            1.019804               6   \n",
            "10   openai      GPT4omini_Collaborative_3_Rounds              7.2            1.166190               6   \n",
            "11   openai                      GPT4omini_Simple              5.2            0.979796               4   \n",
            "\n",
            "    max_components  test_case_count       raw_counts  \n",
            "0                4                5  [0, 2, 4, 3, 0]  \n",
            "1                5                5  [5, 1, 5, 0, 3]  \n",
            "2                8                5  [4, 5, 5, 8, 5]  \n",
            "3                6                5  [5, 5, 5, 5, 6]  \n",
            "4                6                5  [5, 6, 6, 5, 5]  \n",
            "5                5                5  [4, 5, 4, 5, 5]  \n",
            "6                9                5  [8, 8, 4, 9, 7]  \n",
            "7                9                5  [9, 8, 8, 9, 7]  \n",
            "8                8                5  [7, 6, 7, 8, 7]  \n",
            "9                9                5  [9, 6, 7, 8, 8]  \n",
            "10               9                5  [6, 6, 7, 9, 8]  \n",
            "11               7                5  [5, 5, 5, 4, 7]  \n",
            "================================================================================\n",
            "\n",
            "âœ… Successfully saved the analysis to 'component_analysis_summary.csv'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_and_save_to_pandas(base_search_path: str = \"evaluation_results_*\"):\n",
        "    \"\"\"\n",
        "    Analyzes C4 artifacts, calculates component statistics, and saves the\n",
        "    summary to a pandas DataFrame and a CSV file.\n",
        "\n",
        "    Args:\n",
        "        base_search_path (str): The base pattern to find provider directories.\n",
        "    \"\"\"\n",
        "    print(\"ðŸš€ Starting analysis... Will save results to a pandas DataFrame.\")\n",
        "\n",
        "    # --- 1. Data Collection (Same as before) ---\n",
        "    experiment_counts = defaultdict(lambda: defaultdict(list))\n",
        "    search_pattern = os.path.join(base_search_path, \"**\", \"c4_artifacts\")\n",
        "    artifact_dirs = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "    if not artifact_dirs:\n",
        "        print(\"âŒ No 'c4_artifacts' directories found. Please check your path.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(artifact_dirs)} artifact directories to analyze.\")\n",
        "\n",
        "    for artifact_dir in artifact_dirs:\n",
        "        try:\n",
        "            path_parts = artifact_dir.split(os.sep)\n",
        "            provider = path_parts[0].replace(\"evaluation_results_\", \"\")\n",
        "            experiment = path_parts[1]\n",
        "            components_dir = os.path.join(artifact_dir, '3_components')\n",
        "            component_count = 0\n",
        "            if os.path.isdir(components_dir):\n",
        "                definition_files = glob.glob(os.path.join(components_dir, '*_definition.yaml'))\n",
        "                component_count = len(definition_files)\n",
        "            experiment_counts[provider][experiment].append(component_count)\n",
        "        except IndexError:\n",
        "            print(f\"âš ï¸ Could not parse path: {artifact_dir}. Skipping.\")\n",
        "\n",
        "    # --- 2. Process data for DataFrame ---\n",
        "    analysis_data = []\n",
        "    if not experiment_counts:\n",
        "        print(\"No data was successfully collected.\")\n",
        "        return\n",
        "\n",
        "    # Sort for consistent ordering\n",
        "    sorted_providers = sorted(experiment_counts.keys())\n",
        "    for provider in sorted_providers:\n",
        "        sorted_experiments = sorted(experiment_counts[provider].keys())\n",
        "        for experiment in sorted_experiments:\n",
        "            counts = experiment_counts[provider][experiment]\n",
        "            if not counts:\n",
        "                continue\n",
        "\n",
        "            analysis_data.append({\n",
        "                \"provider\": provider,\n",
        "                \"experiment\": experiment,\n",
        "                \"mean_components\": np.mean(counts),\n",
        "                \"std_dev_components\": np.std(counts),\n",
        "                \"min_components\": min(counts),\n",
        "                \"max_components\": max(counts),\n",
        "                \"test_case_count\": len(counts),\n",
        "                \"raw_counts\": counts  # Keep the raw data for potential deeper analysis\n",
        "            })\n",
        "            \n",
        "    # --- 3. Create DataFrame and Save ---\n",
        "    if not analysis_data:\n",
        "        print(\"Analysis yielded no data to save.\")\n",
        "        return\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(analysis_data)\n",
        "    \n",
        "    # Define column order for better presentation\n",
        "    column_order = [\n",
        "        \"provider\", \"experiment\", \"mean_components\", \"std_dev_components\", \n",
        "        \"min_components\", \"max_components\", \"test_case_count\", \"raw_counts\"\n",
        "    ]\n",
        "    df = df[column_order]\n",
        "\n",
        "    # Set display options for cleaner printing\n",
        "    pd.set_option('display.max_rows', 500)\n",
        "    pd.set_option('display.width', 120)\n",
        "    \n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"ðŸ“Š Component Generation Analysis (Pandas DataFrame) ðŸ“Š\")\n",
        "    print(\"=\"*80)\n",
        "    print(df)\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    output_filename = \"component_analysis_summary.csv\"\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    \n",
        "    print(f\"\\nâœ… Successfully saved the analysis to '{output_filename}'\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # You might need to run: pip install pandas numpy\n",
        "    analyze_and_save_to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Analyzing artifacts to generate an LLM-readable table...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ðŸ“‹ LLM-Readable Component Analysis (Markdown Format) ðŸ“‹\n",
            "================================================================================\n",
            "You can copy and paste the table below directly into another LLM prompt.\n",
            "--------------------------------------------------------------------------------\n",
            "| provider   | experiment                           |   mean_components |   min_components |   max_components |\n",
            "|:-----------|:-------------------------------------|------------------:|-----------------:|-----------------:|\n",
            "| gemini     | Gemini15Flash_Collaborative_1_Rounds |               1.8 |                0 |                4 |\n",
            "| gemini     | Gemini15Flash_Collaborative_3_Rounds |               2.8 |                0 |                5 |\n",
            "| gemini     | Gemini15Flash_Simple                 |               5.4 |                4 |                8 |\n",
            "| grok       | Grok3mini_Collaborative_1_Rounds     |               5.2 |                5 |                6 |\n",
            "| grok       | Grok3mini_Collaborative_3_Rounds     |               5.4 |                5 |                6 |\n",
            "| grok       | Grok3mini_Simple                     |               4.6 |                4 |                5 |\n",
            "| openai     | GPT4o_Collaborative_1_Rounds         |               7.2 |                4 |                9 |\n",
            "| openai     | GPT4o_Collaborative_3_Rounds         |               8.2 |                7 |                9 |\n",
            "| openai     | GPT4o_Simple                         |               7   |                6 |                8 |\n",
            "| openai     | GPT4omini_Collaborative_1_Rounds     |               7.6 |                6 |                9 |\n",
            "| openai     | GPT4omini_Collaborative_3_Rounds     |               7.2 |                6 |                9 |\n",
            "| openai     | GPT4omini_Simple                     |               5.2 |                4 |                7 |\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_llm_readable_table(base_search_path: str = \"evaluation_results_*\"):\n",
        "    \"\"\"\n",
        "    Analyzes C4 artifacts and displays the results as a clean,\n",
        "    LLM-readable Markdown table.\n",
        "\n",
        "    Args:\n",
        "        base_search_path (str): The base pattern to find provider directories.\n",
        "    \"\"\"\n",
        "    print(\"ðŸš€ Analyzing artifacts to generate an LLM-readable table...\")\n",
        "\n",
        "    # --- 1. Data Collection (Same as previous scripts) ---\n",
        "    experiment_counts = defaultdict(lambda: defaultdict(list))\n",
        "    search_pattern = os.path.join(base_search_path, \"**\", \"c4_artifacts\")\n",
        "    artifact_dirs = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "    if not artifact_dirs:\n",
        "        print(\"âŒ No 'c4_artifacts' directories found.\")\n",
        "        return\n",
        "\n",
        "    for artifact_dir in artifact_dirs:\n",
        "        try:\n",
        "            path_parts = artifact_dir.split(os.sep)\n",
        "            provider = path_parts[0].replace(\"evaluation_results_\", \"\")\n",
        "            experiment = path_parts[1]\n",
        "            components_dir = os.path.join(artifact_dir, '3_components')\n",
        "            component_count = 0\n",
        "            if os.path.isdir(components_dir):\n",
        "                definition_files = glob.glob(os.path.join(components_dir, '*_definition.yaml'))\n",
        "                component_count = len(definition_files)\n",
        "            experiment_counts[provider][experiment].append(component_count)\n",
        "        except IndexError:\n",
        "            print(f\"âš ï¸ Could not parse path: {artifact_dir}. Skipping.\")\n",
        "\n",
        "    # --- 2. Process data for DataFrame (Same as before) ---\n",
        "    analysis_data = []\n",
        "    if not experiment_counts:\n",
        "        return\n",
        "\n",
        "    for provider in sorted(experiment_counts.keys()):\n",
        "        for experiment in sorted(experiment_counts[provider].keys()):\n",
        "            counts = experiment_counts[provider][experiment]\n",
        "            if not counts: continue\n",
        "            analysis_data.append({\n",
        "                \"provider\": provider,\n",
        "                \"experiment\": experiment,\n",
        "                \"mean_components\": np.mean(counts),\n",
        "                \"min_components\": min(counts),\n",
        "                \"max_components\": max(counts),\n",
        "            })\n",
        "\n",
        "    if not analysis_data:\n",
        "        print(\"Analysis yielded no data to display.\")\n",
        "        return\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(analysis_data)\n",
        "    # Round the mean for cleaner display\n",
        "    df['mean_components'] = df['mean_components'].round(2)\n",
        "\n",
        "    # --- 3. Generate and Display LLM-Readable Table ---\n",
        "    # Select only the columns you requested\n",
        "    table_df = df[['provider', 'experiment', 'mean_components', 'min_components', 'max_components']]\n",
        "\n",
        "    # Use the to_markdown() function to generate the table\n",
        "    markdown_table = table_df.to_markdown(index=False)\n",
        "    \n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"ðŸ“‹ LLM-Readable Component Analysis (Markdown Format) ðŸ“‹\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"You can copy and paste the table below directly into another LLM prompt.\")\n",
        "    print(\"-\"*(80))\n",
        "    print(markdown_table)\n",
        "    print(\"-\"*(80))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # You might need to run: pip install pandas numpy tabulate\n",
        "    # The 'to_markdown' function requires the 'tabulate' library.\n",
        "    generate_llm_readable_table()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
